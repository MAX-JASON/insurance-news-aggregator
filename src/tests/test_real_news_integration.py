"""
çœŸå¯¦æ–°èæŠ“å–èˆ‡æ•´åˆæ¸¬è©¦
Real News Crawling and Integration Test

å°‡çœŸå¯¦æ–°èçˆ¬èŸ²æ•´åˆåˆ°ä¸»ç³»çµ±ä¸¦å¯«å…¥è³‡æ–™åº«
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from crawler.real_crawler_fixed import RealInsuranceNewsCrawler
from database.models import News, NewsSource, NewsCategory, db
from app import create_app
from config.settings import Config
from datetime import datetime, timezone
import logging

logger = logging.getLogger('real_news_integration')

def crawl_and_save_real_news():
    """çˆ¬å–çœŸå¯¦æ–°èä¸¦å„²å­˜åˆ°è³‡æ–™åº«"""
    
    print("ğŸš€ é–‹å§‹çœŸå¯¦æ–°èæŠ“å–èˆ‡æ•´åˆæ¸¬è©¦...")
    
    # å»ºç«‹Flaskæ‡‰ç”¨
    app = create_app(Config)
    
    # åˆå§‹åŒ–çˆ¬èŸ²
    crawler = RealInsuranceNewsCrawler()
    
    # çˆ¬å–æ–°è
    print("\nğŸ“¡ æ­£åœ¨çˆ¬å–çœŸå¯¦æ–°è...")
    news_list = crawler.crawl_all_sources()
    
    if not news_list:
        print("âŒ æ²’æœ‰çˆ¬å–åˆ°ä»»ä½•çœŸå¯¦æ–°è")
        return
    
    print(f"\nâœ… æˆåŠŸçˆ¬å– {len(news_list)} å‰‡çœŸå¯¦æ–°è")
    
    # å„²å­˜åˆ°è³‡æ–™åº«
    with app.app_context():
        saved_count = 0
        for news in news_list:
            try:                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = News.query.filter_by(title=news['title']).first()
                if existing:
                    print(f"  ğŸ“ æ–°èå·²å­˜åœ¨: {news['title'][:30]}...")
                    continue
                
                # å»ºç«‹æ–°çš„æ–°èè¨˜éŒ„
                article = News(
                    title=news['title'],
                    url=news['url'],
                    content=news.get('content', ''),
                    summary=news.get('summary', ''),
                    source=news['source'],
                    category='ä¿éšª',
                    published_date=news.get('published_date', datetime.now(timezone.utc)),
                    is_active=True
                )
                
                db.session.add(article)
                saved_count += 1
                print(f"  âœ… æ–°å¢çœŸå¯¦æ–°è: {news['title'][:30]}...")
                
            except Exception as e:
                print(f"  âŒ å„²å­˜æ–°èå¤±æ•— {news['title'][:30]}: {e}")
                continue
        
        try:
            db.session.commit()
            print(f"\nğŸ‰ æˆåŠŸå„²å­˜ {saved_count} å‰‡çœŸå¯¦æ–°èåˆ°è³‡æ–™åº«")
        except Exception as e:
            db.session.rollback()
            print(f"âŒ è³‡æ–™åº«æäº¤å¤±æ•—: {e}")
            return
      # æª¢æŸ¥çµæœ
    with app.app_context():
        total_news = News.query.count()
        real_news = News.query.filter(
            News.source.in_(['Googleæ–°è', 'è¯åˆæ–°èç¶²', 'è‡ªç”±æ™‚å ±è²¡ç¶“'])
        ).count()
        mock_news = total_news - real_news
        
        print(f"\nğŸ“Š è³‡æ–™åº«ç‹€æ³æ›´æ–°:")
        print(f"  ç¸½æ–°èæ•¸: {total_news}")
        print(f"  çœŸå¯¦æ–°è: {real_news} å‰‡")
        print(f"  æ¨¡æ“¬æ–°è: {mock_news} å‰‡")
        
        if real_news > 0:
            print("\nğŸ‰ æ­å–œï¼ç³»çµ±ç¾åœ¨å¯ä»¥æŠ“å–çœŸå¯¦æ–°èäº†ï¼")
            
            # é¡¯ç¤ºæœ€æ–°çš„çœŸå¯¦æ–°è
            latest_real_news = News.query.filter(
                News.source.in_(['Googleæ–°è', 'è¯åˆæ–°èç¶²', 'è‡ªç”±æ™‚å ±è²¡ç¶“'])
            ).order_by(News.published_date.desc()).limit(3).all()
            
            print("\nğŸ“° æœ€æ–°çœŸå¯¦æ–°è:")
            for i, article in enumerate(latest_real_news, 1):
                print(f"  {i}. {article.title[:50]}...")
                print(f"     ä¾†æº: {article.source}")
                print(f"     æ™‚é–“: {article.published_date}")
                print()
        else:
            print("\nâš ï¸ å°šæœªæˆåŠŸæŠ“å–åˆ°çœŸå¯¦æ–°è")

if __name__ == "__main__":
    crawl_and_save_real_news()
