"""
å¤šä¾†æºæ–°èçˆ¬èŸ²ç³»çµ±
Multi-Source News Crawler System

æ•´åˆå°ç£ä¸»è¦è²¡ç¶“åª’é«”çš„ä¿éšªæ–°èçˆ¬å–
"""

import os
import sys
import requests
import logging
import time
import re
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from dataclasses import dataclass

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger('multi_source_crawler')

@dataclass
class NewsArticle:
    """æ–°èæ–‡ç« æ•¸æ“šçµæ§‹"""
    title: str
    url: str
    summary: str
    content: str
    published_date: datetime
    source: str
    category: str = "ä¿éšªæ–°è"
    author: str = ""
    tags: List[str] = None

class MultiSourceNewsCrawler:
    """å¤šä¾†æºæ–°èçˆ¬èŸ²"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬èŸ²"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # æ–°èä¾†æºé…ç½®
        self.sources = {
            'ctee': {
                'name': 'å·¥å•†æ™‚å ±',
                'base_url': 'https://ctee.com.tw',
                'insurance_url': 'https://ctee.com.tw/category/insurance',
                'parser': self._parse_ctee_news
            },
            'udn_money': {
                'name': 'ç¶“æ¿Ÿæ—¥å ±',
                'base_url': 'https://money.udn.com',
                'insurance_url': 'https://money.udn.com/money/cate/5636',
                'parser': self._parse_udn_news
            },
            'chinatimes_money': {
                'name': 'ä¸­æ™‚æ–°èç¶²è²¡ç¶“',
                'base_url': 'https://www.chinatimes.com',
                'insurance_url': 'https://www.chinatimes.com/money/?chdtv',
                'parser': self._parse_chinatimes_news
            },
            'ltn_ec': {
                'name': 'è‡ªç”±æ™‚å ±è²¡ç¶“',
                'base_url': 'https://ec.ltn.com.tw',
                'insurance_url': 'https://ec.ltn.com.tw/list/paper',
                'parser': self._parse_ltn_news
            }
        }
        
        self.delay_between_requests = 2.0  # è«‹æ±‚é–“éš”
        self.timeout = 15  # è«‹æ±‚è¶…æ™‚
        
        logger.info(f"ğŸ•·ï¸ å¤šä¾†æºæ–°èçˆ¬èŸ²åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æ´ {len(self.sources)} å€‹ä¾†æº")
    
    def crawl_all_sources(self, max_articles_per_source: int = 10) -> List[NewsArticle]:
        """çˆ¬å–æ‰€æœ‰ä¾†æºçš„æ–°è"""
        logger.info("ğŸš€ é–‹å§‹çˆ¬å–æ‰€æœ‰æ–°èä¾†æº...")
        
        all_articles = []
        
        for source_id, source_config in self.sources.items():
            try:
                logger.info(f"ğŸ“° æ­£åœ¨çˆ¬å–: {source_config['name']}")
                articles = self._crawl_source(source_id, source_config, max_articles_per_source)
                
                if articles:
                    all_articles.extend(articles)
                    logger.info(f"âœ… {source_config['name']} æˆåŠŸç²å– {len(articles)} ç¯‡æ–‡ç« ")
                else:
                    logger.warning(f"âš ï¸ {source_config['name']} æœªç²å–åˆ°æ–‡ç« ")
                
                # è«‹æ±‚é–“éš”
                time.sleep(self.delay_between_requests)
                
            except Exception as e:
                logger.error(f"âŒ {source_config['name']} çˆ¬å–å¤±æ•—: {e}")
                continue
        
        logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆï¼Œç¸½å…±ç²å¾— {len(all_articles)} ç¯‡æ–‡ç« ")
        return all_articles
    
    def _crawl_source(self, source_id: str, config: Dict, max_articles: int) -> List[NewsArticle]:
        """çˆ¬å–å–®å€‹ä¾†æºçš„æ–°è"""
        try:
            response = self.session.get(config['insurance_url'], timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ä½¿ç”¨å°æ‡‰çš„è§£æå™¨
            parser = config['parser']
            articles = parser(soup, config, max_articles)
            
            return articles
            
        except Exception as e:
            logger.error(f"çˆ¬å– {config['name']} å¤±æ•—: {e}")
            return []
    
    def _parse_ctee_news(self, soup: BeautifulSoup, config: Dict, max_articles: int) -> List[NewsArticle]:
        """è§£æå·¥å•†æ™‚å ±æ–°è"""
        articles = []
        
        try:
            # æŸ¥æ‰¾æ–°èåˆ—è¡¨
            news_items = soup.select('.archive-list-item, .post-item, article')[:max_articles]
            
            for item in news_items:
                try:
                    # æ¨™é¡Œå’Œé€£çµ
                    title_elem = item.select_one('h2 a, h3 a, .title a, a[title]')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    url = urljoin(config['base_url'], title_elem.get('href', ''))
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚ºä¿éšªç›¸é—œ
                    if not self._is_insurance_related(title):
                        continue
                    
                    # æ‘˜è¦
                    summary_elem = item.select_one('.excerpt, .summary, p')
                    summary = summary_elem.get_text(strip=True) if summary_elem else title[:100]
                    
                    # æ™‚é–“
                    time_elem = item.select_one('.date, .time, time')
                    published_date = self._parse_date(time_elem.get_text(strip=True) if time_elem else '')
                    
                    # ç²å–å®Œæ•´å…§å®¹
                    content = self._fetch_article_content(url)
                    
                    article = NewsArticle(
                        title=title,
                        url=url,
                        summary=summary,
                        content=content,
                        published_date=published_date,
                        source=config['name'],
                        category="ä¿éšªæ–°è"
                    )
                    
                    articles.append(article)
                    logger.info(f"âœ… å·¥å•†æ™‚å ±: {title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"è§£æå·¥å•†æ™‚å ±æ–‡ç« å¤±æ•—: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"è§£æå·¥å•†æ™‚å ±é é¢å¤±æ•—: {e}")
        
        return articles
    
    def _parse_udn_news(self, soup: BeautifulSoup, config: Dict, max_articles: int) -> List[NewsArticle]:
        """è§£æç¶“æ¿Ÿæ—¥å ±æ–°è"""
        articles = []
        
        try:
            # æŸ¥æ‰¾æ–°èåˆ—è¡¨
            news_items = soup.select('.story-list__item, .listing__item, article')[:max_articles]
            
            for item in news_items:
                try:
                    title_elem = item.select_one('h2 a, h3 a, .story-list__title a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    url = urljoin(config['base_url'], title_elem.get('href', ''))
                    
                    if not self._is_insurance_related(title):
                        continue
                    
                    summary_elem = item.select_one('.story-list__summary, .summary')
                    summary = summary_elem.get_text(strip=True) if summary_elem else title[:100]
                    
                    time_elem = item.select_one('.story-list__time, .time')
                    published_date = self._parse_date(time_elem.get_text(strip=True) if time_elem else '')
                    
                    content = self._fetch_article_content(url)
                    
                    article = NewsArticle(
                        title=title,
                        url=url,
                        summary=summary,
                        content=content,
                        published_date=published_date,
                        source=config['name'],
                        category="ä¿éšªæ–°è"
                    )
                    
                    articles.append(article)
                    logger.info(f"âœ… ç¶“æ¿Ÿæ—¥å ±: {title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"è§£æç¶“æ¿Ÿæ—¥å ±æ–‡ç« å¤±æ•—: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"è§£æç¶“æ¿Ÿæ—¥å ±é é¢å¤±æ•—: {e}")
        
        return articles
    
    def _parse_chinatimes_news(self, soup: BeautifulSoup, config: Dict, max_articles: int) -> List[NewsArticle]:
        """è§£æä¸­æ™‚æ–°èç¶²è²¡ç¶“æ–°è"""
        articles = []
        
        try:
            news_items = soup.select('.article-list li, .news-list li')[:max_articles]
            
            for item in news_items:
                try:
                    title_elem = item.select_one('h3 a, h2 a, a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    url = urljoin(config['base_url'], title_elem.get('href', ''))
                    
                    if not self._is_insurance_related(title):
                        continue
                    
                    summary = title[:100]  # ç°¡åŒ–æ‘˜è¦
                    published_date = datetime.now()  # ç°¡åŒ–æ™‚é–“è™•ç†
                    
                    content = self._fetch_article_content(url)
                    
                    article = NewsArticle(
                        title=title,
                        url=url,
                        summary=summary,
                        content=content,
                        published_date=published_date,
                        source=config['name'],
                        category="ä¿éšªæ–°è"
                    )
                    
                    articles.append(article)
                    logger.info(f"âœ… ä¸­æ™‚è²¡ç¶“: {title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"è§£æä¸­æ™‚è²¡ç¶“æ–‡ç« å¤±æ•—: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"è§£æä¸­æ™‚è²¡ç¶“é é¢å¤±æ•—: {e}")
        
        return articles
    
    def _parse_ltn_news(self, soup: BeautifulSoup, config: Dict, max_articles: int) -> List[NewsArticle]:
        """è§£æè‡ªç”±æ™‚å ±è²¡ç¶“æ–°è"""
        articles = []
        
        try:
            news_items = soup.select('.content li, .list li')[:max_articles]
            
            for item in news_items:
                try:
                    title_elem = item.select_one('a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    url = urljoin(config['base_url'], title_elem.get('href', ''))
                    
                    if not self._is_insurance_related(title):
                        continue
                    
                    summary = title[:100]
                    published_date = datetime.now()
                    
                    content = self._fetch_article_content(url)
                    
                    article = NewsArticle(
                        title=title,
                        url=url,
                        summary=summary,
                        content=content,
                        published_date=published_date,
                        source=config['name'],
                        category="ä¿éšªæ–°è"
                    )
                    
                    articles.append(article)
                    logger.info(f"âœ… è‡ªç”±è²¡ç¶“: {title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"è§£æè‡ªç”±è²¡ç¶“æ–‡ç« å¤±æ•—: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"è§£æè‡ªç”±è²¡ç¶“é é¢å¤±æ•—: {e}")
        
        return articles
    
    def _is_insurance_related(self, title: str) -> bool:
        """æª¢æŸ¥æ¨™é¡Œæ˜¯å¦èˆ‡ä¿éšªç›¸é—œ"""
        insurance_keywords = [
            'ä¿éšª', 'ä¿è²»', 'ä¿å–®', 'ç†è³ ', 'æŠ•ä¿', 'æ‰¿ä¿',
            'å£½éšª', 'ç”¢éšª', 'è»Šéšª', 'å¥åº·éšª', 'æ„å¤–éšª', 'é†«ç™‚éšª',
            'ä¿éšªå…¬å¸', 'ä¿éšªæ¥­', 'ä¿éšªæ³•', 'ä¿éšªé‡‘',
            'é‡‘ç®¡æœƒ', 'ä¿éšªå±€', 'ä¿éšªå¸‚å ´', 'ä¿éšœ',
            'å—å±±', 'åœ‹æ³°', 'å¯Œé‚¦', 'æ–°å…‰', 'å°ç£äººå£½',
            'ä¸­åœ‹ä¿¡è¨—', 'ç¬¬ä¸€é‡‘', 'å…†è±', 'ç‰å±±'
        ]
        
        title_lower = title.lower()
        return any(keyword in title for keyword in insurance_keywords)
    
    def _fetch_article_content(self, url: str) -> str:
        """ç²å–æ–‡ç« å®Œæ•´å…§å®¹"""
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # é€šç”¨å…§å®¹é¸æ“‡å™¨
            content_selectors = [
                '.article-content',
                '.story-body',
                '.post-content',
                '.content',
                '.article-body',
                '.news-content',
                'main article',
                '.entry-content'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text(strip=True)
                    break
            
            # å¦‚æœæ²’æ‰¾åˆ°å…§å®¹ï¼Œå˜—è©¦æ‰€æœ‰pæ¨™ç±¤
            if not content:
                paragraphs = soup.select('p')
                content = ' '.join([p.get_text(strip=True) for p in paragraphs[:5]])
            
            # æ¸…ç†å…§å®¹
            content = re.sub(r'\s+', ' ', content)
            return content[:1000]  # é™åˆ¶é•·åº¦
            
        except Exception as e:
            logger.warning(f"ç²å–æ–‡ç« å…§å®¹å¤±æ•— {url}: {e}")
            return ""
    
    def _parse_date(self, date_str: str) -> datetime:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        try:
            # å˜—è©¦å„ç¨®æ—¥æœŸæ ¼å¼
            date_patterns = [
                r'(\d{4})-(\d{2})-(\d{2})',
                r'(\d{4})/(\d{2})/(\d{2})',
                r'(\d{2})-(\d{2})-(\d{4})',
                r'(\d{2})/(\d{2})/(\d{4})'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, date_str)
                if match:
                    groups = match.groups()
                    if len(groups[0]) == 4:  # YYYY-MM-DD format
                        return datetime(int(groups[0]), int(groups[1]), int(groups[2]))
                    else:  # DD-MM-YYYY format
                        return datetime(int(groups[2]), int(groups[1]), int(groups[0]))
            
            # å¦‚æœç„¡æ³•è§£æï¼Œè¿”å›ç•¶å‰æ™‚é–“
            return datetime.now()
            
        except Exception:
            return datetime.now()
    
    def save_to_database(self, articles: List[NewsArticle]) -> Dict[str, Any]:
        """å„²å­˜æ–‡ç« åˆ°è³‡æ–™åº«"""
        try:
            from app import create_app
            from database.models import News, NewsSource, NewsCategory, db
            from config.settings import Config
            
            app = create_app(Config)
            
            with app.app_context():
                saved_count = 0
                duplicate_count = 0
                
                for article in articles:
                    # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    existing = News.query.filter_by(title=article.title).first()
                    if existing:
                        duplicate_count += 1
                        continue
                    
                    # ç²å–æˆ–å‰µå»ºæ–°èä¾†æº
                    source = NewsSource.query.filter_by(name=article.source).first()
                    if not source:
                        source = NewsSource(
                            name=article.source,
                            url="",
                            status='active'
                        )
                        db.session.add(source)
                        db.session.flush()
                    
                    # ç²å–æˆ–å‰µå»ºåˆ†é¡
                    category = NewsCategory.query.filter_by(name=article.category).first()
                    if not category:
                        category = NewsCategory(
                            name=article.category,
                            description="ä¿éšªç›¸é—œæ–°è"
                        )
                        db.session.add(category)
                        db.session.flush()
                    
                    # å‰µå»ºæ–°èè¨˜éŒ„
                    news = News(
                        title=article.title,
                        content=article.content,
                        summary=article.summary,
                        url=article.url,
                        published_date=article.published_date,
                        source_id=source.id,
                        category_id=category.id,
                        status='active',
                        importance_score=0.5,
                        sentiment_score=0.0
                    )
                    
                    db.session.add(news)
                    saved_count += 1
                
                db.session.commit()
                
                result = {
                    'status': 'success',
                    'saved_count': saved_count,
                    'duplicate_count': duplicate_count,
                    'total_processed': len(articles)
                }
                
                logger.info(f"ğŸ’¾ è³‡æ–™åº«å„²å­˜å®Œæˆ: æ–°å¢ {saved_count} ç¯‡ï¼Œé‡è¤‡ {duplicate_count} ç¯‡")
                return result
                
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™åº«å„²å­˜å¤±æ•—: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'saved_count': 0
            }

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸ•·ï¸ å•Ÿå‹•å¤šä¾†æºæ–°èçˆ¬èŸ²ç³»çµ±...")
    
    crawler = MultiSourceNewsCrawler()
    
    # çˆ¬å–æ‰€æœ‰ä¾†æº
    articles = crawler.crawl_all_sources(max_articles_per_source=5)
    
    if articles:
        print(f"\nğŸ“° æˆåŠŸçˆ¬å– {len(articles)} ç¯‡ä¿éšªç›¸é—œæ–°è:")
        for i, article in enumerate(articles, 1):
            print(f"{i}. [{article.source}] {article.title[:60]}...")
        
        # å„²å­˜åˆ°è³‡æ–™åº«
        print("\nğŸ’¾ æ­£åœ¨å„²å­˜åˆ°è³‡æ–™åº«...")
        result = crawler.save_to_database(articles)
        
        print(f"\nğŸ“Š å„²å­˜çµæœ:")
        print(f"  ç‹€æ…‹: {result['status']}")
        print(f"  æ–°å¢: {result.get('saved_count', 0)} ç¯‡")
        print(f"  é‡è¤‡: {result.get('duplicate_count', 0)} ç¯‡")
        
    else:
        print("âš ï¸ æœªèƒ½ç²å–åˆ°ä»»ä½•æ–°èæ–‡ç« ")

if __name__ == "__main__":
    main()
