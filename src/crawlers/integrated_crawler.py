"""
æ•´åˆçœŸå¯¦æ–°èçˆ¬å–èˆ‡æ•¸æ“šæ¸…æ´—
Integrated Real News Crawling with Data Cleaning

æ•´åˆçœŸå¯¦æ–°èçˆ¬å–ã€æ•¸æ“šæ¸…æ´—ã€å»é‡å’Œè³‡æ–™åº«å„²å­˜çš„å®Œæ•´æµç¨‹
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import logging
from datetime import datetime, timezone
from typing import Dict, Any, List
import difflib

from crawler.real_crawler_fixed import RealInsuranceNewsCrawler
from database.models import News, NewsSource, NewsCategory, db
from app import create_app
from config.settings import Config
from analyzer.insurance_dictionary import (
    is_insurance_related,
    calculate_insurance_relevance_score,
    extract_insurance_keywords
)

logger = logging.getLogger('integrated_crawler')

class IntegratedNewsCrawler:
    """æ•´åˆæ–°èçˆ¬èŸ²ç³»çµ±"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•´åˆçˆ¬èŸ²"""
        self.app = create_app(Config)
        self.real_crawler = RealInsuranceNewsCrawler()
        self.similarity_threshold = 0.85
        self.min_content_length = 50
        self.min_insurance_score = 0.1
        
        logger.info("ğŸ”§ æ•´åˆæ–°èçˆ¬èŸ²ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    def clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬å…§å®¹"""
        if not text:
            return ""
        
        import re
        # ç§»é™¤å¤šé¤˜ç©ºç™½å’Œæ›è¡Œ
        text = re.sub(r'\s+', ' ', text.strip())
        # ç§»é™¤HTMLæ¨™ç±¤
        text = re.sub(r'<[^>]+>', '', text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\u4e00-\u9fff\w\s\.\,\!\?\:\;\-\(\)\[\]ã€Œã€ã€ã€ã€ã€‚ï¼Œï¼ï¼Ÿï¼šï¼›]', '', text)
        
        return text.strip()
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """è¨ˆç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0
        return difflib.SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
    
    def validate_news(self, news: Dict[str, Any]) -> tuple:
        """é©—è­‰æ–°èæœ‰æ•ˆæ€§"""
        errors = []
        
        title = news.get('title', '')
        content = news.get('content', '')
        
        if not title or len(title.strip()) < 5:
            errors.append("æ¨™é¡Œå¤ªçŸ­")
        
        if not content or len(content.strip()) < self.min_content_length:
            errors.append("å…§å®¹å¤ªçŸ­")
        
        # æª¢æŸ¥ä¿éšªç›¸é—œæ€§
        full_text = f"{title} {content}"
        insurance_score = calculate_insurance_relevance_score(full_text)
        if insurance_score < self.min_insurance_score:
            errors.append(f"ä¿éšªç›¸é—œæ€§éä½: {insurance_score:.3f}")
        
        return len(errors) == 0, errors
    
    def find_duplicates(self, news_list: List[Dict[str, Any]]) -> List[int]:
        """æ‰¾å‡ºé‡è¤‡æ–°èçš„ç´¢å¼•"""
        to_remove = set()
        
        for i in range(len(news_list)):
            if i in to_remove:
                continue
                
            for j in range(i + 1, len(news_list)):
                if j in to_remove:
                    continue
                
                title_similarity = self.calculate_similarity(
                    news_list[i].get('title', ''),
                    news_list[j].get('title', '')
                )
                
                if title_similarity >= self.similarity_threshold:
                    # ä¿ç•™è¼ƒæ–°çš„æˆ–å…§å®¹è¼ƒè±å¯Œçš„
                    if len(news_list[i].get('content', '')) >= len(news_list[j].get('content', '')):
                        to_remove.add(j)
                    else:
                        to_remove.add(i)
        
        return list(to_remove)
    
    def crawl_and_process(self) -> Dict[str, Any]:
        """çˆ¬å–ä¸¦è™•ç†æ–°èçš„å®Œæ•´æµç¨‹"""
        try:
            logger.info("ğŸš€ é–‹å§‹æ•´åˆæ–°èçˆ¬å–èˆ‡è™•ç†æµç¨‹...")
            
            # 1. çˆ¬å–çœŸå¯¦æ–°è
            print("ğŸ“¡ æ­¥é©Ÿ1: çˆ¬å–çœŸå¯¦æ–°è...")
            raw_news = self.real_crawler.crawl_all_sources()
            
            if not raw_news:
                return {
                    'status': 'no_news',
                    'message': 'æ²’æœ‰çˆ¬å–åˆ°æ–°è',
                    'stats': {'raw': 0, 'cleaned': 0, 'valid': 0, 'unique': 0, 'saved': 0}
                }
            
            print(f"âœ… çˆ¬å–åˆ° {len(raw_news)} å‰‡åŸå§‹æ–°è")
            
            # 2. æ¸…æ´—æ•¸æ“š
            print("ğŸ§¹ æ­¥é©Ÿ2: æ¸…æ´—æ–°èæ•¸æ“š...")
            cleaned_news = []
            for news in raw_news:
                cleaned_news.append({
                    'title': self.clean_text(news.get('title', '')),
                    'content': self.clean_text(news.get('content', '')),
                    'summary': self.clean_text(news.get('summary', '')),
                    'url': news.get('url', ''),
                    'source': news.get('source', ''),
                    'published_date': news.get('published_date', datetime.now(timezone.utc))
                })
            
            print(f"âœ… æ¸…æ´—å®Œæˆ")
            
            # 3. é©—è­‰æœ‰æ•ˆæ€§
            print("âœ… æ­¥é©Ÿ3: é©—è­‰æ–°èæœ‰æ•ˆæ€§...")
            valid_news = []
            invalid_count = 0
            
            for news in cleaned_news:
                is_valid, errors = self.validate_news(news)
                if is_valid:
                    valid_news.append(news)
                else:
                    invalid_count += 1
                    logger.debug(f"ç„¡æ•ˆæ–°è: {news.get('title', '')[:30]} - {errors}")
            
            print(f"âœ… æœ‰æ•ˆæ–°è: {len(valid_news)} å‰‡ï¼Œç„¡æ•ˆ: {invalid_count} å‰‡")
            
            # 4. å»é‡è™•ç†
            print("ğŸ” æ­¥é©Ÿ4: å»é‡è™•ç†...")
            duplicate_indices = self.find_duplicates(valid_news)
            unique_news = [news for i, news in enumerate(valid_news) if i not in duplicate_indices]
            
            print(f"âœ… å»é‡å®Œæˆï¼Œç§»é™¤ {len(duplicate_indices)} å‰‡é‡è¤‡ï¼Œå‰©é¤˜ {len(unique_news)} å‰‡")
            
            # 5. å„²å­˜åˆ°è³‡æ–™åº«
            print("ğŸ’¾ æ­¥é©Ÿ5: å„²å­˜åˆ°è³‡æ–™åº«...")
            saved_count = 0
            
            with self.app.app_context():
                for news in unique_news:
                    try:
                        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
                        existing = News.query.filter_by(title=news['title']).first()
                        if existing:
                            continue
                        
                        # å–å¾—æˆ–å»ºç«‹æ–°èä¾†æº
                        source = NewsSource.query.filter_by(name=news['source']).first()
                        if not source:
                            source = NewsSource(
                                name=news['source'],
                                url=news.get('url', ''),
                                description=f"ä¾†è‡ª{news['source']}çš„æ–°è"
                            )
                            db.session.add(source)
                            db.session.flush()
                        
                        # å–å¾—æˆ–å»ºç«‹æ–°èåˆ†é¡
                        category = NewsCategory.query.filter_by(name='ä¿éšª').first()
                        if not category:
                            category = NewsCategory(
                                name='ä¿éšª',
                                description='ä¿éšªç›¸é—œæ–°è'
                            )
                            db.session.add(category)
                            db.session.flush()
                        
                        # è¨ˆç®—ä¿éšªç›¸é—œæ€§å’Œé‡è¦æ€§
                        full_text = f"{news['title']} {news['content']}"
                        insurance_score = calculate_insurance_relevance_score(full_text)
                        importance_score = min(insurance_score * 1.2, 1.0)
                        
                        # å»ºç«‹æ–°èè¨˜éŒ„
                        article = News(
                            title=news['title'],
                            url=news['url'],
                            content=news['content'],
                            summary=news['summary'],
                            source_id=source.id,
                            category_id=category.id,
                            published_date=news['published_date'],
                            importance_score=importance_score
                        )
                        
                        db.session.add(article)
                        saved_count += 1
                        
                    except Exception as e:
                        logger.error(f"å„²å­˜æ–°èå¤±æ•—: {news.get('title', '')[:30]} - {e}")
                        continue
                
                try:
                    db.session.commit()
                    print(f"âœ… æˆåŠŸå„²å­˜ {saved_count} å‰‡æ–°èåˆ°è³‡æ–™åº«")
                except Exception as e:
                    db.session.rollback()
                    logger.error(f"è³‡æ–™åº«æäº¤å¤±æ•—: {e}")
                    saved_count = 0
            
            # 6. çµ±è¨ˆçµæœ
            stats = {
                'raw': len(raw_news),
                'cleaned': len(cleaned_news),
                'valid': len(valid_news),
                'unique': len(unique_news),
                'saved': saved_count,
                'invalid': invalid_count,
                'duplicates': len(duplicate_indices)
            }
            
            logger.info(f"ğŸ‰ æ•´åˆçˆ¬å–å®Œæˆ: {stats}")
            
            return {
                'status': 'success',
                'message': f'æˆåŠŸè™•ç† {saved_count} å‰‡æ–°è',
                'stats': stats,
                'timestamp': datetime.now(timezone.utc)
            }
            
        except Exception as e:
            logger.error(f"âŒ æ•´åˆçˆ¬å–å¤±æ•—: {e}")
            return {
                'status': 'error',
                'message': str(e),
                'stats': {'raw': 0, 'cleaned': 0, 'valid': 0, 'unique': 0, 'saved': 0}
            }
    
    def get_database_stats(self) -> Dict[str, Any]:
        """å–å¾—è³‡æ–™åº«çµ±è¨ˆ"""
        with self.app.app_context():
            total_news = News.query.count()
            
            # çµ±è¨ˆçœŸå¯¦æ–°èå’Œæ¨¡æ“¬æ–°è
            real_sources = NewsSource.query.filter(
                NewsSource.name.in_(['Googleæ–°è', 'è¯åˆæ–°èç¶²', 'è‡ªç”±æ™‚å ±è²¡ç¶“'])
            ).all()
            real_source_ids = [s.id for s in real_sources] if real_sources else []
            
            real_count = News.query.filter(News.source_id.in_(real_source_ids)).count() if real_source_ids else 0
            mock_count = total_news - real_count
            
            return {
                'total_news': total_news,
                'real_news': real_count,
                'mock_news': mock_count,
                'sources': len(NewsSource.query.all())
            }

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸš€ å•Ÿå‹•æ•´åˆæ–°èçˆ¬å–ç³»çµ±...")
    
    crawler = IntegratedNewsCrawler()
    
    # é¡¯ç¤ºç•¶å‰çµ±è¨ˆ
    print("\nğŸ“Š ç•¶å‰è³‡æ–™åº«ç‹€æ³:")
    stats = crawler.get_database_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # åŸ·è¡Œçˆ¬å–
    print("\nğŸ”„ é–‹å§‹åŸ·è¡Œæ•´åˆçˆ¬å–...")
    result = crawler.crawl_and_process()
    
    print(f"\nğŸ“‹ åŸ·è¡Œçµæœ:")
    print(f"  ç‹€æ…‹: {result['status']}")
    print(f"  è¨Šæ¯: {result['message']}")
    
    if 'stats' in result:
        print(f"  çµ±è¨ˆ:")
        for key, value in result['stats'].items():
            print(f"    {key}: {value}")
    
    # é¡¯ç¤ºæ›´æ–°å¾Œçµ±è¨ˆ
    print("\nğŸ“Š æ›´æ–°å¾Œè³‡æ–™åº«ç‹€æ³:")
    stats = crawler.get_database_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")

if __name__ == "__main__":
    main()
