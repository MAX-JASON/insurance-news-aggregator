"""
å°ç£ä¿éšªæ–°èå°ˆæ¥­çˆ¬èŸ²å¼•æ“
Taiwan Insurance News Professional Crawler Engine

å°ˆé–€çˆ¬å–å°ç£ä¿éšªæ¥­ç›¸é—œæ–°èçš„é«˜ç´šçˆ¬èŸ²ç³»çµ±
"""

import os
import sys
import requests
import logging
import time
import random
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import json
import re
from dataclasses import dataclass
import sqlite3

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger('taiwan_insurance_crawler')

@dataclass
class NewsArticle:
    """æ–°èæ–‡ç« æ•¸æ“šçµæ§‹"""
    title: str
    content: str
    summary: str
    url: str
    source: str
    category: str
    published_date: datetime
    author: Optional[str] = None
    tags: List[str] = None
    importance_score: float = 0.0

class TaiwanInsuranceCrawler:
    """å°ç£ä¿éšªæ–°èå°ˆæ¥­çˆ¬èŸ²"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬èŸ²"""
        
        # å°ç£ä¿éšªå°ˆæ¥­æ–°èæºé…ç½®
        self.news_sources = {
            # ä¿éšªå°ˆæ¥­é¡
            'goodins': {
                'name': 'ä¿éšªé›²é›œèªŒ',
                'base_url': 'https://www.goodins.life',
                'news_url': 'https://www.goodins.life/news',
                'category': 'ä¿éšªå°ˆæ¥­',
                'selectors': {
                    'title': '.article-title, h1, h2',
                    'content': '.article-content, .content, .post-content',
                    'summary': '.excerpt, .summary',
                    'date': '.date, .publish-time, time'
                }
            },
            'rmim': {
                'name': 'ä¿éšªäº‹æ¥­ç™¼å±•ä¸­å¿ƒ',
                'base_url': 'https://www.rmim.com.tw',
                'news_url': 'https://www.rmim.com.tw/news',
                'category': 'ä¿éšªå°ˆæ¥­',
                'selectors': {
                    'title': '.news-title, h1',
                    'content': '.news-content, .content',
                    'date': '.news-date, .date'
                }
            },
            'lia_roc': {
                'name': 'ä¸­è¯æ°‘åœ‹äººå£½ä¿éšªå•†æ¥­åŒæ¥­å…¬æœƒ',
                'base_url': 'https://www.lia-roc.org.tw',
                'news_url': 'https://www.lia-roc.org.tw/news',
                'category': 'ä¿éšªå…¬æœƒ',
                'selectors': {
                    'title': '.title, h1, h2',
                    'content': '.content, .article-content',
                    'date': '.date, .time'
                }
            },
            'tii': {
                'name': 'ä¸­è¯æ°‘åœ‹ç”¢ç‰©ä¿éšªå•†æ¥­åŒæ¥­å…¬æœƒ',
                'base_url': 'https://www.tii.org.tw',
                'news_url': 'https://www.tii.org.tw/news',
                'category': 'ä¿éšªå…¬æœƒ',
                'selectors': {
                    'title': '.news-title, h1',
                    'content': '.news-content, .content',
                    'date': '.news-date, .date'
                }
            },
            
            # è²¡ç¶“ç†è²¡é¡
            'udn_money': {
                'name': 'ç¶“æ¿Ÿæ—¥å ±ç†è²¡',
                'base_url': 'https://money.udn.com',
                'news_url': 'https://money.udn.com/money/cate/5591',
                'category': 'è²¡ç¶“ç†è²¡',
                'selectors': {
                    'title': '.story-list__text h3, h1',
                    'content': '.article-content__editor, #story_body',
                    'date': '.story-list__time, time',
                    'link': '.story-list__text a'
                }
            },
            'ctee': {
                'name': 'å·¥å•†æ™‚å ±è²¡ç¶“',
                'base_url': 'https://www.ctee.com.tw',
                'news_url': 'https://www.ctee.com.tw/livenews/fm',
                'category': 'è²¡ç¶“ç†è²¡',
                'selectors': {
                    'title': '.title a, h1',
                    'content': '.article-content, .content',
                    'date': '.date, time',
                    'link': '.title a'
                }
            },
            'wealth': {
                'name': 'è²¡è¨Šé›œèªŒ',
                'base_url': 'https://www.wealth.com.tw',
                'news_url': 'https://www.wealth.com.tw/lists/money',
                'category': 'è²¡ç¶“ç†è²¡',
                'selectors': {
                    'title': '.article-title, h1, h2',
                    'content': '.article-content, .content',
                    'date': '.article-date, .date'
                }
            },
            'businesstoday': {
                'name': 'ä»Šå‘¨åˆŠ',
                'base_url': 'https://www.businesstoday.com.tw',
                'news_url': 'https://www.businesstoday.com.tw/list-content.aspx?id=308021',
                'category': 'è²¡ç¶“ç†è²¡',
                'selectors': {
                    'title': '.article-title, h1',
                    'content': '.article-content, .content',
                    'date': '.article-date, .date'
                }
            },
            
            # é†«ç™‚é•·ç…§é¡  
            'heho': {
                'name': 'Hehoå¥åº·',
                'base_url': 'https://heho.com.tw',
                'news_url': 'https://heho.com.tw/category/é•·ç…§',
                'category': 'é†«ç™‚é•·ç…§',
                'selectors': {
                    'title': '.entry-title, h1',
                    'content': '.entry-content, .content',
                    'date': '.entry-date, time'
                }
            },
            'edh': {
                'name': 'æ—©å®‰å¥åº·',
                'base_url': 'https://www.edh.tw',
                'news_url': 'https://www.edh.tw/cancer',
                'category': 'é†«ç™‚é•·ç…§',
                'selectors': {
                    'title': '.article-title, h1',
                    'content': '.article-content, .content',
                    'date': '.article-date, .date'
                }
            },
            'commonhealth': {
                'name': 'åº·å¥é›œèªŒ',
                'base_url': 'https://www.commonhealth.com.tw',
                'news_url': 'https://www.commonhealth.com.tw/insurance',
                'category': 'é†«ç™‚é•·ç…§',
                'selectors': {
                    'title': '.article-title, h1',
                    'content': '.article-content, .content',
                    'date': '.article-date, .date'
                }
            }
        }
        
        # å°ç£ä¿éšªé—œéµè©ï¼ˆæ“´å……ç‰ˆï¼‰
        self.taiwan_insurance_keywords = [
            # åŸºæœ¬ä¿éšªè©å½™
            'ä¿éšª', 'ä¿è²»', 'ä¿å–®', 'ç†è³ ', 'æŠ•ä¿', 'æ‰¿ä¿', 'çºŒä¿', 'é€€ä¿',
            'å£½éšª', 'ç”¢éšª', 'è»Šéšª', 'å¥åº·éšª', 'æ„å¤–éšª', 'é†«ç™‚éšª', 'ç™Œç—‡éšª',
            'å¤±èƒ½éšª', 'é•·ç…§éšª', 'å¹´é‡‘éšª', 'å„²è“„éšª', 'æŠ•è³‡å‹ä¿å–®',
            
            # å°ç£ä¿éšªå…¬å¸
            'å—å±±äººå£½', 'åœ‹æ³°äººå£½', 'å¯Œé‚¦äººå£½', 'æ–°å…‰äººå£½', 'å°ç£äººå£½',
            'ä¸­åœ‹ä¿¡è¨—', 'ç¬¬ä¸€é‡‘äººå£½', 'å…†è±äººå£½', 'ç‰å±±äººå£½', 'å®æ³°äººå£½',
            'ä¿å¾·ä¿¡äººå£½', 'å®‰è¯äººå£½', 'ä¸‰å•†ç¾é‚¦', 'é é›„äººå£½', 'åº·å¥äººå£½',
            'æ–°å®‰æ±äº¬æµ·ä¸Š', 'å¯Œé‚¦ç”¢éšª', 'åœ‹æ³°ç”¢éšª', 'æ–°å…‰ç”¢éšª', 'å—å±±ç”¢éšª',
            
            # ç›£ç†æ©Ÿé—œ
            'é‡‘ç®¡æœƒ', 'ä¿éšªå±€', 'é‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒ', 'ä¿éšªç›£ç†',
            
            # æ³•è¦åˆ¶åº¦
            'ä¿éšªæ³•', 'ä¿éšªæ¥­æ³•', 'ä¿éšªä»£ç†äºº', 'ä¿éšªç¶“ç´€äºº', 'ä¿éšªå…¬è­‰äºº',
            'RBC', 'æ¸…å„Ÿèƒ½åŠ›', 'è³‡æœ¬é©è¶³ç‡', 'æº–å‚™é‡‘', 'è²¬ä»»æº–å‚™é‡‘',
            
            # å°ç£ç‰¹æœ‰
            'å…¨æ°‘å¥ä¿', 'å‹ä¿', 'å‹é€€', 'åœ‹æ°‘å¹´é‡‘', 'è¾²ä¿', 'è»å…¬æ•™ä¿éšª',
            'äºŒä»£å¥ä¿', 'è£œå……ä¿è²»', 'å¥ä¿å¡', 'å¥ä¿ç½²',
            
            # é•·ç…§ç›¸é—œ
            'é•·æœŸç…§é¡§', 'é•·ç…§2.0', 'å¤±æ™ºç—‡', 'å¤±èƒ½', 'å±…å®¶ç…§è­·', 'æ—¥ç…§ä¸­å¿ƒ',
            
            # ä¿éšªç§‘æŠ€
            'InsurTech', 'æ•¸ä½ä¿éšª', 'ç·šä¸ŠæŠ•ä¿', 'ä¿éšªç§‘æŠ€', 'AIç†è³ ',
            
            # æŠ•è³‡ç†è²¡
            'åˆ©è®Šå‹', 'åˆ†ç´…ä¿å–®', 'æŠ•è³‡é€£çµ', 'è¬èƒ½å£½éšª', 'è®Šé¡å¹´é‡‘'
        ]
        
        # User-Agent è¼ªæ›æ± 
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.59 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0'
        ]
        
        # è«‹æ±‚é…ç½®
        self.session = requests.Session()
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        logger.info(f"ğŸš€ å°ç£ä¿éšªæ–°èçˆ¬èŸ²åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æ´ {len(self.news_sources)} å€‹æ–°èæº")
    
    def get_random_user_agent(self) -> str:
        """ç²å–éš¨æ©Ÿ User-Agent"""
        return random.choice(self.user_agents)
    
    def make_request(self, url: str, retries: int = 3) -> Optional[requests.Response]:
        """ç™¼é€HTTPè«‹æ±‚ï¼ŒåŒ…å«é‡è©¦æ©Ÿåˆ¶"""
        for attempt in range(retries):
            try:
                # éš¨æ©Ÿå»¶é²
                time.sleep(random.uniform(1, 3))
                
                # è¨­ç½®éš¨æ©Ÿ User-Agent
                self.session.headers['User-Agent'] = self.get_random_user_agent()
                
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                
                logger.info(f"âœ… æˆåŠŸè«‹æ±‚: {url}")
                return response
                
            except requests.exceptions.RequestException as e:
                logger.warning(f"âš ï¸ è«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt + 1}/{retries}): {url} - {e}")
                if attempt < retries - 1:
                    time.sleep(random.uniform(2, 5))
                
        logger.error(f"âŒ è«‹æ±‚æœ€çµ‚å¤±æ•—: {url}")
        return None
    
    def extract_article_links(self, source_key: str) -> List[str]:
        """æå–æ–‡ç« é€£çµåˆ—è¡¨"""
        source = self.news_sources[source_key]
        news_url = source['news_url']
        
        logger.info(f"ğŸ“¡ æ­£åœ¨ç²å–æ–‡ç« åˆ—è¡¨: {source['name']}")
        
        response = self.make_request(news_url)
        if not response:
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        links = []
        
        # æ ¹æ“šä¸åŒç¶²ç«™èª¿æ•´é€£çµæå–é‚è¼¯
        if source_key == 'udn_money':
            # ç¶“æ¿Ÿæ—¥å ±ç‰¹æ®Šè™•ç†
            link_elements = soup.select('.story-list__text a, .story__headline a')
        elif source_key == 'ctee':
            # å·¥å•†æ™‚å ±ç‰¹æ®Šè™•ç†
            link_elements = soup.select('.title a, .headline a')
        else:
            # é€šç”¨è™•ç†
            link_elements = soup.select('a[href*="/news/"], a[href*="/article/"], a[href*="/post/"]')
        
        for link in link_elements:
            href = link.get('href')
            if href:
                # è™•ç†ç›¸å°é€£çµ
                if href.startswith('/'):
                    full_url = urljoin(source['base_url'], href)
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                
                # éæ¿¾æ‰éæ–°èé€£çµ
                if self.is_news_link(full_url):
                    links.append(full_url)
        
        logger.info(f"ğŸ“‹ {source['name']} ç™¼ç¾ {len(links)} å€‹æ–‡ç« é€£çµ")
        return links[:20]  # é™åˆ¶æ•¸é‡
    
    def is_news_link(self, url: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºæ–°èæ–‡ç« é€£çµ"""
        # æ’é™¤ä¸éœ€è¦çš„é€£çµ
        exclude_patterns = [
            '/tag/', '/category/', '/author/', '/search/',
            '/login', '/register', '/about', '/contact',
            '.pdf', '.jpg', '.png', '.gif', '/video/',
            'javascript:', 'mailto:', '#'
        ]
        
        for pattern in exclude_patterns:
            if pattern in url.lower():
                return False
        
        return True
    
    def extract_article_content(self, url: str, source_key: str) -> Optional[NewsArticle]:
        """æå–å–®ç¯‡æ–‡ç« å…§å®¹"""
        source = self.news_sources[source_key]
        
        response = self.make_request(url)
        if not response:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        selectors = source['selectors']
        
        try:
            # æå–æ¨™é¡Œ
            title_element = soup.select_one(selectors['title'])
            title = title_element.get_text(strip=True) if title_element else "ç„¡æ¨™é¡Œ"
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«ä¿éšªé—œéµè©
            if not self.contains_insurance_keywords(title):
                return None
            
            # æå–å…§å®¹
            content_elements = soup.select(selectors['content'])
            content = ""
            for element in content_elements:
                content += element.get_text(strip=True) + "\n"
            
            if not content.strip():
                logger.warning(f"âš ï¸ ç„¡æ³•æå–å…§å®¹: {url}")
                return None
            
            # æª¢æŸ¥å…§å®¹æ˜¯å¦åŒ…å«ä¿éšªé—œéµè©
            if not self.contains_insurance_keywords(content):
                return None
            
            # æå–æ‘˜è¦
            summary_element = soup.select_one(selectors.get('summary', ''))
            summary = summary_element.get_text(strip=True) if summary_element else content[:200]
            
            # æå–ç™¼å¸ƒæ—¥æœŸ
            date_element = soup.select_one(selectors.get('date', ''))
            published_date = self.parse_date(date_element.get_text(strip=True) if date_element else "")
            
            # è¨ˆç®—é‡è¦æ€§è©•åˆ†
            importance_score = self.calculate_importance_score(title, content)
            
            return NewsArticle(
                title=title,
                content=content.strip(),
                summary=summary,
                url=url,
                source=source['name'],
                category=source['category'],
                published_date=published_date,
                importance_score=importance_score
            )
            
        except Exception as e:
            logger.error(f"âŒ å…§å®¹æå–å¤±æ•—: {url} - {e}")
            return None
    
    def contains_insurance_keywords(self, text: str) -> bool:
        """æª¢æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¿éšªé—œéµè©"""
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in self.taiwan_insurance_keywords)
    
    def parse_date(self, date_str: str) -> datetime:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return datetime.now()
        
        # å¸¸è¦‹çš„å°ç£æ—¥æœŸæ ¼å¼
        date_patterns = [
            r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})',  # 2024/12/15 æˆ– 2024-12-15
            r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',  # 15/12/2024
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',      # 2024å¹´12æœˆ15æ—¥
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, date_str)
            if match:
                try:
                    groups = match.groups()
                    if len(groups) == 3:
                        # åˆ¤æ–·å¹´æœˆæ—¥é †åº
                        if len(groups[0]) == 4:  # å¹´åœ¨å‰
                            year, month, day = map(int, groups)
                        else:  # å¹´åœ¨å¾Œ
                            day, month, year = map(int, groups)
                        
                        return datetime(year, month, day)
                except ValueError:
                    continue
        
        return datetime.now()
    
    def calculate_importance_score(self, title: str, content: str) -> float:
        """è¨ˆç®—æ–°èé‡è¦æ€§è©•åˆ†"""
        score = 0.0
        
        # æ¨™é¡Œä¸­çš„é—œéµè©æ¬Šé‡æ›´é«˜
        for keyword in self.taiwan_insurance_keywords:
            if keyword in title:
                score += 0.3
            if keyword in content:
                score += 0.1
        
        # ç‰¹åˆ¥é‡è¦çš„é—œéµè©
        important_keywords = ['é‡‘ç®¡æœƒ', 'ä¿éšªå±€', 'æ³•è¦', 'ä¿®æ³•', 'é‡å¤§', 'ç·Šæ€¥', 'è­¦ç¤º']
        for keyword in important_keywords:
            if keyword in title:
                score += 0.5
            if keyword in content:
                score += 0.2
        
        return min(score, 1.0)  # æœ€å¤§å€¼ç‚º1.0
    
    def crawl_source(self, source_key: str, max_articles: int = 10) -> List[NewsArticle]:
        """çˆ¬å–æŒ‡å®šæ–°èæº"""
        logger.info(f"ğŸ•·ï¸ é–‹å§‹çˆ¬å–: {self.news_sources[source_key]['name']}")
        
        articles = []
        
        try:
            # ç²å–æ–‡ç« é€£çµ
            links = self.extract_article_links(source_key)
            
            # çˆ¬å–æ–‡ç« å…§å®¹
            for i, link in enumerate(links[:max_articles]):
                logger.info(f"ğŸ“° è™•ç†æ–‡ç«  {i+1}/{min(len(links), max_articles)}: {link}")
                
                article = self.extract_article_content(link, source_key)
                if article:
                    articles.append(article)
                    logger.info(f"âœ… æˆåŠŸæå–: {article.title}")
                
                # éš¨æ©Ÿå»¶é²é¿å…è¢«å°
                time.sleep(random.uniform(2, 4))
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–å¤±æ•—: {self.news_sources[source_key]['name']} - {e}")
        
        logger.info(f"ğŸ‰ {self.news_sources[source_key]['name']} å®Œæˆï¼Œç²å¾— {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    
    def crawl_all_sources(self, max_articles_per_source: int = 5) -> List[NewsArticle]:
        """çˆ¬å–æ‰€æœ‰æ–°èæº"""
        logger.info("ğŸš€ é–‹å§‹å…¨æºçˆ¬å–...")
        
        all_articles = []
        
        for source_key in self.news_sources:
            try:
                articles = self.crawl_source(source_key, max_articles_per_source)
                all_articles.extend(articles)
                
                # æºé–“å»¶é²
                time.sleep(random.uniform(5, 10))
                
            except Exception as e:
                logger.error(f"âŒ æºçˆ¬å–å¤±æ•—: {source_key} - {e}")
                continue
        
        logger.info(f"ğŸ‰ å…¨æºçˆ¬å–å®Œæˆï¼Œç¸½å…±ç²å¾— {len(all_articles)} ç¯‡æ–‡ç« ")
        return all_articles
    
    def save_to_database(self, articles: List[NewsArticle]) -> Dict[str, Any]:
        """å„²å­˜æ–‡ç« åˆ°è³‡æ–™åº«"""
        try:
            base_dir = os.path.dirname(os.path.abspath(__file__))
            db_path = os.path.join(base_dir, "instance", "insurance_news.db")
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            saved_count = 0
            duplicate_count = 0
            
            for article in articles:
                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
                cursor.execute("SELECT id FROM news WHERE url = ?", (article.url,))
                if cursor.fetchone():
                    duplicate_count += 1
                    continue
                
                # æ’å…¥æ–°æ–‡ç« 
                cursor.execute("""
                    INSERT INTO news (
                        title, content, summary, url, source_id, category_id,
                        published_date, crawled_date, importance_score,
                        status, created_at, updated_at
                    ) VALUES (?, ?, ?, ?, 1, 1, ?, ?, ?, 'active', ?, ?)
                """, (
                    article.title,
                    article.content,
                    article.summary,
                    article.url,
                    article.published_date,
                    datetime.now(),
                    article.importance_score,
                    datetime.now(),
                    datetime.now()
                ))
                
                saved_count += 1
            
            conn.commit()
            conn.close()
            
            logger.info(f"ğŸ’¾ è³‡æ–™åº«å„²å­˜å®Œæˆ: æ–°å¢ {saved_count} ç¯‡ï¼Œé‡è¤‡ {duplicate_count} ç¯‡")
            
            return {
                'status': 'success',
                'saved_count': saved_count,
                'duplicate_count': duplicate_count
            }
            
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™åº«å„²å­˜å¤±æ•—: {e}")
            return {
                'status': 'error',
                'error': str(e)
            }

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸ‡¹ğŸ‡¼ å°ç£ä¿éšªæ–°èå°ˆæ¥­çˆ¬èŸ²å¼•æ“å•Ÿå‹•")
    print("=" * 50)
    
    crawler = TaiwanInsuranceCrawler()
    
    # çˆ¬å–æ‰€æœ‰æ–°èæº
    articles = crawler.crawl_all_sources(max_articles_per_source=3)
    
    if articles:
        print(f"\nğŸ“Š çˆ¬å–çµæœ:")
        print(f"  ç¸½æ–‡ç« æ•¸: {len(articles)} ç¯‡")
        
        # æŒ‰ä¾†æºçµ±è¨ˆ
        source_stats = {}
        for article in articles:
            source_stats[article.source] = source_stats.get(article.source, 0) + 1
        
        print(f"  ä¾†æºåˆ†å¸ƒ:")
        for source, count in source_stats.items():
            print(f"    {source}: {count} ç¯‡")
        
        # å„²å­˜åˆ°è³‡æ–™åº«
        print(f"\nğŸ’¾ å„²å­˜åˆ°è³‡æ–™åº«...")
        result = crawler.save_to_database(articles)
        
        if result['status'] == 'success':
            print(f"âœ… å„²å­˜æˆåŠŸ: æ–°å¢ {result['saved_count']} ç¯‡ï¼Œé‡è¤‡ {result['duplicate_count']} ç¯‡")
        else:
            print(f"âŒ å„²å­˜å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
    
    else:
        print("âš ï¸ æœªèƒ½ç²å–åˆ°ä»»ä½•æ–‡ç« ")

if __name__ == "__main__":
    main()
