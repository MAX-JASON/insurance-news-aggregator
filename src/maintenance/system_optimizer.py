"""
ç³»çµ±å„ªåŒ–èˆ‡ç›£æ§è…³æœ¬
System Optimization and Monitoring Script

æ ¹æ“šå¯¦æ–½è¨ˆåŠƒé€²è¡Œç³»çµ±å„ªåŒ–ã€éŒ¯èª¤è™•ç†æ”¹å–„å’Œæ•ˆèƒ½ç›£æ§
"""

import os
import sys
import sqlite3
import logging
import time
import json
from datetime import datetime, timedelta
from typing import Dict, Any, List
from pathlib import Path

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger('system_optimizer')

class SystemOptimizer:
    """ç³»çµ±å„ªåŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å„ªåŒ–å™¨"""
        self.db_path = "instance/insurance_news.db"
        self.logs_dir = "logs"
        self.optimization_report = {
            'timestamp': datetime.now().isoformat(),
            'optimizations': [],
            'errors': [],
            'performance': {}
        }
        logger.info("ğŸ”§ ç³»çµ±å„ªåŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def optimize_database(self):
        """å„ªåŒ–è³‡æ–™åº«"""
        logger.info("ğŸ“Š é–‹å§‹è³‡æ–™åº«å„ªåŒ–...")
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æª¢æŸ¥ä¸¦å‰µå»ºç´¢å¼•
            self._create_database_indexes(cursor)
            
            # æ¸…ç†é‡è¤‡æ•¸æ“š
            self._clean_duplicate_data(cursor)
            
            # å„ªåŒ–è³‡æ–™åº«
            cursor.execute("VACUUM")
            cursor.execute("ANALYZE")
            
            conn.commit()
            conn.close()
            
            self.optimization_report['optimizations'].append({
                'type': 'database',
                'action': 'è³‡æ–™åº«å„ªåŒ–ã€ç´¢å¼•å‰µå»ºã€é‡è¤‡æ•¸æ“šæ¸…ç†',
                'status': 'success'
            })
            
            logger.info("âœ… è³‡æ–™åº«å„ªåŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™åº«å„ªåŒ–å¤±æ•—: {e}")
            self.optimization_report['errors'].append({
                'type': 'database',
                'error': str(e)
            })
    
    def _create_database_indexes(self, cursor):
        """å‰µå»ºè³‡æ–™åº«ç´¢å¼•"""
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_news_published_date ON news(published_date)",
            "CREATE INDEX IF NOT EXISTS idx_news_status ON news(status)",
            "CREATE INDEX IF NOT EXISTS idx_news_source_id ON news(source_id)",
            "CREATE INDEX IF NOT EXISTS idx_news_category_id ON news(category_id)",
            "CREATE INDEX IF NOT EXISTS idx_news_title ON news(title)",
            "CREATE INDEX IF NOT EXISTS idx_news_created_at ON news(created_at)",
            "CREATE INDEX IF NOT EXISTS idx_news_importance_score ON news(importance_score)",
            "CREATE INDEX IF NOT EXISTS idx_news_sentiment_score ON news(sentiment_score)"
        ]
        
        for index_sql in indexes:
            try:
                cursor.execute(index_sql)
                logger.info(f"âœ… ç´¢å¼•å‰µå»ºæˆåŠŸ: {index_sql.split('IF NOT EXISTS')[1].split('ON')[0].strip()}")
            except Exception as e:
                logger.warning(f"âš ï¸ ç´¢å¼•å‰µå»ºå¤±æ•—: {e}")
    
    def _clean_duplicate_data(self, cursor):
        """æ¸…ç†é‡è¤‡æ•¸æ“š"""
        # æŸ¥æ‰¾é‡è¤‡æ¨™é¡Œçš„æ–°è
        cursor.execute("""
            SELECT title, COUNT(*) as count 
            FROM news 
            GROUP BY title 
            HAVING count > 1
        """)
        
        duplicates = cursor.fetchall()
        
        if duplicates:
            logger.info(f"ç™¼ç¾ {len(duplicates)} çµ„é‡è¤‡æ–°è")
            
            for title, count in duplicates:
                # ä¿ç•™æœ€æ–°çš„ä¸€æ¢ï¼Œåˆªé™¤å…¶ä»–é‡è¤‡çš„
                cursor.execute("""
                    DELETE FROM news 
                    WHERE title = ? AND id NOT IN (
                        SELECT id FROM news 
                        WHERE title = ? 
                        ORDER BY created_at DESC 
                        LIMIT 1
                    )
                """, (title, title))
                
                deleted_count = cursor.rowcount
                if deleted_count > 0:
                    logger.info(f"åˆªé™¤é‡è¤‡æ–°è: {title} (åˆªé™¤ {deleted_count} æ¢)")
    
    def check_system_health(self):
        """æª¢æŸ¥ç³»çµ±å¥åº·ç‹€æ…‹"""
        logger.info("ğŸ¥ é–‹å§‹ç³»çµ±å¥åº·æª¢æŸ¥...")
        
        health_report = {
            'database': self._check_database_health(),
            'files': self._check_file_system(),
            'logs': self._check_log_files(),
            'dependencies': self._check_dependencies()
        }
        
        self.optimization_report['performance']['health_check'] = health_report
        
        # è¨ˆç®—ç¸½é«”å¥åº·åˆ†æ•¸
        total_score = 0
        max_score = 0
        
        for category, status in health_report.items():
            if isinstance(status, dict) and 'score' in status:
                total_score += status['score']
                max_score += 100
        
        overall_score = (total_score / max_score * 100) if max_score > 0 else 0
        
        logger.info(f"ğŸ“Š ç³»çµ±å¥åº·åˆ†æ•¸: {overall_score:.1f}/100")
        
        return health_report
    
    def _check_database_health(self):
        """æª¢æŸ¥è³‡æ–™åº«å¥åº·ç‹€æ…‹"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æª¢æŸ¥è¡¨æ ¼å®Œæ•´æ€§
            cursor.execute("PRAGMA integrity_check")
            integrity = cursor.fetchone()[0]
            
            # æª¢æŸ¥æ•¸æ“šé‡
            cursor.execute("SELECT COUNT(*) FROM news")
            news_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM news_source")
            sources_count = cursor.fetchone()[0]
            
            # æª¢æŸ¥æœ€æ–°æ•¸æ“šæ™‚é–“
            cursor.execute("SELECT MAX(created_at) FROM news")
            latest_news = cursor.fetchone()[0]
            
            conn.close()
            
            # è¨ˆç®—åˆ†æ•¸
            score = 100
            if integrity != 'ok':
                score -= 50
            if news_count < 10:
                score -= 20
            if sources_count < 1:
                score -= 20
            
            if latest_news:
                latest_date = datetime.fromisoformat(latest_news.replace('Z', '+00:00'))
                hours_old = (datetime.now() - latest_date.replace(tzinfo=None)).total_seconds() / 3600
                if hours_old > 24:
                    score -= 10
            
            return {
                'status': 'healthy' if score >= 80 else 'warning' if score >= 60 else 'critical',
                'score': max(0, score),
                'details': {
                    'integrity': integrity,
                    'news_count': news_count,
                    'sources_count': sources_count,
                    'latest_news': latest_news
                }
            }
            
        except Exception as e:
            logger.error(f"è³‡æ–™åº«å¥åº·æª¢æŸ¥å¤±æ•—: {e}")
            return {
                'status': 'critical',
                'score': 0,
                'error': str(e)
            }
    
    def _check_file_system(self):
        """æª¢æŸ¥æ–‡ä»¶ç³»çµ±"""
        try:
            required_dirs = ['logs', 'instance', 'web/static', 'web/templates']
            required_files = ['run.py', 'config/settings.py', 'database/models.py']
            
            missing_dirs = []
            missing_files = []
            
            for dir_path in required_dirs:
                if not os.path.exists(dir_path):
                    missing_dirs.append(dir_path)
            
            for file_path in required_files:
                if not os.path.exists(file_path):
                    missing_files.append(file_path)
            
            score = 100
            score -= len(missing_dirs) * 10
            score -= len(missing_files) * 15
            
            return {
                'status': 'healthy' if score >= 80 else 'warning' if score >= 60 else 'critical',
                'score': max(0, score),
                'missing_dirs': missing_dirs,
                'missing_files': missing_files
            }
            
        except Exception as e:
            return {
                'status': 'critical',
                'score': 0,
                'error': str(e)
            }
    
    def _check_log_files(self):
        """æª¢æŸ¥æ—¥èªŒæ–‡ä»¶"""
        try:
            if not os.path.exists(self.logs_dir):
                return {
                    'status': 'warning',
                    'score': 50,
                    'message': 'æ—¥èªŒç›®éŒ„ä¸å­˜åœ¨'
                }
            
            log_files = list(Path(self.logs_dir).glob('*.log'))
            
            # æª¢æŸ¥æ—¥èªŒæ–‡ä»¶å¤§å°
            large_files = []
            for log_file in log_files:
                size_mb = log_file.stat().st_size / 1024 / 1024
                if size_mb > 100:  # è¶…é100MB
                    large_files.append(f"{log_file.name}: {size_mb:.1f}MB")
            
            score = 100
            if len(log_files) == 0:
                score -= 30
            score -= len(large_files) * 10
            
            return {
                'status': 'healthy' if score >= 80 else 'warning',
                'score': max(0, score),
                'log_files_count': len(log_files),
                'large_files': large_files
            }
            
        except Exception as e:
            return {
                'status': 'critical',
                'score': 0,
                'error': str(e)
            }
    
    def _check_dependencies(self):
        """æª¢æŸ¥ä¾è³´é …"""
        try:
            required_packages = [
                'flask', 'sqlalchemy', 'requests', 'beautifulsoup4',
                'jieba', 'textblob', 'feedparser', 'schedule'
            ]
            
            missing_packages = []
            
            for package in required_packages:
                try:
                    __import__(package)
                except ImportError:
                    missing_packages.append(package)
            
            score = 100 - len(missing_packages) * 12.5
            
            return {
                'status': 'healthy' if score >= 80 else 'warning' if score >= 60 else 'critical',
                'score': max(0, score),
                'missing_packages': missing_packages,
                'checked_packages': len(required_packages)
            }
            
        except Exception as e:
            return {
                'status': 'critical',
                'score': 0,
                'error': str(e)
            }
    
    def optimize_logs(self):
        """å„ªåŒ–æ—¥èªŒæ–‡ä»¶"""
        logger.info("ğŸ“ é–‹å§‹æ—¥èªŒå„ªåŒ–...")
        
        try:
            if not os.path.exists(self.logs_dir):
                os.makedirs(self.logs_dir)
                logger.info("âœ… å‰µå»ºæ—¥èªŒç›®éŒ„")
            
            # å£“ç¸®å¤§å‹æ—¥èªŒæ–‡ä»¶
            log_files = list(Path(self.logs_dir).glob('*.log'))
            optimized_count = 0
            
            for log_file in log_files:
                size_mb = log_file.stat().st_size / 1024 / 1024
                
                # å¦‚æœæ—¥èªŒæ–‡ä»¶è¶…é50MBï¼Œå‰µå»ºå‚™ä»½ä¸¦æ¸…ç©º
                if size_mb > 50:
                    backup_name = f"{log_file.stem}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
                    backup_path = log_file.parent / backup_name
                    
                    # é‡å‘½ååŸæ–‡ä»¶ç‚ºå‚™ä»½
                    log_file.rename(backup_path)
                    
                    # å‰µå»ºæ–°çš„ç©ºæ—¥èªŒæ–‡ä»¶
                    log_file.touch()
                    
                    logger.info(f"âœ… å„ªåŒ–æ—¥èªŒæ–‡ä»¶: {log_file.name} ({size_mb:.1f}MB)")
                    optimized_count += 1
            
            self.optimization_report['optimizations'].append({
                'type': 'logs',
                'action': f'å„ªåŒ– {optimized_count} å€‹æ—¥èªŒæ–‡ä»¶',
                'status': 'success'
            })
            
            logger.info(f"âœ… æ—¥èªŒå„ªåŒ–å®Œæˆï¼Œè™•ç† {optimized_count} å€‹æ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"âŒ æ—¥èªŒå„ªåŒ–å¤±æ•—: {e}")
            self.optimization_report['errors'].append({
                'type': 'logs',
                'error': str(e)
            })
    
    def performance_benchmark(self):
        """æ•ˆèƒ½åŸºæº–æ¸¬è©¦"""
        logger.info("âš¡ é–‹å§‹æ•ˆèƒ½åŸºæº–æ¸¬è©¦...")
        
        try:
            # è³‡æ–™åº«æŸ¥è©¢æ•ˆèƒ½æ¸¬è©¦
            db_performance = self._test_database_performance()
            
            # æ–‡ä»¶I/Oæ•ˆèƒ½æ¸¬è©¦
            io_performance = self._test_io_performance()
            
            performance_data = {
                'database': db_performance,
                'io': io_performance,
                'timestamp': datetime.now().isoformat()
            }
            
            self.optimization_report['performance']['benchmark'] = performance_data
            
            logger.info("âœ… æ•ˆèƒ½åŸºæº–æ¸¬è©¦å®Œæˆ")
            return performance_data
            
        except Exception as e:
            logger.error(f"âŒ æ•ˆèƒ½åŸºæº–æ¸¬è©¦å¤±æ•—: {e}")
            return {'error': str(e)}
    
    def _test_database_performance(self):
        """æ¸¬è©¦è³‡æ–™åº«æ•ˆèƒ½"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æ¸¬è©¦ç°¡å–®æŸ¥è©¢
            start_time = time.time()
            cursor.execute("SELECT COUNT(*) FROM news")
            cursor.fetchone()
            simple_query_time = time.time() - start_time
            
            # æ¸¬è©¦è¤‡é›œæŸ¥è©¢
            start_time = time.time()
            cursor.execute("""
                SELECT n.title, s.name, COUNT(*) as count
                FROM news n 
                LEFT JOIN news_source s ON n.source_id = s.id 
                WHERE n.status = 'active' 
                GROUP BY s.name 
                ORDER BY count DESC 
                LIMIT 10
            """)
            cursor.fetchall()
            complex_query_time = time.time() - start_time
            
            conn.close()
            
            return {
                'simple_query_ms': round(simple_query_time * 1000, 2),
                'complex_query_ms': round(complex_query_time * 1000, 2),
                'status': 'good' if complex_query_time < 0.1 else 'slow'
            }
            
        except Exception as e:
            return {'error': str(e)}
    
    def _test_io_performance(self):
        """æ¸¬è©¦I/Oæ•ˆèƒ½"""
        try:
            test_file = "temp_performance_test.txt"
            test_data = "æ¸¬è©¦æ•¸æ“š" * 1000
            
            # å¯«å…¥æ¸¬è©¦
            start_time = time.time()
            with open(test_file, 'w', encoding='utf-8') as f:
                f.write(test_data)
            write_time = time.time() - start_time
            
            # è®€å–æ¸¬è©¦
            start_time = time.time()
            with open(test_file, 'r', encoding='utf-8') as f:
                f.read()
            read_time = time.time() - start_time
            
            # æ¸…ç†æ¸¬è©¦æ–‡ä»¶
            os.remove(test_file)
            
            return {
                'write_ms': round(write_time * 1000, 2),
                'read_ms': round(read_time * 1000, 2),
                'status': 'good' if write_time < 0.01 else 'slow'
            }
            
        except Exception as e:
            return {'error': str(e)}
    
    def generate_optimization_report(self):
        """ç”Ÿæˆå„ªåŒ–å ±å‘Š"""
        report_file = f"optimization_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(self.optimization_report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“‹ å„ªåŒ–å ±å‘Šå·²ç”Ÿæˆ: {report_file}")
            return report_file
            
        except Exception as e:
            logger.error(f"âŒ å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
            return None
    
    def run_full_optimization(self):
        """åŸ·è¡Œå®Œæ•´å„ªåŒ–"""
        logger.info("ğŸš€ é–‹å§‹ç³»çµ±å®Œæ•´å„ªåŒ–...")
        
        start_time = time.time()
        
        # åŸ·è¡Œå„é …å„ªåŒ–
        self.optimize_database()
        self.optimize_logs()
        
        # å¥åº·æª¢æŸ¥
        health_status = self.check_system_health()
        
        # æ•ˆèƒ½æ¸¬è©¦
        performance_data = self.performance_benchmark()
        
        # è¨˜éŒ„ç¸½åŸ·è¡Œæ™‚é–“
        total_time = time.time() - start_time
        self.optimization_report['execution_time_seconds'] = round(total_time, 2)
        
        # ç”Ÿæˆå ±å‘Š
        report_file = self.generate_optimization_report()
        
        logger.info(f"ğŸ‰ ç³»çµ±å„ªåŒ–å®Œæˆï¼è€—æ™‚ {total_time:.2f} ç§’")
        
        return {
            'status': 'success',
            'execution_time': total_time,
            'report_file': report_file,
            'health_status': health_status,
            'performance': performance_data
        }

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸ”§ å•Ÿå‹•ç³»çµ±å„ªåŒ–èˆ‡ç›£æ§...")
    
    optimizer = SystemOptimizer()
    result = optimizer.run_full_optimization()
    
    print(f"\nğŸ“‹ å„ªåŒ–çµæœ:")
    print(f"  ç‹€æ…‹: {result['status']}")
    print(f"  åŸ·è¡Œæ™‚é–“: {result['execution_time']:.2f} ç§’")
    print(f"  å ±å‘Šæ–‡ä»¶: {result['report_file']}")
    
    # é¡¯ç¤ºå¥åº·ç‹€æ…‹æ‘˜è¦
    health = result['health_status']
    print(f"\nğŸ¥ ç³»çµ±å¥åº·ç‹€æ…‹:")
    for component, status in health.items():
        if isinstance(status, dict):
            score = status.get('score', 0)
            state = status.get('status', 'unknown')
            print(f"  {component}: {state} ({score}/100)")

if __name__ == "__main__":
    main()
