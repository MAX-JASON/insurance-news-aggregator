#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
çˆ¬èŸ²æ¨¡çµ„æ¸¬è©¦
Crawler Module Test

æ¸¬è©¦çœŸå¯¦çˆ¬èŸ²åŠŸèƒ½èˆ‡ç®¡ç†å™¨æ•´åˆ
"""

import os
import sys
import logging
import json
from datetime import datetime
import time

# è¨­ç½®è·¯å¾‘
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = current_dir
sys.path.insert(0, project_root)

# è¨­ç½®è©³ç´°æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join(project_root, 'logs', 'crawler_test.log'), encoding='utf-8')
    ]
)
logger = logging.getLogger('crawler_test')

def test_crawlers():
    """æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½"""
    from crawler.manager import CrawlerManager
    
    print("\n" + "="*60)
    print("ğŸ§ª çˆ¬èŸ²åŠŸèƒ½æ¸¬è©¦é–‹å§‹")
    print("="*60)
    
    # åˆå§‹åŒ–çˆ¬èŸ²ç®¡ç†å™¨
    manager = CrawlerManager()
    
    print("\nğŸ“‹ å¯ç”¨çˆ¬èŸ²æ¸…å–®:")
    for name in manager.crawlers.keys():
        print(f"  - {name}")
    
    # 1. æ¸¬è©¦æ¨¡æ“¬æ•¸æ“šç”Ÿæˆ
    print("\n\nğŸ¤– æ¸¬è©¦1: æ¨¡æ“¬æ•¸æ“šç”Ÿæˆ")
    try:
        mock_result = manager.run_crawlers(source_name='mock', limit=5)
        mock_news = mock_result.get('news', [])
        print(f"âœ… æ¨¡æ“¬æ•¸æ“šç”ŸæˆæˆåŠŸ: {len(mock_news)} å‰‡æ–°è")
        for i, news in enumerate(mock_news[:2], 1):
            print(f"  ğŸ“° æ¨¡æ“¬æ–°è {i}: {news.get('title', 'ç„¡æ¨™é¡Œ')}")
    except Exception as e:
        print(f"âŒ æ¨¡æ“¬æ•¸æ“šç”Ÿæˆå¤±æ•—: {e}")
    
    # 2. æ¸¬è©¦çœŸå¯¦çˆ¬èŸ²
    print("\n\nğŸ” æ¸¬è©¦2: çœŸå¯¦çˆ¬èŸ²")
    if 'real' in manager.crawlers:
        try:
            print("  æ­£åœ¨åŸ·è¡ŒçœŸå¯¦çˆ¬èŸ²...")
            real_crawler = manager.crawlers['real']
            real_news = real_crawler.crawl_google_news()
            print(f"  âœ… Googleæ–°èçˆ¬å–æˆåŠŸ: {len(real_news)} å‰‡æ–°è")
            for i, news in enumerate(real_news[:2], 1):
                print(f"    ğŸ“° çœŸå¯¦æ–°è {i}: {news.get('title', 'ç„¡æ¨™é¡Œ')}")
        except Exception as e:
            print(f"  âŒ çœŸå¯¦çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
    else:
        print("  âš ï¸ çœŸå¯¦çˆ¬èŸ²æœªåœ¨ç®¡ç†å™¨ä¸­è¨»å†Š")
    
    # 3. æ¸¬è©¦RSSçˆ¬èŸ²
    print("\n\nğŸ“¡ æ¸¬è©¦3: RSSçˆ¬èŸ²")
    if 'rss' in manager.crawlers:
        try:
            print("  æ­£åœ¨åŸ·è¡ŒRSSçˆ¬èŸ²...")
            rss_crawler = manager.crawlers['rss']
            rss_news = rss_crawler.crawl_all_feeds()
            print(f"  âœ… RSSçˆ¬å–æˆåŠŸ: {len(rss_news)} å‰‡æ–°è")
            for i, news in enumerate(rss_news[:2], 1):
                print(f"    ğŸ“° RSSæ–°è {i}: {news.get('title', 'ç„¡æ¨™é¡Œ')}")
        except Exception as e:
            print(f"  âŒ RSSçˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
    else:
        print("  âš ï¸ RSSçˆ¬èŸ²æœªåœ¨ç®¡ç†å™¨ä¸­è¨»å†Š")
    
    # 4. æ¸¬è©¦æ•´åˆçˆ¬èŸ²åŠŸèƒ½ (æœ€é—œéµæ¸¬è©¦)
    print("\n\nğŸš€ æ¸¬è©¦4: æ•´åˆçˆ¬èŸ²æ¸¬è©¦ (run_all_crawlers)")
    try:
        print("  æ­£åœ¨åŸ·è¡Œæ•´åˆçˆ¬èŸ²...")
        
        # ä½¿ç”¨çœŸå¯¦çˆ¬èŸ²
        start_time = time.time()
        result = manager.run_all_crawlers(use_real=True)
        elapsed = time.time() - start_time
        
        if result.get('status') == 'success':
            news_count = result.get('total', 0)
            print(f"  âœ… æ•´åˆçˆ¬èŸ²æˆåŠŸ: ç²å– {news_count} å‰‡æ–°èï¼Œè€—æ™‚ {elapsed:.1f} ç§’")
            print("  ğŸ“Š çˆ¬å–çµæœçµ±è¨ˆ:")
            for crawl_result in result.get('results', []):
                status = 'âœ…' if crawl_result.get('success') else 'âŒ'
                print(f"    {status} {crawl_result.get('source', 'æœªçŸ¥')}: {crawl_result.get('news_count', 0)} å‰‡æ–°è")
        else:
            print(f"  âŒ æ•´åˆçˆ¬èŸ²å¤±æ•—: {result.get('message', 'æœªçŸ¥éŒ¯èª¤')}")
    except Exception as e:
        print(f"  âŒ æ•´åˆçˆ¬èŸ²æ¸¬è©¦å¤±æ•—: {e}")
    
    print("\n" + "="*60)
    print("ğŸ çˆ¬èŸ²åŠŸèƒ½æ¸¬è©¦å®Œæˆ")
    print("="*60)

if __name__ == "__main__":
    os.makedirs(os.path.join(project_root, 'logs'), exist_ok=True)
    test_crawlers()
