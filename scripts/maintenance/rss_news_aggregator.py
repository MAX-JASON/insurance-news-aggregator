"""
RSSæ–°èèšåˆå™¨
RSS News Aggregator

ä½¿ç”¨RSSæºç²å–å°ç£è²¡ç¶“ä¿éšªæ–°èï¼Œç¹éåçˆ¬èŸ²é™åˆ¶
"""

import os
import sys
import feedparser
import logging
import time
import re
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from urllib.parse import urljoin
import requests
from dataclasses import dataclass

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger('rss_aggregator')

@dataclass
class RSSNewsItem:
    """RSSæ–°èé …ç›®"""
    title: str
    url: str
    summary: str
    published_date: datetime
    source: str
    category: str = "è²¡ç¶“æ–°è"
    
class RSSNewsAggregator:
    """RSSæ–°èèšåˆå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–èšåˆå™¨"""
        # å°å…¥å°ç£æ–°èæºé…ç½®
        try:
            from config.taiwan_sources import TAIWAN_INSURANCE_SOURCES, TAIWAN_INSURANCE_KEYWORDS
            
            # RSSæ–°èæºé…ç½®ï¼ˆå°ˆæ³¨å°ç£ä¿éšªï¼‰
            self.rss_sources = {}
            
            # åŠ è¼‰RSSæº
            rss_sources = TAIWAN_INSURANCE_SOURCES.get('rss_sources', {})
            for source_id, source_info in rss_sources.items():
                self.rss_sources[source_id] = {
                    'name': source_info['name'],
                    'url': source_info['rss_url'],
                    'category': source_info['category']
                }
            
            # åŠ è¼‰æœ‰RSSçš„è²¡ç¶“åª’é«”
            financial_sources = TAIWAN_INSURANCE_SOURCES.get('financial_media', {})
            for source_id, source_info in financial_sources.items():
                if source_info.get('rss_url'):
                    self.rss_sources[source_id] = {
                        'name': source_info['name'],
                        'url': source_info['rss_url'],
                        'category': source_info['category']
                    }
            
            # ä½¿ç”¨æ“´å……çš„å°ç£ä¿éšªé—œéµè©
            self.insurance_keywords = []
            for category_keywords in TAIWAN_INSURANCE_KEYWORDS.values():
                self.insurance_keywords.extend(category_keywords)
                
        except ImportError:
            logger.warning("ç„¡æ³•å°å…¥å°ç£æ–°èæºé…ç½®ï¼Œä½¿ç”¨é è¨­é…ç½®")
            # é è¨­é…ç½®ï¼ˆä¿ç•™åŸæœ‰çš„ä½œç‚ºå‚™ç”¨ï¼‰
            self.rss_sources = {
                'google_insurance_tw': {
                    'name': 'Googleæ–°è-å°ç£ä¿éšª',
                    'url': 'https://news.google.com/rss/search?q=å°ç£+ä¿éšª&hl=zh-TW&gl=TW&ceid=TW:zh-Hant',
                    'category': 'RSSæ–°è'
                },
                'google_financial_tw': {
                    'name': 'Googleæ–°è-å°ç£é‡‘è',
                    'url': 'https://news.google.com/rss/search?q=å°ç£+é‡‘è+ä¿éšª&hl=zh-TW&gl=TW&ceid=TW:zh-Hant',
                    'category': 'RSSæ–°è'
                },
                'ctee_rss': {
                    'name': 'å·¥å•†æ™‚å ±è²¡ç¶“',
                    'url': 'https://www.ctee.com.tw/rss/ctee-fm.xml',
                    'category': 'è²¡ç¶“æ–°è'
                },
                'storm_finance': {
                    'name': 'é¢¨å‚³åª’è²¡ç¶“',
                    'url': 'https://www.storm.mg/feeds/finance',
                    'category': 'ç¶²è·¯åª’é«”'
                }
            }
            
            # å°ç£ä¿éšªç›¸é—œé—œéµè©
            self.insurance_keywords = [
                'ä¿éšª', 'ä¿è²»', 'ä¿å–®', 'ç†è³ ', 'æŠ•ä¿', 'æ‰¿ä¿',
                'å£½éšª', 'ç”¢éšª', 'è»Šéšª', 'å¥åº·éšª', 'æ„å¤–éšª', 'é†«ç™‚éšª',
                'ä¿éšªå…¬å¸', 'ä¿éšªæ¥­', 'ä¿éšªæ³•', 'ä¿éšªé‡‘', 'ä¿éšœ',
                'å—å±±', 'åœ‹æ³°', 'å¯Œé‚¦', 'æ–°å…‰', 'å°ç£äººå£½',
                'ä¸­åœ‹ä¿¡è¨—', 'ç¬¬ä¸€é‡‘', 'å…†è±', 'ç‰å±±', 'é‡‘ç®¡æœƒ', 'ä¿éšªå±€',
                'å…¨æ°‘å¥ä¿', 'å‹ä¿', 'å‹é€€', 'åœ‹æ°‘å¹´é‡‘', 'é•·ç…§', 'å¤±èƒ½'
            ]
        
        logger.info(f"ğŸ“¡ RSSæ–°èèšåˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æ´ {len(self.rss_sources)} å€‹RSSæº")
    
    def fetch_all_news(self, max_items_per_source: int = 20) -> List[RSSNewsItem]:
        """ç²å–æ‰€æœ‰RSSæºçš„æ–°è"""
        logger.info("ğŸš€ é–‹å§‹ç²å–RSSæ–°è...")
        
        all_news = []
        
        for source_id, source_config in self.rss_sources.items():
            try:
                logger.info(f"ğŸ“¡ æ­£åœ¨ç²å–: {source_config['name']}")
                news_items = self._fetch_rss_news(source_config, max_items_per_source)
                
                if news_items:
                    all_news.extend(news_items)
                    logger.info(f"âœ… {source_config['name']} ç²å– {len(news_items)} ç¯‡æ–‡ç« ")
                else:
                    logger.warning(f"âš ï¸ {source_config['name']} æœªç²å–åˆ°æ–‡ç« ")
                
                # é¿å…è«‹æ±‚éæ–¼é »ç¹
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"âŒ {source_config['name']} ç²å–å¤±æ•—: {e}")
                continue
        
        # ç¯©é¸ä¿éšªç›¸é—œæ–°è
        insurance_news = self._filter_insurance_news(all_news)
        
        logger.info(f"ğŸ‰ RSSèšåˆå®Œæˆï¼Œç¸½å…±ç²å¾— {len(all_news)} ç¯‡æ–‡ç« ï¼Œå…¶ä¸­ {len(insurance_news)} ç¯‡ä¿éšªç›¸é—œ")
        return insurance_news
    
    def _fetch_rss_news(self, source_config: Dict, max_items: int) -> List[RSSNewsItem]:
        """ç²å–å–®å€‹RSSæºçš„æ–°è"""
        try:
            # ä½¿ç”¨feedparserè§£æRSS
            feed = feedparser.parse(source_config['url'])
            
            if feed.bozo:
                logger.warning(f"RSSè§£æè­¦å‘Š: {source_config['name']}")
            
            news_items = []
            
            for entry in feed.entries[:max_items]:
                try:
                    # åŸºæœ¬ä¿¡æ¯
                    title = entry.get('title', '').strip()
                    url = entry.get('link', '')
                    summary = entry.get('summary', title)
                    
                    # æ¸…ç†HTMLæ¨™ç±¤
                    summary = re.sub(r'<[^>]+>', '', summary)
                    summary = re.sub(r'\s+', ' ', summary).strip()
                    
                    # è§£æç™¼å¸ƒæ™‚é–“
                    published_date = self._parse_entry_date(entry)
                    
                    # å‰µå»ºæ–°èé …ç›®
                    news_item = RSSNewsItem(
                        title=title,
                        url=url,
                        summary=summary[:300],  # é™åˆ¶æ‘˜è¦é•·åº¦
                        published_date=published_date,
                        source=source_config['name'],
                        category=source_config['category']
                    )
                    
                    news_items.append(news_item)
                    
                except Exception as e:
                    logger.warning(f"è§£æRSSæ¢ç›®å¤±æ•—: {e}")
                    continue
            
            return news_items
            
        except Exception as e:
            logger.error(f"ç²å–RSSå¤±æ•— {source_config['name']}: {e}")
            return []
    
    def _parse_entry_date(self, entry) -> datetime:
        """è§£æRSSæ¢ç›®çš„æ™‚é–“"""
        try:
            # å˜—è©¦å„ç¨®æ™‚é–“æ¬„ä½
            time_fields = ['published_parsed', 'updated_parsed']
            
            for field in time_fields:
                time_struct = getattr(entry, field, None)
                if time_struct:
                    return datetime(*time_struct[:6])
            
            # å¦‚æœéƒ½æ²’æœ‰ï¼Œä½¿ç”¨ç•¶å‰æ™‚é–“
            return datetime.now()
            
        except Exception:
            return datetime.now()
    
    def _filter_insurance_news(self, news_items: List[RSSNewsItem]) -> List[RSSNewsItem]:
        """ç¯©é¸ä¿éšªç›¸é—œæ–°è"""
        insurance_news = []
        
        for item in news_items:
            # æª¢æŸ¥æ¨™é¡Œå’Œæ‘˜è¦æ˜¯å¦åŒ…å«ä¿éšªé—œéµè©
            text_to_check = f"{item.title} {item.summary}".lower()
            
            is_insurance_related = any(
                keyword in text_to_check for keyword in self.insurance_keywords
            )
            
            if is_insurance_related:
                item.category = "ä¿éšªæ–°è"  # æ›´æ–°åˆ†é¡
                insurance_news.append(item)
                logger.info(f"ğŸ” ä¿éšªç›¸é—œ: {item.title[:50]}...")
        
        return insurance_news
    
    def save_to_database(self, news_items: List[RSSNewsItem]) -> Dict[str, Any]:
        """å„²å­˜æ–°èåˆ°è³‡æ–™åº«"""
        try:
            # ä½¿ç”¨ç›´æ¥ SQLite æ“ä½œå„²å­˜
            from direct_db_save import save_news_directly
            return save_news_directly(news_items)
            
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™åº«å„²å­˜å¤±æ•—: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'saved_count': 0
            }
    
    def generate_summary_report(self, news_items: List[RSSNewsItem]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
        if not news_items:
            return {'message': 'æ²’æœ‰æ–°èæ•¸æ“š'}
        
        # æŒ‰ä¾†æºçµ±è¨ˆ
        source_stats = {}
        for item in news_items:
            source_stats[item.source] = source_stats.get(item.source, 0) + 1
        
        # æŒ‰æ—¥æœŸçµ±è¨ˆ
        today = datetime.now().date()
        today_count = sum(1 for item in news_items if item.published_date.date() == today)
        
        # é—œéµè©çµ±è¨ˆ
        keyword_counts = {}
        for item in news_items:
            text = f"{item.title} {item.summary}".lower()
            for keyword in self.insurance_keywords:
                if keyword in text:
                    keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
        
        # å–å‰5å€‹é—œéµè©
        top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        
        return {
            'total_news': len(news_items),
            'today_news': today_count,
            'source_distribution': source_stats,
            'top_keywords': top_keywords,
            'latest_news': [
                {'title': item.title, 'source': item.source}
                for item in sorted(news_items, key=lambda x: x.published_date, reverse=True)[:3]
            ]
        }

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸ“¡ å•Ÿå‹•RSSæ–°èèšåˆå™¨...")
    
    aggregator = RSSNewsAggregator()
    
    # ç²å–RSSæ–°è
    news_items = aggregator.fetch_all_news(max_items_per_source=15)
    
    if news_items:
        print(f"\nğŸ“° æˆåŠŸç²å– {len(news_items)} ç¯‡ä¿éšªç›¸é—œæ–°è:")
        
        for i, item in enumerate(news_items[:10], 1):  # åªé¡¯ç¤ºå‰10ç¯‡
            print(f"{i}. [{item.source}] {item.title}")
        
        if len(news_items) > 10:
            print(f"... é‚„æœ‰ {len(news_items) - 10} ç¯‡æ–°è")
        
        # ç”Ÿæˆæ‘˜è¦å ±å‘Š
        report = aggregator.generate_summary_report(news_items)
        
        print(f"\nğŸ“Š æ–°èæ‘˜è¦:")
        print(f"  ç¸½æ•¸: {report['total_news']} ç¯‡")
        print(f"  ä»Šæ—¥: {report['today_news']} ç¯‡")
        print(f"  ä¾†æºåˆ†å¸ƒ: {report['source_distribution']}")
        print(f"  ç†±é–€é—œéµè©: {dict(report['top_keywords'])}")
        
        # å„²å­˜åˆ°è³‡æ–™åº«
        print("\nğŸ’¾ æ­£åœ¨å„²å­˜åˆ°è³‡æ–™åº«...")
        result = aggregator.save_to_database(news_items)
        
        print(f"\nğŸ“‹ å„²å­˜çµæœ:")
        print(f"  ç‹€æ…‹: {result['status']}")
        if result['status'] == 'success':
            print(f"  æ–°å¢: {result['saved_count']} ç¯‡")
            print(f"  é‡è¤‡: {result['duplicate_count']} ç¯‡")
        else:
            print(f"  éŒ¯èª¤: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
        
    else:
        print("âš ï¸ æœªèƒ½ç²å–åˆ°ä»»ä½•ä¿éšªç›¸é—œæ–°è")

if __name__ == "__main__":
    main()
