"""
æ•¸æ“šæ¸…æ´—å’Œå»é‡ç³»çµ±
Data Cleaning and Deduplication System

æä¾›æ–°èæ•¸æ“šçš„æ¸…æ´—ã€å»é‡ã€é©—è­‰å’Œå“è³ªæ§åˆ¶åŠŸèƒ½
"""

import re
import logging
import hashlib
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from urllib.parse import urlparse
from difflib import SequenceMatcher

from database.models import News, NewsSource, db
from analyzer.insurance_dictionary import is_insurance_related, calculate_insurance_relevance_score

logger = logging.getLogger('data_cleaner')

class NewsDataCleaner:
    """æ–°èæ•¸æ“šæ¸…æ´—å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•¸æ“šæ¸…æ´—å™¨"""
        self.similarity_threshold = 0.85  # ç›¸ä¼¼åº¦é–¾å€¼
        self.min_content_length = 50     # æœ€å°å…§å®¹é•·åº¦
        self.max_title_length = 200      # æœ€å¤§æ¨™é¡Œé•·åº¦
        self.min_insurance_score = 0.1   # æœ€å°ä¿éšªç›¸é—œæ€§åˆ†æ•¸
        
        # ç„¡æ•ˆå…§å®¹æ¨¡å¼
        self.invalid_patterns = [
            r'404|not found|é é¢ä¸å­˜åœ¨',
            r'error|éŒ¯èª¤|ç³»çµ±ç¶­è­·',
            r'æ¸¬è©¦|test|example',
            r'å»£å‘Š|advertisement|sponsor'
        ]
        
        logger.info("ğŸ§¹ æ–°èæ•¸æ“šæ¸…æ´—ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    def clean_text(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡æœ¬å…§å®¹
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…æ´—å¾Œçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ç§»é™¤å¤šé¤˜ç©ºç™½å’Œæ›è¡Œ
        text = re.sub(r'\s+', ' ', text.strip())
        
        # ç§»é™¤HTMLæ¨™ç±¤
        text = re.sub(r'<[^>]+>', '', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•¸å­—ã€åŸºæœ¬æ¨™é»ï¼‰
        text = re.sub(r'[^\u4e00-\u9fff\w\s\.\,\!\?\:\;\-\(\)\[\]ã€Œã€ã€ã€ã€ã€‚ï¼Œï¼ï¼Ÿï¼šï¼›]', '', text)
        
        # ç§»é™¤é‡è¤‡æ¨™é»
        text = re.sub(r'[ã€‚ï¼Œï¼ï¼Ÿï¼šï¼›]{2,}', 'ã€‚', text)
        text = re.sub(r'[\.]{2,}', '.', text)
        
        return text.strip()
    
    def clean_url(self, url: str) -> str:
        """
        æ¸…æ´—URL
        
        Args:
            url: åŸå§‹URL
            
        Returns:
            æ¸…æ´—å¾Œçš„URL
        """
        if not url:
            return ""
        
        # ç§»é™¤è¿½è¹¤åƒæ•¸
        tracking_params = ['utm_source', 'utm_medium', 'utm_campaign', 'utm_content', 'utm_term']
        
        try:
            parsed = urlparse(url)
            if parsed.query:
                query_parts = []
                for part in parsed.query.split('&'):
                    if '=' in part:
                        key = part.split('=')[0]
                        if key not in tracking_params:
                            query_parts.append(part)
                
                clean_query = '&'.join(query_parts)
                url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                if clean_query:
                    url += f"?{clean_query}"
            
            return url
        except:
            return url
    
    def validate_news_article(self, article_data: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """
        é©—è­‰æ–°èæ–‡ç« çš„æœ‰æ•ˆæ€§
        
        Args:
            article_data: æ–°èæ–‡ç« è³‡æ–™
            
        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, éŒ¯èª¤åŸå› åˆ—è¡¨)
        """
        errors = []
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        title = article_data.get('title', '')
        content = article_data.get('content', '')
        url = article_data.get('url', '')
        
        if not title or len(title.strip()) < 5:
            errors.append("æ¨™é¡Œå¤ªçŸ­æˆ–ç‚ºç©º")
        
        if len(title) > self.max_title_length:
            errors.append(f"æ¨™é¡Œéé•·ï¼ˆè¶…é{self.max_title_length}å­—ç¬¦ï¼‰")
        
        if not content or len(content.strip()) < self.min_content_length:
            errors.append(f"å…§å®¹å¤ªçŸ­ï¼ˆå°‘æ–¼{self.min_content_length}å­—ç¬¦ï¼‰")
        
        if not url or not self._is_valid_url(url):
            errors.append("URLç„¡æ•ˆ")
        
        # æª¢æŸ¥ç„¡æ•ˆå…§å®¹æ¨¡å¼
        full_text = f"{title} {content}".lower()
        for pattern in self.invalid_patterns:
            if re.search(pattern, full_text, re.IGNORECASE):
                errors.append(f"åŒ…å«ç„¡æ•ˆå…§å®¹æ¨¡å¼: {pattern}")
        
        # æª¢æŸ¥ä¿éšªç›¸é—œæ€§
        insurance_score = calculate_insurance_relevance_score(full_text)
        if insurance_score < self.min_insurance_score:
            errors.append(f"ä¿éšªç›¸é—œæ€§éä½: {insurance_score:.3f}")
        
        return len(errors) == 0, errors
    
    def _is_valid_url(self, url: str) -> bool:
        """æª¢æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed = urlparse(url)
            return bool(parsed.scheme and parsed.netloc)
        except:
            return False
    
    def calculate_content_similarity(self, text1: str, text2: str) -> float:
        """
        è¨ˆç®—å…©æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•¸ (0-1)
        """
        if not text1 or not text2:
            return 0.0
        
        # ä½¿ç”¨SequenceMatcherè¨ˆç®—ç›¸ä¼¼åº¦
        similarity = SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
        return similarity
    
    def find_duplicate_news(self, news_list: List[Dict[str, Any]]) -> List[Tuple[int, int, float]]:
        """
        æ‰¾å‡ºé‡è¤‡çš„æ–°è
        
        Args:
            news_list: æ–°èåˆ—è¡¨
            
        Returns:
            é‡è¤‡æ–°èçš„ç´¢å¼•å°å’Œç›¸ä¼¼åº¦
        """
        duplicates = []
        
        for i in range(len(news_list)):
            for j in range(i + 1, len(news_list)):
                news1 = news_list[i]
                news2 = news_list[j]
                
                # æ¯”è¼ƒæ¨™é¡Œç›¸ä¼¼åº¦
                title_similarity = self.calculate_content_similarity(
                    news1.get('title', ''), 
                    news2.get('title', '')
                )
                
                # æ¯”è¼ƒå…§å®¹ç›¸ä¼¼åº¦
                content_similarity = self.calculate_content_similarity(
                    news1.get('content', ''), 
                    news2.get('content', '')
                )
                
                # ç¶œåˆç›¸ä¼¼åº¦ï¼ˆæ¨™é¡Œæ¬Šé‡æ›´é«˜ï¼‰
                combined_similarity = (title_similarity * 0.7 + content_similarity * 0.3)
                
                if combined_similarity >= self.similarity_threshold:
                    duplicates.append((i, j, combined_similarity))
        
        return duplicates
    
    def remove_duplicates_from_database(self) -> Dict[str, Any]:
        """
        å¾è³‡æ–™åº«ä¸­ç§»é™¤é‡è¤‡æ–°è
        
        Returns:
            æ¸…ç†çµæœçµ±è¨ˆ
        """
        try:
            logger.info("ğŸ” é–‹å§‹å°‹æ‰¾è³‡æ–™åº«ä¸­çš„é‡è¤‡æ–°è...")
            
            news_articles = News.query.filter_by(status='active').all()
            news_data = []
            
            for news in news_articles:
                news_data.append({
                    'id': news.id,
                    'title': news.title,
                    'content': news.content or '',
                    'url': news.url,
                    'published_date': news.published_date
                })
            
            duplicates = self.find_duplicate_news(news_data)
            
            if not duplicates:
                logger.info("âœ… æ²’æœ‰ç™¼ç¾é‡è¤‡æ–°è")
                return {
                    'total_checked': len(news_data),
                    'duplicates_found': 0,
                    'removed_count': 0,
                    'duplicates': []
                }
            
            # æ±ºå®šè¦åˆªé™¤çš„æ–°èï¼ˆä¿ç•™è¼ƒæ–°çš„ï¼‰
            to_remove = set()
            duplicate_info = []
            
            for i, j, similarity in duplicates:
                news1 = news_data[i]
                news2 = news_data[j]
                
                # ä¿ç•™è¼ƒæ–°çš„æ–°è
                if news1['published_date'] > news2['published_date']:
                    to_remove.add(news2['id'])
                    kept_id = news1['id']
                    removed_id = news2['id']
                else:
                    to_remove.add(news1['id'])
                    kept_id = news2['id']
                    removed_id = news1['id']
                
                duplicate_info.append({
                    'similarity': similarity,
                    'kept_id': kept_id,
                    'removed_id': removed_id,
                    'kept_title': news1['title'] if kept_id == news1['id'] else news2['title'],
                    'removed_title': news2['title'] if removed_id == news2['id'] else news1['title']
                })
            
            # åŸ·è¡Œåˆªé™¤
            removed_count = 0
            for news_id in to_remove:
                news = News.query.get(news_id)
                if news:
                    db.session.delete(news)
                    removed_count += 1
            
            db.session.commit()
            
            logger.info(f"âœ… é‡è¤‡æ–°èæ¸…ç†å®Œæˆï¼šæª¢æŸ¥ {len(news_data)} å‰‡ï¼Œç™¼ç¾ {len(duplicates)} çµ„é‡è¤‡ï¼Œåˆªé™¤ {removed_count} å‰‡")
            
            return {
                'total_checked': len(news_data),
                'duplicates_found': len(duplicates),
                'removed_count': removed_count,
                'duplicates': duplicate_info
            }
            
        except Exception as e:
            logger.error(f"âŒ é‡è¤‡æ–°èæ¸…ç†å¤±æ•—: {e}")
            return {
                'total_checked': 0,
                'duplicates_found': 0,
                'removed_count': 0,
                'error': str(e)
            }
    
    def clean_news_batch(self, news_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ‰¹é‡æ¸…æ´—æ–°èæ•¸æ“š
        
        Args:
            news_list: æ–°èåˆ—è¡¨
            
        Returns:
            æ¸…æ´—çµæœ
        """
        try:
            logger.info(f"ğŸ§¹ é–‹å§‹æ‰¹é‡æ¸…æ´— {len(news_list)} å‰‡æ–°è...")
            
            cleaned_news = []
            invalid_news = []
            duplicates = []
            
            # æ¸…æ´—æ¯å‰‡æ–°è
            for i, news in enumerate(news_list):
                try:
                    # æ¸…æ´—æ–‡æœ¬
                    cleaned_title = self.clean_text(news.get('title', ''))
                    cleaned_content = self.clean_text(news.get('content', ''))
                    cleaned_summary = self.clean_text(news.get('summary', ''))
                    cleaned_url = self.clean_url(news.get('url', ''))
                    
                    cleaned_news_item = {
                        **news,
                        'title': cleaned_title,
                        'content': cleaned_content,
                        'summary': cleaned_summary,
                        'url': cleaned_url
                    }
                    
                    # é©—è­‰æ¸…æ´—å¾Œçš„æ–°è
                    is_valid, errors = self.validate_news_article(cleaned_news_item)
                    
                    if is_valid:
                        cleaned_news.append(cleaned_news_item)
                    else:
                        invalid_news.append({
                            'index': i,
                            'news': news,
                            'errors': errors
                        })
                        
                except Exception as e:
                    logger.error(f"âŒ æ¸…æ´—ç¬¬ {i} å‰‡æ–°èå¤±æ•—: {e}")
                    invalid_news.append({
                        'index': i,
                        'news': news,
                        'errors': [f"æ¸…æ´—å¤±æ•—: {str(e)}"]
                    })
            
            # å»é‡è™•ç†
            if cleaned_news:
                duplicate_pairs = self.find_duplicate_news(cleaned_news)
                duplicates = duplicate_pairs
                
                # ç§»é™¤é‡è¤‡é …ï¼ˆä¿ç•™ç¬¬ä¸€å€‹ï¼‰
                to_remove = set()
                for i, j, similarity in duplicate_pairs:
                    to_remove.add(j)  # ç§»é™¤å¾Œé¢çš„
                
                # å‰µå»ºå»é‡å¾Œçš„åˆ—è¡¨
                final_news = [news for i, news in enumerate(cleaned_news) if i not in to_remove]
            else:
                final_news = []
            
            result = {
                'original_count': len(news_list),
                'cleaned_count': len(cleaned_news),
                'final_count': len(final_news),
                'invalid_count': len(invalid_news),
                'duplicate_count': len(duplicates),
                'cleaned_news': final_news,
                'invalid_news': invalid_news,
                'duplicates': duplicates
            }
            
            logger.info(f"âœ… æ‰¹é‡æ¸…æ´—å®Œæˆï¼š{len(news_list)} â†’ {len(final_news)} å‰‡æ–°è")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ¸…æ´—å¤±æ•—: {e}")
            return {
                'error': str(e),
                'original_count': len(news_list),
                'cleaned_news': []
            }

# å…¨åŸŸæ¸…æ´—å™¨å¯¦ä¾‹
_cleaner_instance = None

def get_data_cleaner() -> NewsDataCleaner:
    """å–å¾—å…¨åŸŸæ•¸æ“šæ¸…æ´—å™¨å¯¦ä¾‹"""
    global _cleaner_instance
    if _cleaner_instance is None:
        _cleaner_instance = NewsDataCleaner()
    return _cleaner_instance

if __name__ == "__main__":
    # æ¸¬è©¦æ•¸æ“šæ¸…æ´—ç³»çµ±
    print("ğŸ§ª æ¸¬è©¦æ–°èæ•¸æ“šæ¸…æ´—ç³»çµ±...")
    
    cleaner = NewsDataCleaner()
    
    # æ¸¬è©¦æ–‡æœ¬æ¸…æ´—
    test_text = "  é€™æ˜¯ä¸€å‰‡  <b>ä¿éšª</b>  æ–°è!!!   "
    cleaned = cleaner.clean_text(test_text)
    print(f"æ–‡æœ¬æ¸…æ´—æ¸¬è©¦: '{test_text}' â†’ '{cleaned}'")
    
    # æ¸¬è©¦URLæ¸…æ´—
    test_url = "https://example.com/news?utm_source=google&utm_medium=cpc&id=123"
    cleaned_url = cleaner.clean_url(test_url)
    print(f"URLæ¸…æ´—æ¸¬è©¦: {test_url} â†’ {cleaned_url}")
    
    # æ¸¬è©¦ç›¸ä¼¼åº¦è¨ˆç®—
    text1 = "å°ç£äººå£½æ¨å‡ºæ–°ä¿éšªç”¢å“"
    text2 = "å°ç£äººå£½ç™¼è¡¨æ–°çš„ä¿éšªå•†å“"
    similarity = cleaner.calculate_content_similarity(text1, text2)
    print(f"ç›¸ä¼¼åº¦æ¸¬è©¦: {similarity:.3f}")
    
    print("âœ… æ•¸æ“šæ¸…æ´—ç³»çµ±æ¸¬è©¦å®Œæˆ")
