"""
è‡ªå‹•åŒ–çˆ¬èŸ²æ’ç¨‹ç³»çµ±
Automated Crawler Scheduler

å®šæœŸåŸ·è¡Œæ–°èçˆ¬å–ä»»å‹™ï¼Œç¢ºä¿æ–°èè³‡æ–™çš„æ™‚æ•ˆæ€§
"""

import logging
import time
import threading
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path
import json
import schedule

from crawler.manager import CrawlerManager
from database.models import News, NewsSource, db
from app import create_app
from config.settings import Config

logger = logging.getLogger('scheduler')

class CrawlerScheduler:
    """çˆ¬èŸ²æ’ç¨‹ç®¡ç†å™¨"""
    
    def __init__(self, app=None):
        """
        åˆå§‹åŒ–æ’ç¨‹å™¨
        
        Args:
            app: Flaskæ‡‰ç”¨å¯¦ä¾‹
        """
        self.app = app or create_app(Config)
        self.crawler_manager = CrawlerManager()
        self.is_running = False
        self.scheduler_thread = None
        self.last_run_results = {}
        
        # è¨­å®šæ’ç¨‹é…ç½®
        self.schedule_config = {
            'real_news_interval': 30,  # çœŸå¯¦æ–°èçˆ¬å–é–“éš”ï¼ˆåˆ†é˜ï¼‰
            'mock_news_interval': 60,  # æ¨¡æ“¬æ–°èçˆ¬å–é–“éš”ï¼ˆåˆ†é˜ï¼‰
            'cleanup_interval': 24,    # æ¸…ç†éæœŸè³‡æ–™é–“éš”ï¼ˆå°æ™‚ï¼‰
            'stats_interval': 6        # çµ±è¨ˆæ›´æ–°é–“éš”ï¼ˆå°æ™‚ï¼‰
        }
        
        logger.info("ğŸ• çˆ¬èŸ²æ’ç¨‹ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    def setup_schedules(self):
        """è¨­å®šæ’ç¨‹ä»»å‹™"""
        try:
            # çœŸå¯¦æ–°èçˆ¬å– - æ¯30åˆ†é˜
            schedule.every(self.schedule_config['real_news_interval']).minutes.do(
                self._run_real_news_crawl
            )
            
            # æ¨¡æ“¬æ–°èçˆ¬å– - æ¯å°æ™‚ï¼ˆå‚™ç”¨ï¼‰
            schedule.every(self.schedule_config['mock_news_interval']).minutes.do(
                self._run_mock_news_crawl
            )
            
            # è³‡æ–™æ¸…ç† - æ¯å¤©
            schedule.every(self.schedule_config['cleanup_interval']).hours.do(
                self._run_cleanup_task
            )
            
            # çµ±è¨ˆæ›´æ–° - æ¯6å°æ™‚
            schedule.every(self.schedule_config['stats_interval']).hours.do(
                self._update_statistics
            )
            
            logger.info("âœ… æ’ç¨‹ä»»å‹™è¨­å®šå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ’ç¨‹è¨­å®šå¤±æ•—: {e}")
    
    def start(self):
        """å•Ÿå‹•æ’ç¨‹å™¨"""
        if self.is_running:
            logger.warning("âš ï¸ æ’ç¨‹å™¨å·²åœ¨é‹è¡Œä¸­")
            return
        
        self.is_running = True
        self.setup_schedules()
        
        # å•Ÿå‹•æ’ç¨‹åŸ·è¡Œç·’
        self.scheduler_thread = threading.Thread(target=self._run_scheduler, daemon=True)
        self.scheduler_thread.start()
        
        logger.info("ğŸš€ çˆ¬èŸ²æ’ç¨‹å™¨å·²å•Ÿå‹•")
        
        # ç«‹å³åŸ·è¡Œä¸€æ¬¡çœŸå¯¦æ–°èçˆ¬å–
        threading.Thread(target=self._run_real_news_crawl, daemon=True).start()
    
    def stop(self):
        """åœæ­¢æ’ç¨‹å™¨"""
        self.is_running = False
        schedule.clear()
        
        if self.scheduler_thread and self.scheduler_thread.is_alive():
            self.scheduler_thread.join(timeout=5)
        
        logger.info("ğŸ›‘ çˆ¬èŸ²æ’ç¨‹å™¨å·²åœæ­¢")
    
    def _run_scheduler(self):
        """åŸ·è¡Œæ’ç¨‹å¾ªç’°"""
        while self.is_running:
            try:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
            except Exception as e:
                logger.error(f"âŒ æ’ç¨‹åŸ·è¡ŒéŒ¯èª¤: {e}")
                time.sleep(60)
    
    def _run_real_news_crawl(self):
        """åŸ·è¡ŒçœŸå¯¦æ–°èçˆ¬å–ä»»å‹™"""
        try:
            logger.info("ğŸ“¡ é–‹å§‹åŸ·è¡ŒçœŸå¯¦æ–°èçˆ¬å–ä»»å‹™...")
            
            with self.app.app_context():
                result = self.crawler_manager.crawl_all_sources(use_mock=False)
                
                self.last_run_results['real_news'] = {
                    'timestamp': datetime.now(),
                    'status': result.get('status'),
                    'message': result.get('message'),
                    'news_count': sum(r.get('news_count', 0) for r in result.get('results', [])),
                    'duration': result.get('duration', 0)
                }
                
                if result.get('status') == 'success':
                    logger.info(f"âœ… çœŸå¯¦æ–°èçˆ¬å–å®Œæˆ: {self.last_run_results['real_news']['news_count']} å‰‡æ–°è")
                else:
                    logger.warning(f"âš ï¸ çœŸå¯¦æ–°èçˆ¬å–ç•°å¸¸: {result.get('message')}")
                    
        except Exception as e:
            logger.error(f"âŒ çœŸå¯¦æ–°èçˆ¬å–ä»»å‹™å¤±æ•—: {e}")
            self.last_run_results['real_news'] = {
                'timestamp': datetime.now(),
                'status': 'error',
                'message': str(e),
                'news_count': 0,
                'duration': 0
            }
    
    def _run_mock_news_crawl(self):
        """åŸ·è¡Œæ¨¡æ“¬æ–°èçˆ¬å–ä»»å‹™ï¼ˆå‚™ç”¨ï¼‰"""
        try:
            # æª¢æŸ¥æœ€è¿‘æ˜¯å¦æœ‰æˆåŠŸçš„çœŸå¯¦æ–°èçˆ¬å–
            real_news_result = self.last_run_results.get('real_news')
            if (real_news_result and 
                real_news_result.get('status') == 'success' and
                datetime.now() - real_news_result['timestamp'] < timedelta(hours=2)):
                logger.info("â­ï¸ è·³éæ¨¡æ“¬æ–°èçˆ¬å–ï¼ˆçœŸå¯¦æ–°èçˆ¬å–æ­£å¸¸ï¼‰")
                return
            
            logger.info("ğŸ“ åŸ·è¡Œå‚™ç”¨æ¨¡æ“¬æ–°èçˆ¬å–ä»»å‹™...")
            
            with self.app.app_context():
                result = self.crawler_manager.crawl_all_sources(use_mock=True)
                
                self.last_run_results['mock_news'] = {
                    'timestamp': datetime.now(),
                    'status': result.get('status'),
                    'message': result.get('message'),
                    'news_count': sum(r.get('news_count', 0) for r in result.get('results', [])),
                    'duration': result.get('duration', 0)
                }
                
                logger.info(f"âœ… å‚™ç”¨æ–°èçˆ¬å–å®Œæˆ: {self.last_run_results['mock_news']['news_count']} å‰‡æ–°è")
                    
        except Exception as e:
            logger.error(f"âŒ æ¨¡æ“¬æ–°èçˆ¬å–ä»»å‹™å¤±æ•—: {e}")
    
    def _run_cleanup_task(self):
        """åŸ·è¡Œè³‡æ–™æ¸…ç†ä»»å‹™"""
        try:
            logger.info("ğŸ§¹ é–‹å§‹åŸ·è¡Œè³‡æ–™æ¸…ç†ä»»å‹™...")
            
            with self.app.app_context():
                # åˆªé™¤è¶…é30å¤©çš„èˆŠæ–°èï¼ˆä¿ç•™é‡è¦æ–°èï¼‰
                cutoff_date = datetime.now() - timedelta(days=30)
                old_news = News.query.filter(
                    News.created_at < cutoff_date,
                    News.importance_score < 0.7  # ä¿ç•™é‡è¦æ–°è
                ).all()
                
                deleted_count = 0
                for news in old_news:
                    db.session.delete(news)
                    deleted_count += 1
                
                if deleted_count > 0:
                    db.session.commit()
                    logger.info(f"âœ… æ¸…ç†å®Œæˆï¼šåˆªé™¤ {deleted_count} å‰‡éæœŸæ–°è")
                else:
                    logger.info("âœ… æ¸…ç†å®Œæˆï¼šæ²’æœ‰éœ€è¦åˆªé™¤çš„éæœŸæ–°è")
                
                # æ¸…ç†åˆ†æå¿«å–
                from analyzer.cache import get_cache
                cache = get_cache()
                expired_count = cache.clear_expired()
                logger.info(f"âœ… æ¸…ç†éæœŸå¿«å–ï¼š{expired_count} å€‹æª”æ¡ˆ")
                
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™æ¸…ç†ä»»å‹™å¤±æ•—: {e}")
    
    def _update_statistics(self):
        """æ›´æ–°çµ±è¨ˆè³‡æ–™"""
        try:
            logger.info("ğŸ“Š æ›´æ–°çµ±è¨ˆè³‡æ–™...")
            
            with self.app.app_context():
                # æ›´æ–°æ–°èä¾†æºçµ±è¨ˆ
                sources = NewsSource.query.all()
                for source in sources:
                    source.total_news_count = News.query.filter_by(source_id=source.id).count()
                
                db.session.commit()
                logger.info("âœ… çµ±è¨ˆè³‡æ–™æ›´æ–°å®Œæˆ")
                
        except Exception as e:
            logger.error(f"âŒ çµ±è¨ˆè³‡æ–™æ›´æ–°å¤±æ•—: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """å–å¾—æ’ç¨‹å™¨ç‹€æ…‹"""
        return {
            'is_running': self.is_running,
            'last_run_results': self.last_run_results,
            'schedule_config': self.schedule_config,
            'next_runs': {
                'real_news': self._get_next_run_time('real_news'),
                'mock_news': self._get_next_run_time('mock_news'),
                'cleanup': self._get_next_run_time('cleanup'),
                'stats': self._get_next_run_time('stats')
            }
        }
    
    def _get_next_run_time(self, task_type: str) -> Optional[str]:
        """å–å¾—ä¸‹æ¬¡åŸ·è¡Œæ™‚é–“"""
        try:
            jobs = schedule.get_jobs()
            for job in jobs:
                if task_type in str(job.job_func):
                    return str(job.next_run) if job.next_run else None
            return None
        except:
            return None
    
    def manual_crawl(self, use_real: bool = True) -> Dict[str, Any]:
        """æ‰‹å‹•è§¸ç™¼çˆ¬å–ä»»å‹™"""
        try:
            logger.info(f"ğŸ”„ æ‰‹å‹•è§¸ç™¼çˆ¬å–ä»»å‹™ (çœŸå¯¦æ–°è: {use_real})")
            
            with self.app.app_context():
                result = self.crawler_manager.crawl_all_sources(use_mock=not use_real)
                
                task_type = 'real_news' if use_real else 'mock_news'
                self.last_run_results[task_type] = {
                    'timestamp': datetime.now(),
                    'status': result.get('status'),
                    'message': result.get('message'),
                    'news_count': sum(r.get('news_count', 0) for r in result.get('results', [])),
                    'duration': result.get('duration', 0),
                    'manual': True
                }
                
                return self.last_run_results[task_type]
                
        except Exception as e:
            logger.error(f"âŒ æ‰‹å‹•çˆ¬å–å¤±æ•—: {e}")
            return {
                'timestamp': datetime.now(),
                'status': 'error',
                'message': str(e),
                'news_count': 0,
                'duration': 0,
                'manual': True
            }

# å…¨åŸŸæ’ç¨‹å™¨å¯¦ä¾‹
_scheduler_instance = None

def get_scheduler(app=None) -> CrawlerScheduler:
    """å–å¾—å…¨åŸŸæ’ç¨‹å™¨å¯¦ä¾‹"""
    global _scheduler_instance
    if _scheduler_instance is None:
        _scheduler_instance = CrawlerScheduler(app)
    return _scheduler_instance

def start_scheduler(app=None):
    """å•Ÿå‹•æ’ç¨‹å™¨"""
    scheduler = get_scheduler(app)
    scheduler.start()
    return scheduler

if __name__ == "__main__":
    # æ¸¬è©¦æ’ç¨‹å™¨
    print("ğŸ§ª æ¸¬è©¦çˆ¬èŸ²æ’ç¨‹ç³»çµ±...")
    
    scheduler = CrawlerScheduler()
    print("âœ… æ’ç¨‹å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    # å–å¾—ç‹€æ…‹
    status = scheduler.get_status()
    print(f"ç‹€æ…‹: {status}")
    
    print("âœ… æ’ç¨‹ç³»çµ±æ¸¬è©¦å®Œæˆ")
