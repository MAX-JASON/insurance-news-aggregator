#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è‡ªå‹•æ–°èæ¸…ç†æœå‹™
Auto News Cleanup Service

å®šæœŸæ¸…ç†è¶…éæŒ‡å®šå¤©æ•¸çš„æ–°èï¼Œä¿æŒç³»çµ±æ•ˆèƒ½å’Œè³‡æ–™æ•´æ½”
"""

import os
import sys
import time
import schedule
import sqlite3
from datetime import datetime, timedelta, timezone
import logging

# è¨­ç½®æ—¥èªŒ
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
log_file = os.path.join(project_root, 'logs', 'news_cleanup.log')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class NewsCleanupService:
    """æ–°èæ¸…ç†æœå‹™"""
    
    def __init__(self, max_age_days=7):
        self.max_age_days = max_age_days
        self.db_path = self._find_database()
        
    def _find_database(self):
        """å°‹æ‰¾è³‡æ–™åº«æª”æ¡ˆ"""
        # å–å¾—ç®¡ç†è…³æœ¬æ‰€åœ¨ç›®éŒ„çš„ä¸Šä¸€å±¤ç›®éŒ„ï¼ˆå°ˆæ¡ˆæ ¹ç›®éŒ„ï¼‰
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(current_dir)
        
        possible_paths = [
            os.path.join(project_root, "instance", "insurance_news.db"),
            os.path.join(project_root, "instance", "dev_insurance_news.db")
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
        
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è³‡æ–™åº«æª”æ¡ˆï¼Œæœå°‹è·¯å¾‘: {possible_paths}")
    
    def cleanup_old_news(self):
        """æ¸…ç†èˆŠæ–°è"""
        try:
            logger.info(f"é–‹å§‹æ¸…ç†è¶…é {self.max_age_days} å¤©çš„èˆŠæ–°è...")
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # è¨ˆç®—æˆªæ­¢æ—¥æœŸ
            cutoff_date = (datetime.now(timezone.utc) - timedelta(days=self.max_age_days)).isoformat()
            
            # æŸ¥æ‰¾è¦åˆªé™¤çš„æ–°è
            cursor.execute("""
                SELECT COUNT(*) 
                FROM news 
                WHERE (published_date < ? OR crawled_date < ?)
                AND status = 'active'
            """, (cutoff_date, cutoff_date))
            
            old_count = cursor.fetchone()[0]
            
            if old_count == 0:
                logger.info("æ²’æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„èˆŠæ–°è")
                return True
            
            # åŸ·è¡Œè»Ÿåˆªé™¤
            cursor.execute("""
                UPDATE news 
                SET status = 'deleted', updated_at = ?
                WHERE (published_date < ? OR crawled_date < ?)
                AND status = 'active'
            """, (datetime.now(timezone.utc).isoformat(), cutoff_date, cutoff_date))
            
            deleted_count = cursor.rowcount
            conn.commit()
            
            # ç²å–çµ±è¨ˆè³‡è¨Š
            cursor.execute("SELECT COUNT(*) FROM news WHERE status = 'active'")
            active_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM news WHERE status = 'deleted'")
            deleted_total = cursor.fetchone()[0]
            
            conn.close()
            
            logger.info(f"âœ… æˆåŠŸæ¸…ç† {deleted_count} æ¢èˆŠæ–°è")
            logger.info(f"ğŸ“Š ç•¶å‰çµ±è¨ˆ - æ´»èº: {active_count}, å·²åˆªé™¤: {deleted_total}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†å¤±æ•—: {e}")
            return False
    
    def cleanup_by_count(self, max_count=200):
        """æŒ‰æ•¸é‡é™åˆ¶æ¸…ç†æ–°è"""
        try:
            logger.info(f"é–‹å§‹æŒ‰æ•¸é‡æ¸…ç†ï¼Œä¿ç•™æœ€æ–° {max_count} æ¢æ–°è...")
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ç²å–ç•¶å‰æ´»èºæ–°èç¸½æ•¸
            cursor.execute("SELECT COUNT(*) FROM news WHERE status = 'active'")
            current_count = cursor.fetchone()[0]
            
            if current_count <= max_count:
                logger.info(f"ç•¶å‰æ–°èæ•¸é‡ ({current_count}) æœªè¶…éé™åˆ¶ ({max_count})")
                return True
            
            # æ‰¾åˆ°è¦ä¿ç•™çš„æ–°èID
            cursor.execute("""
                SELECT id FROM news 
                WHERE status = 'active'
                ORDER BY published_date DESC, crawled_date DESC
                LIMIT ?
            """, (max_count,))
            
            keep_ids = [str(row[0]) for row in cursor.fetchall()]
            
            # åˆªé™¤ä¸åœ¨ä¿ç•™åˆ—è¡¨ä¸­çš„æ–°è
            placeholders = ','.join(['?'] * len(keep_ids))
            cursor.execute(f"""
                UPDATE news 
                SET status = 'deleted', updated_at = ?
                WHERE status = 'active' AND id NOT IN ({placeholders})
            """, [datetime.now(timezone.utc).isoformat()] + keep_ids)
            
            deleted_count = cursor.rowcount
            conn.commit()
            conn.close()
            
            logger.info(f"âœ… æŒ‰æ•¸é‡æ¸…ç†å®Œæˆï¼Œåˆªé™¤äº† {deleted_count} æ¢èˆŠæ–°è")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æŒ‰æ•¸é‡æ¸…ç†å¤±æ•—: {e}")
            return False
    
    def get_status(self):
        """ç²å–æ¸…ç†æœå‹™ç‹€æ…‹"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # çµ±è¨ˆè³‡è¨Š
            cursor.execute("SELECT COUNT(*) FROM news WHERE status = 'active'")
            active_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM news WHERE status = 'deleted'")
            deleted_count = cursor.fetchone()[0]
            
            # æœ€èˆŠçš„æ´»èºæ–°è
            cursor.execute("""
                SELECT published_date FROM news 
                WHERE status = 'active' AND published_date IS NOT NULL
                ORDER BY published_date ASC LIMIT 1
            """)
            oldest_result = cursor.fetchone()
            oldest_date = oldest_result[0] if oldest_result else None
            
            # æœ€æ–°çš„æ´»èºæ–°è
            cursor.execute("""
                SELECT published_date FROM news 
                WHERE status = 'active' AND published_date IS NOT NULL
                ORDER BY published_date DESC LIMIT 1
            """)
            newest_result = cursor.fetchone()
            newest_date = newest_result[0] if newest_result else None
            
            conn.close()
            
            return {
                'active_news': active_count,
                'deleted_news': deleted_count,
                'oldest_news_date': oldest_date,
                'newest_news_date': newest_date,
                'max_age_days': self.max_age_days,
                'database_path': self.db_path
            }
            
        except Exception as e:
            logger.error(f"ç²å–ç‹€æ…‹å¤±æ•—: {e}")
            return None

def run_scheduled_cleanup():
    """åŸ·è¡Œæ’ç¨‹æ¸…ç†"""
    logger.info("ğŸ§¹ åŸ·è¡Œæ’ç¨‹æ–°èæ¸…ç†...")
    
    service = NewsCleanupService(max_age_days=7)
    
    # å…ˆæŒ‰æ—¥æœŸæ¸…ç†
    if service.cleanup_old_news():
        # å†æŒ‰æ•¸é‡é™åˆ¶æ¸…ç†ï¼ˆé¿å…è³‡æ–™åº«éå¤§ï¼‰
        service.cleanup_by_count(max_count=500)
    
    logger.info("ğŸ‰ æ’ç¨‹æ¸…ç†å®Œæˆ")

def start_scheduler():
    """å•Ÿå‹•æ’ç¨‹å™¨"""
    logger.info("ğŸš€ å•Ÿå‹•æ–°èæ¸…ç†æ’ç¨‹æœå‹™...")
    logger.info("ğŸ“… æ’ç¨‹è¨­å®š:")
    logger.info("  - æ¯æ—¥ 02:00 è‡ªå‹•æ¸…ç†èˆŠæ–°è")
    logger.info("  - æ¯ 6 å°æ™‚æª¢æŸ¥ä¸€æ¬¡æ•¸é‡é™åˆ¶")
    
    # è¨­å®šæ’ç¨‹
    schedule.every().day.at("02:00").do(run_scheduled_cleanup)
    schedule.every(6).hours.do(lambda: NewsCleanupService().cleanup_by_count(500))
    
    # ç«‹å³åŸ·è¡Œä¸€æ¬¡æ¸…ç†
    run_scheduled_cleanup()
    
    # æŒçºŒé‹è¡Œæ’ç¨‹
    try:
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ æ’ç¨‹æœå‹™å·²åœæ­¢")

def main():
    """ä¸»ç¨‹å¼"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ–°èæ¸…ç†æœå‹™')
    parser.add_argument('--run-once', action='store_true', help='åªåŸ·è¡Œä¸€æ¬¡æ¸…ç†ï¼Œä¸å•Ÿå‹•æ’ç¨‹')
    parser.add_argument('--status', action='store_true', help='é¡¯ç¤ºæ¸…ç†æœå‹™ç‹€æ…‹')
    parser.add_argument('--days', type=int, default=7, help='ä¿ç•™æ–°èçš„å¤©æ•¸')
    
    args = parser.parse_args()
    
    if args.status:
        # é¡¯ç¤ºç‹€æ…‹
        service = NewsCleanupService(args.days)
        status = service.get_status()
        
        if status:
            print("ğŸ“Š æ–°èæ¸…ç†æœå‹™ç‹€æ…‹:")
            print("=" * 40)
            print(f"æ´»èºæ–°è: {status['active_news']}")
            print(f"å·²åˆªé™¤æ–°è: {status['deleted_news']}")
            print(f"ä¿ç•™å¤©æ•¸: {status['max_age_days']}")
            print(f"æœ€èˆŠæ–°è: {status['oldest_news_date']}")
            print(f"æœ€æ–°æ–°è: {status['newest_news_date']}")
            print(f"è³‡æ–™åº«: {status['database_path']}")
        else:
            print("âŒ ç„¡æ³•ç²å–ç‹€æ…‹è³‡è¨Š")
            
    elif args.run_once:
        # åŸ·è¡Œä¸€æ¬¡æ¸…ç†
        service = NewsCleanupService(args.days)
        service.cleanup_old_news()
        service.cleanup_by_count(500)
        
    else:
        # å•Ÿå‹•æ’ç¨‹æœå‹™
        start_scheduler()

if __name__ == "__main__":
    main()
