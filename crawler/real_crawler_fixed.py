"""
çœŸå¯¦æ–°èçˆ¬èŸ²å¯¦ä½œ
Real News Crawler Implementation

å¯¦ä½œèƒ½å¤ æŠ“å–çœŸå¯¦ä¿éšªæ–°èçš„çˆ¬èŸ²
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import logging
import time
import random
import re
import urllib.parse
import feedparser

logger = logging.getLogger('crawler.real')

class RealInsuranceNewsCrawler:
    """çœŸå¯¦ä¿éšªæ–°èçˆ¬èŸ²"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
    
    def crawl_google_news(self) -> List[Dict[str, Any]]:
        """æœç´¢Googleæ–°èä¸­çš„ä¿éšªç›¸é—œå…§å®¹"""
        news_list = []
        
        try:
            print("ğŸ” æ­£åœ¨æœç´¢Googleæ–°èä¿éšªå…§å®¹...")
            
            # ä½¿ç”¨Googleæ–°èæœç´¢ - ä¿®å¾©URLç·¨ç¢¼å•é¡Œ
            search_query = "å°ç£ä¿éšª"
            encoded_query = urllib.parse.quote(search_query)
            search_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
            
            # å˜—è©¦RSSæ–¹å¼
            feed = feedparser.parse(search_url)
            
            if not feed.bozo and hasattr(feed, 'entries'):
                for entry in feed.entries[:8]:  # é™åˆ¶8å‰‡æ–°è
                    try:
                        title = entry.get('title', '')
                        if not self._is_insurance_related(title):
                            continue
                        
                        news_item = {
                            'title': title,
                            'url': entry.get('link', ''),
                            'summary': entry.get('summary', '')[:200],
                            'published_date': self._parse_feed_date(entry),
                            'source': 'Googleæ–°è',
                            'content': ''
                        }
                        
                        news_list.append(news_item)
                        print(f"  âœ… æ‰¾åˆ°Googleæ–°è: {title[:30]}...")
                        
                    except Exception as e:
                        logger.debug(f"è™•ç†Googleæ–°èé …ç›®å¤±æ•—: {e}")
                        continue
            
            print(f"âœ… Googleæ–°èå…±æ‰¾åˆ° {len(news_list)} å‰‡ä¿éšªæ–°è")
            return news_list
            
        except Exception as e:
            print(f"âŒ æœç´¢Googleæ–°èå¤±æ•—: {e}")
            return []
    
    def crawl_udn_finance(self) -> List[Dict[str, Any]]:
        """çˆ¬å–è¯åˆæ–°èç¶²ç¶“æ¿Ÿæ—¥å ±è²¡ç¶“æ–°è"""
        news_list = []
        
        try:
            print("ğŸ” æ­£åœ¨çˆ¬å–è¯åˆæ–°èç¶²ç¶“æ¿Ÿæ—¥å ±...")
            url = "https://udn.com/news/cate/2/6644"  # ç¶“æ¿Ÿæ—¥å ±é‡‘èè¦è
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # å°‹æ‰¾æ–°èé …ç›®
            news_items = soup.find_all(['div', 'li'], class_=['story-list__item', 'titleicon', 'story-headline'])
            
            for item in news_items[:15]:
                try:
                    # æå–æ¨™é¡Œå’Œé€£çµ
                    title_elem = item.find(['h2', 'h3', 'a'])
                    if not title_elem:
                        continue
                    
                    if title_elem.name == 'a':
                        link_elem = title_elem
                        title = title_elem.get_text(strip=True)
                    else:
                        link_elem = title_elem.find('a')
                        if not link_elem:
                            continue
                        title = link_elem.get_text(strip=True)
                    
                    url = link_elem.get('href', '')
                    
                    # åªè™•ç†ä¿éšªç›¸é—œæ–°è
                    if not self._is_insurance_related(title):
                        continue
                    
                    # ç¢ºä¿URLæ˜¯å®Œæ•´çš„
                    if url.startswith('/'):
                        url = 'https://udn.com' + url
                    
                    news_item = {
                        'title': title,
                        'url': url,
                        'summary': '',
                        'published_date': datetime.now(timezone.utc),
                        'source': 'è¯åˆæ–°èç¶²',
                        'content': ''
                    }
                    
                    news_list.append(news_item)
                    print(f"  âœ… æ‰¾åˆ°ä¿éšªæ–°è: {title[:30]}...")
                    
                    # å»¶é²é¿å…éå¿«è«‹æ±‚
                    time.sleep(random.uniform(1, 2))
                    
                except Exception as e:
                    logger.debug(f"è™•ç†è¯åˆæ–°èç¶²é …ç›®å¤±æ•—: {e}")
                    continue
            
            print(f"âœ… è¯åˆæ–°èç¶²å…±æ‰¾åˆ° {len(news_list)} å‰‡ä¿éšªæ–°è")
            return news_list
            
        except Exception as e:
            print(f"âŒ çˆ¬å–è¯åˆæ–°èç¶²å¤±æ•—: {e}")
            return []
    
    def crawl_ltn_finance(self) -> List[Dict[str, Any]]:
        """çˆ¬å–è‡ªç”±æ™‚å ±è²¡ç¶“æ–°è"""
        news_list = []
        
        try:
            print("ğŸ” æ­£åœ¨çˆ¬å–è‡ªç”±æ™‚å ±è²¡ç¶“æ–°è...")
            url = "https://ec.ltn.com.tw/list/finance"
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æ›´æ–°é¸æ“‡å™¨ä¾†åŒ¹é…è‡ªç”±æ™‚å ±çš„çµæ§‹
            news_items = soup.find_all(['div', 'li'], class_=['tit', 'boxTitle', 'listItem'])
            
            for item in news_items[:15]:  # å¢åŠ æª¢æŸ¥æ•¸é‡
                try:
                    # æå–æ¨™é¡Œå’Œé€£çµ
                    title_elem = item.find(['h2', 'h3', 'a'])
                    if not title_elem:
                        continue
                    
                    # å¦‚æœæ˜¯é€£çµç›´æ¥å–ç”¨ï¼Œå¦å‰‡æ‰¾é€£çµ
                    if title_elem.name == 'a':
                        link_elem = title_elem
                        title = title_elem.get_text(strip=True)
                    else:
                        link_elem = title_elem.find('a')
                        if not link_elem:
                            continue
                        title = link_elem.get_text(strip=True)
                    
                    url = link_elem.get('href', '')
                    
                    # åªè™•ç†ä¿éšªç›¸é—œæ–°è
                    if not self._is_insurance_related(title):
                        continue
                    
                    # ç¢ºä¿URLæ˜¯å®Œæ•´çš„
                    if url.startswith('/'):
                        url = 'https://ec.ltn.com.tw' + url
                    
                    news_item = {
                        'title': title,
                        'url': url,
                        'summary': '',
                        'published_date': datetime.now(timezone.utc),
                        'source': 'è‡ªç”±æ™‚å ±è²¡ç¶“',
                        'content': ''
                    }
                    
                    news_list.append(news_item)
                    print(f"  âœ… æ‰¾åˆ°ä¿éšªæ–°è: {title[:30]}...")
                    
                    # å»¶é²é¿å…éå¿«è«‹æ±‚
                    time.sleep(random.uniform(1, 2))
                    
                except Exception as e:
                    logger.debug(f"è™•ç†è‡ªç”±æ™‚å ±æ–°èé …ç›®å¤±æ•—: {e}")
                    continue
            
            print(f"âœ… è‡ªç”±æ™‚å ±å…±æ‰¾åˆ° {len(news_list)} å‰‡ä¿éšªæ–°è")
            return news_list
            
        except Exception as e:
            print(f"âŒ çˆ¬å–è‡ªç”±æ™‚å ±å¤±æ•—: {e}")
            return []
    
    def _is_insurance_related(self, title: str) -> bool:
        """æª¢æŸ¥æ¨™é¡Œæ˜¯å¦èˆ‡ä¿éšªç›¸é—œ"""
        insurance_keywords = [
            'ä¿éšª', 'ä¿è²»', 'ä¿å–®', 'ç†è³ ', 'æŠ•ä¿', 'æ‰¿ä¿', 'ä¿éšœ',
            'å£½éšª', 'ç”¢éšª', 'å¥åº·éšª', 'æ„å¤–éšª', 'é†«ç™‚éšª', 'ç™Œç—‡éšª',
            'ä¿éšªå…¬å¸', 'ä¿éšªæ¥­', 'é‡‘ç®¡æœƒ', 'é‡‘è', 'ç†è²¡',
            'é€€ä¼‘é‡‘', 'å¹´é‡‘', 'å„²è“„éšª', 'æŠ•è³‡å‹ä¿å–®'
        ]
        
        title_lower = title.lower()
        return any(keyword in title for keyword in insurance_keywords)
    
    def _parse_feed_date(self, entry) -> datetime:
        """è§£æRSS feedçš„æ—¥æœŸ"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                return datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
        except:
            pass
        return datetime.now(timezone.utc)
    
    def crawl_all_sources(self) -> List[Dict[str, Any]]:
        """çˆ¬å–æ‰€æœ‰ä¾†æºçš„æ–°è"""
        all_news = []
        
        print("ğŸš€ é–‹å§‹çˆ¬å–çœŸå¯¦ä¿éšªæ–°è...")
        
        # çˆ¬å–å„ç¨®ä¾†æº
        crawlers = [
            ('Googleæ–°è', self.crawl_google_news),
            ('è¯åˆæ–°èç¶²', self.crawl_udn_finance),
            ('è‡ªç”±æ™‚å ±', self.crawl_ltn_finance),
        ]
        
        for name, crawler in crawlers:
            try:
                print(f"\nğŸ“¡ çˆ¬å–ä¾†æº: {name}")
                news_list = crawler()
                all_news.extend(news_list)
                print(f"âœ… {name} å®Œæˆï¼Œç²å¾— {len(news_list)} å‰‡æ–°è")
                time.sleep(random.uniform(3, 5))  # ä¾†æºé–“å»¶é²
            except Exception as e:
                print(f"âŒ çˆ¬èŸ² {name} åŸ·è¡Œå¤±æ•—: {e}")
                continue
        
        # å»é‡
        unique_news = []
        seen_titles = set()
        
        for news in all_news:
            title_key = news['title'][:50]  # ä½¿ç”¨å‰50å­—ç¬¦ä½œç‚ºå»é‡æ¨™æº–
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        print(f"\nğŸ‰ ç¸½å…±çˆ¬å–åˆ° {len(unique_news)} å‰‡ç¨ç‰¹çš„ä¿éšªæ–°è")
        return unique_news

def test_real_crawler():
    """æ¸¬è©¦çœŸå¯¦çˆ¬èŸ²"""
    crawler = RealInsuranceNewsCrawler()
    
    print("ğŸ§ª æ¸¬è©¦çœŸå¯¦ä¿éšªæ–°èçˆ¬èŸ²...")
    news_list = crawler.crawl_all_sources()
    
    if news_list:
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(news_list)} å‰‡çœŸå¯¦æ–°è:")
        for i, news in enumerate(news_list[:5], 1):
            print(f"\nğŸ“° æ–°è {i}:")
            print(f"æ¨™é¡Œ: {news['title']}")
            print(f"ä¾†æº: {news['source']}")
            print(f"ç¶²å€: {news['url']}")
            if news['summary']:
                print(f"æ‘˜è¦: {news['summary'][:100]}...")
    else:
        print("âŒ æ²’æœ‰æˆåŠŸçˆ¬å–åˆ°ä»»ä½•çœŸå¯¦æ–°è")
    
    return news_list

if __name__ == "__main__":
    test_real_crawler()
