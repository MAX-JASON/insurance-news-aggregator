"""
çœŸå¯¦æ–°èçˆ¬èŸ²å¯¦ä½œ
Real News Crawler Implementation

å¯¦ä½œèƒ½å¤ æŠ“å–çœŸå¯¦ä¿éšªæ–°èçš„çˆ¬èŸ²
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import logging
import time
import random
import re

logger = logging.getLogger('crawler.real')

class RealInsuranceNewsCrawler:
    """çœŸå¯¦ä¿éšªæ–°èçˆ¬èŸ²"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
        
        # å¯ç”¨çš„æ–°èä¾†æº
        self.sources = [
            {
                'name': 'è‡ªç”±æ™‚å ±è²¡ç¶“',
                'base_url': 'https://ec.ltn.com.tw',
                'search_url': 'https://ec.ltn.com.tw/list/finance',
                'type': 'web'
            },
            {
                'name': 'å·¥å•†æ™‚å ±',
                'base_url': 'https://www.chinatimes.com',
                'search_url': 'https://www.chinatimes.com/finance',
                'type': 'web'
            }        ]
    
    def crawl_ltn_finance(self) -> List[Dict[str, Any]]:
        """çˆ¬å–è‡ªç”±æ™‚å ±è²¡ç¶“æ–°è"""
        news_list = []
        
        try:
            print("ğŸ” æ­£åœ¨çˆ¬å–è‡ªç”±æ™‚å ±è²¡ç¶“æ–°è...")
            url = "https://ec.ltn.com.tw/list/finance"
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æ›´æ–°é¸æ“‡å™¨ä¾†åŒ¹é…è‡ªç”±æ™‚å ±çš„çµæ§‹
            news_items = soup.find_all(['div', 'li'], class_=['tit', 'boxTitle', 'listItem'])
            
            for item in news_items[:15]:  # å¢åŠ æª¢æŸ¥æ•¸é‡
                try:
                    # æå–æ¨™é¡Œå’Œé€£çµ
                    title_elem = item.find(['h2', 'h3', 'a'])
                    if not title_elem:
                        continue
                    
                    # å¦‚æœæ˜¯é€£çµç›´æ¥å–ç”¨ï¼Œå¦å‰‡æ‰¾é€£çµ
                    if title_elem.name == 'a':
                        link_elem = title_elem
                        title = title_elem.get_text(strip=True)
                    else:
                        link_elem = title_elem.find('a')
                        if not link_elem:
                            continue
                        title = link_elem.get_text(strip=True)
                    
                    url = link_elem.get('href', '')
                    
                    # åªè™•ç†ä¿éšªç›¸é—œæ–°è
                    if not self._is_insurance_related(title):
                        continue
                    
                    # ç¢ºä¿URLæ˜¯å®Œæ•´çš„
                    if url.startswith('/'):
                        url = 'https://ec.ltn.com.tw' + url
                    
                    # æå–æ™‚é–“
                    time_elem = item.find('span', class_='time')
                    published_date = self._parse_date(time_elem.get_text() if time_elem else '')
                    
                    # æå–æ‘˜è¦
                    summary_elem = item.find('p')
                    summary = summary_elem.get_text(strip=True) if summary_elem else ''
                    
                    news_item = {
                        'title': title,
                        'url': url,
                        'summary': summary[:200],
                        'published_date': published_date,
                        'source': 'è‡ªç”±æ™‚å ±è²¡ç¶“',
                        'content': ''
                    }
                    
                    # å˜—è©¦ç²å–å®Œæ•´å…§å®¹
                    content = self._get_article_content(url)
                    if content:
                        news_item['content'] = content
                    
                    news_list.append(news_item)
                    print(f"  âœ… æ‰¾åˆ°ä¿éšªæ–°è: {title[:30]}...")
                    
                    # å»¶é²é¿å…éå¿«è«‹æ±‚
                    time.sleep(random.uniform(1, 2))
                    
                except Exception as e:
                    logger.debug(f"è™•ç†è‡ªç”±æ™‚å ±æ–°èé …ç›®å¤±æ•—: {e}")
                    continue
            
            print(f"âœ… è‡ªç”±æ™‚å ±å…±æ‰¾åˆ° {len(news_list)} å‰‡ä¿éšªæ–°è")
            return news_list
            
        except Exception as e:
            print(f"âŒ çˆ¬å–è‡ªç”±æ™‚å ±å¤±æ•—: {e}")
            return []
    
    def crawl_udn_finance(self) -> List[Dict[str, Any]]:
        """çˆ¬å–è¯åˆæ–°èç¶²ç¶“æ¿Ÿæ—¥å ±è²¡ç¶“æ–°è"""
        news_list = []
        
        try:
            print("ğŸ” æ­£åœ¨çˆ¬å–è¯åˆæ–°èç¶²ç¶“æ¿Ÿæ—¥å ±...")
            url = "https://udn.com/news/cate/2/6644"  # ç¶“æ¿Ÿæ—¥å ±é‡‘èè¦è
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # å°‹æ‰¾æ–°èé …ç›®
            news_items = soup.find_all(['div', 'li'], class_=['story-list__item', 'titleicon', 'story-headline'])
            
            for item in news_items[:15]:
                try:
                    # æå–æ¨™é¡Œå’Œé€£çµ
                    title_elem = item.find(['h2', 'h3', 'a'])
                    if not title_elem:
                        continue
                    
                    if title_elem.name == 'a':
                        link_elem = title_elem
                        title = title_elem.get_text(strip=True)
                    else:
                        link_elem = title_elem.find('a')
                        if not link_elem:
                            continue
                        title = link_elem.get_text(strip=True)
                    
                    url = link_elem.get('href', '')
                    
                    # åªè™•ç†ä¿éšªç›¸é—œæ–°è
                    if not self._is_insurance_related(title):
                        continue
                    
                    # ç¢ºä¿URLæ˜¯å®Œæ•´çš„
                    if url.startswith('/'):
                        url = 'https://udn.com' + url
                    
                    # æå–æ™‚é–“å’Œæ‘˜è¦
                    time_elem = item.find('time')
                    if not time_elem:
                        time_elem = item.find('span', class_='story-list__time')
                    
                    published_date = self._parse_date(time_elem.get_text() if time_elem else '')
                    
                    # æå–æ‘˜è¦
                    summary_elem = item.find('p', class_='story-list__summary')
                    if not summary_elem:
                        summary_elem = item.find('p')
                    summary = summary_elem.get_text(strip=True) if summary_elem else ''
                    
                    news_item = {
                        'title': title,
                        'url': url,
                        'summary': summary[:200],
                        'published_date': published_date,
                        'source': 'è¯åˆæ–°èç¶²',
                        'content': ''
                    }
                    
                    # å˜—è©¦ç²å–å®Œæ•´å…§å®¹
                    content = self._get_article_content(url)
                    if content:
                        news_item['content'] = content
                    
                    news_list.append(news_item)
                    print(f"  âœ… æ‰¾åˆ°ä¿éšªæ–°è: {title[:30]}...")
                    
                    # å»¶é²é¿å…éå¿«è«‹æ±‚
                    time.sleep(random.uniform(1, 2))
                    
                except Exception as e:
                    logger.debug(f"è™•ç†è¯åˆæ–°èç¶²é …ç›®å¤±æ•—: {e}")
                    continue
            
            print(f"âœ… è¯åˆæ–°èç¶²å…±æ‰¾åˆ° {len(news_list)} å‰‡ä¿éšªæ–°è")
            return news_list
            
        except Exception as e:
            print(f"âŒ çˆ¬å–è¯åˆæ–°èç¶²å¤±æ•—: {e}")
            return []
    
    def crawl_yahoo_news(self) -> List[Dict[str, Any]]:
        """çˆ¬å–Yahooæ–°èçš„ä¿éšªé—œéµå­—æœç´¢çµæœ"""
        news_list = []
        
        try:
            print("ğŸ” æ­£åœ¨æœç´¢Yahooæ–°èä¿éšªå…§å®¹...")
            
            # ç°¡åŒ–Yahooæ–°èæœç´¢
            search_url = "https://tw.news.yahoo.com/tag/ä¿éšª"
            
            response = self.session.get(search_url, timeout=15)
            if response.status_code != 200:
                print(f"Yahooæ–°èå›æ‡‰ç‹€æ…‹: {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # å°‹æ‰¾æ–°èé …ç›®
            news_items = soup.find_all(['h3', 'h4', 'div'], class_=['Mb(5px)', 'title', 'StreamMegaItem'])
            
            for item in news_items[:10]:
                try:
                    # æå–æ¨™é¡Œå’Œé€£çµ
                    title_elem = item.find('a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get('href', '')
                    
                    # åªè™•ç†ä¿éšªç›¸é—œæ–°è
                    if not self._is_insurance_related(title):
                        continue
                    
                    # ç¢ºä¿URLæ˜¯å®Œæ•´çš„
                    if url.startswith('/'):
                        url = 'https://tw.news.yahoo.com' + url
                    elif not url.startswith('http'):
                        continue
                    
                    news_item = {
                        'title': title,
                        'url': url,
                        'summary': '',
                        'published_date': datetime.now(timezone.utc),
                        'source': 'Yahooæ–°è',
                        'content': ''
                    }
                    
                    news_list.append(news_item)
                    print(f"  âœ… æ‰¾åˆ°Yahooæ–°è: {title[:30]}...")
                    
                except Exception as e:
                    logger.debug(f"è™•ç†Yahooæ–°èé …ç›®å¤±æ•—: {e}")
                    continue
            
            print(f"âœ… Yahooæ–°èå…±æ‰¾åˆ° {len(news_list)} å‰‡ç›¸é—œæ–°è")
            return news_list
            
        except Exception as e:
            print(f"âŒ æœç´¢Yahooæ–°èå¤±æ•—: {e}")
            return []
        """çˆ¬å–Yahooæ–°èçš„ä¿éšªé—œéµå­—æœç´¢çµæœ"""
        news_list = []
        
        try:
            print("ğŸ” æ­£åœ¨æœç´¢Yahooæ–°èä¿éšªå…§å®¹...")
            
            # Yahooæ–°èæœç´¢APIæˆ–ç¶²é 
            search_keywords = ['ä¿éšª', 'ä¿è²»', 'ç†è³ ', 'å£½éšª', 'ç”¢éšª']
            
            for keyword in search_keywords[:2]:  # é™åˆ¶æœç´¢é—œéµå­—
                try:
                    # ä½¿ç”¨Yahooæ–°èæœç´¢
                    search_url = f"https://tw.news.yahoo.com/tag/{keyword}"
                    
                    response = self.session.get(search_url, timeout=15)
                    if response.status_code != 200:
                        continue
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # å°‹æ‰¾æ–°èé …ç›®ï¼ˆYahooçš„çµæ§‹å¯èƒ½è®ŠåŒ–ï¼‰
                    # é€™è£¡ä½¿ç”¨é€šç”¨çš„æ¨™ç±¤æœç´¢
                    potential_links = soup.find_all('a', href=True)
                    
                    for link in potential_links[:20]:
                        try:
                            href = link.get('href', '')
                            text = link.get_text(strip=True)
                            
                            # éæ¿¾æ˜é¡¯çš„æ–°èé€£çµ
                            if (len(text) > 10 and 
                                any(word in text for word in ['ä¿éšª', 'ä¿è²»', 'ç†è³ ']) and
                                'news' in href and
                                len(text) < 100):
                                
                                # ç¢ºä¿URLå®Œæ•´
                                if href.startswith('/'):
                                    href = 'https://tw.news.yahoo.com' + href
                                elif not href.startswith('http'):
                                    continue
                                
                                news_item = {
                                    'title': text,
                                    'url': href,
                                    'summary': '',
                                    'published_date': datetime.now(timezone.utc),
                                    'source': 'Yahooæ–°è',
                                    'content': ''
                                }
                                
                                # é¿å…é‡è¤‡
                                if not any(n['title'] == text for n in news_list):
                                    news_list.append(news_item)
                                    print(f"  âœ… æ‰¾åˆ°Yahooæ–°è: {text[:30]}...")
                                
                                if len(news_list) >= 5:  # é™åˆ¶æ•¸é‡
                                    break
                        except:
                            continue
                    
                    time.sleep(random.uniform(2, 3))
                    
                except Exception as e:
                    logger.debug(f"æœç´¢Yahooé—œéµå­— {keyword} å¤±æ•—: {e}")
                    continue
            
            print(f"âœ… Yahooæ–°èå…±æ‰¾åˆ° {len(news_list)} å‰‡ç›¸é—œæ–°è")
            return news_list
            
        except Exception as e:
            print(f"âŒ æœç´¢Yahooæ–°èå¤±æ•—: {e}")
            return []
      def crawl_google_news(self) -> List[Dict[str, Any]]:
        """æœç´¢Googleæ–°èä¸­çš„ä¿éšªç›¸é—œå…§å®¹"""
        news_list = []
        
        try:
            print("ğŸ” æ­£åœ¨æœç´¢Googleæ–°èä¿éšªå…§å®¹...")
            
            # ä½¿ç”¨Googleæ–°èæœç´¢ - ä¿®å¾©URLç·¨ç¢¼å•é¡Œ
            import urllib.parse
            search_query = "å°ç£ä¿éšª"
            encoded_query = urllib.parse.quote(search_query)
            search_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
            
            # å˜—è©¦RSSæ–¹å¼
            import feedparser
            
            feed = feedparser.parse(search_url)
            
            if not feed.bozo and hasattr(feed, 'entries'):
                for entry in feed.entries[:8]:  # é™åˆ¶8å‰‡æ–°è
                    try:
                        title = entry.get('title', '')
                        if not self._is_insurance_related(title):
                            continue
                        
                        news_item = {
                            'title': title,
                            'url': entry.get('link', ''),
                            'summary': entry.get('summary', '')[:200],
                            'published_date': self._parse_feed_date(entry),
                            'source': 'Googleæ–°è',
                            'content': ''
                        }
                        
                        news_list.append(news_item)
                        print(f"  âœ… æ‰¾åˆ°Googleæ–°è: {title[:30]}...")
                        
                    except Exception as e:
                        logger.debug(f"è™•ç†Googleæ–°èé …ç›®å¤±æ•—: {e}")
                        continue
            
            print(f"âœ… Googleæ–°èå…±æ‰¾åˆ° {len(news_list)} å‰‡ä¿éšªæ–°è")
            return news_list
            
        except Exception as e:
            print(f"âŒ æœç´¢Googleæ–°èå¤±æ•—: {e}")
            return []
    
    def _is_insurance_related(self, title: str) -> bool:
        """æª¢æŸ¥æ¨™é¡Œæ˜¯å¦èˆ‡ä¿éšªç›¸é—œ"""
        insurance_keywords = [
            'ä¿éšª', 'ä¿è²»', 'ä¿å–®', 'ç†è³ ', 'æŠ•ä¿', 'æ‰¿ä¿', 'ä¿éšœ',
            'å£½éšª', 'ç”¢éšª', 'å¥åº·éšª', 'æ„å¤–éšª', 'é†«ç™‚éšª', 'ç™Œç—‡éšª',
            'é‡‘ç®¡æœƒ', 'ä¿éšªå…¬å¸', 'ä¿éšªæ¥­', 'ä¿éšªæ³•', 'ä¿éšªé‡‘',
            'å¹´é‡‘', 'é€€ä¼‘é‡‘', 'ä¿éšªç§‘æŠ€', 'Insurtech', 'å†ä¿éšª'
        ]
        
        return any(keyword in title for keyword in insurance_keywords)
    
    def _parse_date(self, date_str: str) -> datetime:
        """è§£ææ—¥æœŸå­—ä¸²"""
        try:
            # è™•ç†å¸¸è¦‹çš„æ—¥æœŸæ ¼å¼
            if 'å°æ™‚å‰' in date_str:
                return datetime.now(timezone.utc)
            elif 'åˆ†é˜å‰' in date_str:
                return datetime.now(timezone.utc)
            elif 'ä»Šå¤©' in date_str or 'ä»Šæ—¥' in date_str:
                return datetime.now(timezone.utc)
            elif 'æ˜¨å¤©' in date_str or 'æ˜¨æ—¥' in date_str:
                return datetime.now(timezone.utc)
            else:
                return datetime.now(timezone.utc)
        except:
            return datetime.now(timezone.utc)
    
    def _parse_feed_date(self, entry) -> datetime:
        """è§£æRSS feedçš„æ—¥æœŸ"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                return datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
            else:
                return datetime.now(timezone.utc)
        except:
            return datetime.now(timezone.utc)
    
    def _get_article_content(self, url: str) -> str:
        """ç²å–æ–‡ç« å®Œæ•´å…§å®¹"""
        try:
            if not url:
                return ""
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement']):
                element.decompose()
            
            # å°‹æ‰¾å…§å®¹å®¹å™¨
            content_selectors = [
                'div.news_content',
                'div.article-body',
                'div.post-content',
                'article',
                '.content',
                '.newstext'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    paragraphs = content_elem.find_all('p')
                    content = '\n'.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 10])
                    break
            
            # å¦‚æœæ²’æ‰¾åˆ°ï¼Œæå–æ‰€æœ‰æ®µè½
            if not content:
                paragraphs = soup.find_all('p')
                content = '\n'.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            
            return content[:2000] if content else ""
            
        except Exception as e:
            logger.debug(f"ç²å–æ–‡ç« å…§å®¹å¤±æ•— {url}: {e}")
            return ""
      def crawl_all_sources(self) -> List[Dict[str, Any]]:
        """çˆ¬å–æ‰€æœ‰ä¾†æºçš„æ–°è"""
        all_news = []
        
        print("ğŸš€ é–‹å§‹çˆ¬å–çœŸå¯¦ä¿éšªæ–°è...")
        
        # çˆ¬å–å„ç¨®ä¾†æº
        crawlers = [
            ('Googleæ–°è', self.crawl_google_news),
            ('è¯åˆæ–°èç¶²', self.crawl_udn_finance),
            ('è‡ªç”±æ™‚å ±', self.crawl_ltn_finance),
            ('Yahooæ–°è', self.crawl_yahoo_news),
        ]
        
        for name, crawler in crawlers:
            try:
                print(f"\nğŸ“¡ çˆ¬å–ä¾†æº: {name}")
                news_list = crawler()
                all_news.extend(news_list)
                print(f"âœ… {name} å®Œæˆï¼Œç²å¾— {len(news_list)} å‰‡æ–°è")
                time.sleep(random.uniform(3, 5))  # ä¾†æºé–“å»¶é²
            except Exception as e:
                print(f"âŒ çˆ¬èŸ² {name} åŸ·è¡Œå¤±æ•—: {e}")
                continue
        
        # å»é‡
        unique_news = []
        seen_titles = set()
        
        for news in all_news:
            title_key = news['title'][:50]  # ä½¿ç”¨å‰50å­—ç¬¦ä½œç‚ºå»é‡æ¨™æº–
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        print(f"\nğŸ‰ ç¸½å…±çˆ¬å–åˆ° {len(unique_news)} å‰‡ç¨ç‰¹çš„ä¿éšªæ–°è")
        return unique_news

def test_real_crawler():
    """æ¸¬è©¦çœŸå¯¦çˆ¬èŸ²"""
    crawler = RealInsuranceNewsCrawler()
    
    print("ğŸ§ª æ¸¬è©¦çœŸå¯¦ä¿éšªæ–°èçˆ¬èŸ²...")
    news_list = crawler.crawl_all_sources()
    
    if news_list:
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(news_list)} å‰‡çœŸå¯¦æ–°è:")
        for i, news in enumerate(news_list[:5], 1):
            print(f"\nğŸ“° æ–°è {i}:")
            print(f"æ¨™é¡Œ: {news['title']}")
            print(f"ä¾†æº: {news['source']}")
            print(f"ç¶²å€: {news['url']}")
            if news['summary']:
                print(f"æ‘˜è¦: {news['summary'][:100]}...")
            if news['content']:
                print(f"å…§å®¹: {news['content'][:100]}...")
    else:
        print("âŒ æ²’æœ‰æˆåŠŸçˆ¬å–åˆ°ä»»ä½•çœŸå¯¦æ–°è")
    
    return news_list

if __name__ == "__main__":
    test_real_crawler()
