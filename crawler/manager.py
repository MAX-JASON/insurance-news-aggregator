"""
çˆ¬èŸ²ç®¡ç†å™¨
Crawler Manager

çµ±ä¸€ç®¡ç†æ‰€æœ‰çˆ¬èŸ²çš„åŸ·è¡Œå’Œèª¿åº¦
"""

import logging
import threading
import time
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

# é¿å…å¾ªç’°ä¾è³´
import importlib

# å°å…¥è¼”åŠ©æ¨¡çµ„
from crawler.helper import CrawlerHelper
from crawler.date_filter import NewsDateFilter

logger = logging.getLogger('crawler.manager')

class CrawlerManager:
    """çˆ¬èŸ²ç®¡ç†å™¨"""
    
    def __init__(self):
        # åˆå§‹åŒ–æ—¥æœŸéæ¿¾å™¨
        self.date_filter = NewsDateFilter()
        
        # å‹•æ…‹åŠ è¼‰æ¨¡çµ„
        mock_generator = importlib.import_module('crawler.mock_generator').MockNewsGenerator
        rss_crawler = importlib.import_module('crawler.rss_crawler').RSSNewsCrawler
        
        try:
            ctee_crawler = importlib.import_module('crawler.ctee_insurance_crawler').CTeeInsuranceCrawler
            logger.info("å·²è¼‰å…¥çˆ¬èŸ²: å·¥å•†æ™‚å ±ä¿éšªç‰ˆ")
        except Exception as e:
            logger.error(f"è¼‰å…¥å·¥å•†æ™‚å ±çˆ¬èŸ²å¤±æ•—: {e}")
            ctee_crawler = None

        # åˆå§‹åŒ–çˆ¬èŸ²
        self.crawlers = {
            'mock': mock_generator(),
            'rss': rss_crawler(),
        }
        
        if ctee_crawler:
            self.crawlers['ctee'] = ctee_crawler()
        
        try:
            # å…¶ä»–å¯èƒ½çš„çˆ¬èŸ²
            real_crawler = importlib.import_module('crawler.real_crawler_fixed').RealInsuranceNewsCrawler
            self.crawlers['real'] = real_crawler()
            logger.info("å·²è¼‰å…¥çˆ¬èŸ²: ç¤ºç¯„çˆ¬èŸ²")
        except Exception as e:
            logger.warning(f"è¼‰å…¥çœŸå¯¦çˆ¬èŸ²å¤±æ•— (éé—œéµéŒ¯èª¤): {e}")
        
        self.is_running = False
        self.last_crawl_time = None
        self.stats = {
            'total_news': 0,
            'successful_crawls': 0,
            'failed_crawls': 0
        }
        
        # çˆ¬èŸ²æ§åˆ¶è¨­ç½®
        self.auto_crawl_enabled = True  # æ˜¯å¦å•Ÿç”¨è‡ªå‹•çˆ¬èŸ²
        self.scheduler_thread = None  # å®šæ™‚çˆ¬èŸ²ç·šç¨‹
        self.should_stop = False  # åœæ­¢ä¿¡è™Ÿ
    
    def run_crawlers(self, source_name=None, limit=10):
        """
        é‹è¡Œçˆ¬èŸ²çš„å…¼å®¹æ–¹æ³•
        
        Args:
            source_name: æ–°èä¾†æºåç¨±
            limit: æŠ“å–æ•¸é‡é™åˆ¶
            
        Returns:
            çµæœå­—å…¸
        """
        logger.info(f"é‹è¡Œçˆ¬èŸ²: {source_name or 'å…¨éƒ¨'}, é™åˆ¶: {limit}")
        
        if source_name:
            # é‹è¡Œç‰¹å®šçˆ¬èŸ²
            crawler = self.crawlers.get(source_name)
            if not crawler:
                return {
                    'status': 'error',
                    'message': f'æ‰¾ä¸åˆ°çˆ¬èŸ²: {source_name}',
                    'total': 0,
                    'new': 0
                }
                
            try:
                if hasattr(crawler, 'crawl'):
                    result = crawler.crawl(max_pages=1, max_details=limit)
                    news_items = result.get('news', [])
                else:
                    news_items = crawler.generate_news(count=limit)
                
                return {
                    'status': 'success',
                    'message': 'çˆ¬èŸ²åŸ·è¡ŒæˆåŠŸ',
                    'total': len(news_items),
                    'new': len(news_items),
                    'news': news_items
                }
            except Exception as e:
                logger.error(f"çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
                return {
                    'status': 'error',
                    'message': f'çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}',
                    'total': 0,
                    'new': 0
                }
        else:
            # é‹è¡Œæ‰€æœ‰çˆ¬èŸ²
            return self.crawl_all_sources(use_mock=True)
    
    def crawl_all_sources(self, use_mock: bool = True) -> Dict[str, Any]:
        """çˆ¬å–æ‰€æœ‰ä¾†æºçš„æ–°è"""
        if self.is_running:
            return {'status': 'error', 'message': 'çˆ¬èŸ²å·²åœ¨é‹è¡Œä¸­'}
        
        self.is_running = True
        start_time = datetime.now(timezone.utc)
        all_news = []
        crawl_results = []
        
        try:
            logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œæ–°èçˆ¬å–ä»»å‹™")
            
            if use_mock:
                # ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š
                logger.info("ğŸ“ ä½¿ç”¨æ¨¡æ“¬æ–°èç”Ÿæˆå™¨")
                mock_news = self.crawlers['mock'].generate_news(count=5)
                all_news.extend(mock_news)
                crawl_results.append({
                    'source': 'æ¨¡æ“¬æ–°èç”Ÿæˆå™¨',
                    'success': True,
                    'news_count': len(mock_news),
                    'message': 'æˆåŠŸç”Ÿæˆæ¨¡æ“¬æ–°è'
                })
                self.stats['successful_crawls'] += 1
            
            # å˜—è©¦å·¥å•†æ™‚å ±çˆ¬èŸ²
            if 'ctee' in self.crawlers:
                logger.info("ğŸ” å˜—è©¦å·¥å•†æ™‚å ±ä¿éšªç‰ˆçˆ¬èŸ²")
                try:
                    ctee_crawler = self.crawlers['ctee']
                    result = ctee_crawler.crawl(max_pages=1, max_details=10)
                    ctee_news = result.get('news', [])
                    all_news.extend(ctee_news)
                    crawl_results.append({
                        'source': 'å·¥å•†æ™‚å ±ä¿éšªç‰ˆ',
                        'success': True,
                        'news_count': len(ctee_news),
                        'message': f'æˆåŠŸçˆ¬å–å·¥å•†æ™‚å ±æ–°è {len(ctee_news)} å‰‡'
                    })
                    self.stats['successful_crawls'] += 1
                except Exception as e:
                    logger.error(f"å·¥å•†æ™‚å ±çˆ¬èŸ²å¤±æ•—: {e}")
                    crawl_results.append({
                        'source': 'å·¥å•†æ™‚å ±ä¿éšªç‰ˆ', 
                        'success': False,
                        'news_count': 0,
                        'message': f'çˆ¬å–å¤±æ•—: {str(e)}'
                    })
                    self.stats['failed_crawls'] += 1
            
            # å˜—è©¦å…¶ä»–çˆ¬èŸ²
            if 'real' in self.crawlers:
                # ä½¿ç”¨çœŸå¯¦çˆ¬èŸ²
                logger.info("ğŸ” å˜—è©¦çœŸå¯¦æ–°èçˆ¬èŸ²")
                try:
                    real_news = self.crawlers['real'].crawl_all_sources()
                    all_news.extend(real_news)
                    crawl_results.append({
                        'source': 'çœŸå¯¦æ–°èçˆ¬èŸ²',
                        'success': True,
                        'news_count': len(real_news),
                        'message': f'æˆåŠŸçˆ¬å–çœŸå¯¦æ–°è {len(real_news)} å‰‡'
                    })
                    self.stats['successful_crawls'] += 1
                except Exception as e:
                    logger.error(f"çœŸå¯¦çˆ¬èŸ²å¤±æ•—: {e}")
                    crawl_results.append({
                        'source': 'çœŸå¯¦çˆ¬èŸ²', 
                        'success': False,
                        'news_count': 0,
                        'message': f'çˆ¬å–å¤±æ•—: {str(e)}'
                    })
                    self.stats['failed_crawls'] += 1
            
            # å‚™ç”¨RSSçˆ¬èŸ² (å¯¦é©—æ€§)
            logger.info("ğŸ” å˜—è©¦RSSçˆ¬èŸ² (å‚™ç”¨)")
            try:
                rss_news = self.crawlers['rss'].crawl_all_feeds()
                all_news.extend(rss_news)
                crawl_results.append({
                    'source': 'RSSçˆ¬èŸ²',
                    'success': True,
                    'news_count': len(rss_news),
                    'message': 'æˆåŠŸçˆ¬å–RSSæ–°è'
                })
                self.stats['successful_crawls'] += 1
            except Exception as e:
                logger.error(f"RSSçˆ¬èŸ²å¤±æ•—: {e}")
                crawl_results.append({
                    'source': 'RSSçˆ¬èŸ²', 
                    'success': False,
                    'news_count': 0,
                    'message': f'çˆ¬å–å¤±æ•—: {str(e)}'
                })
                self.stats['failed_crawls'] += 1
            
            # å„²å­˜æ–°èåˆ°è³‡æ–™åº«
            if all_news:
                # æ‡‰ç”¨æ—¥æœŸéæ¿¾
                filtered_news = self.date_filter.filter_news_list(all_news)
                if filtered_news:
                    saved_count = self._save_news_to_database(filtered_news)
                    self.stats['total_news'] += saved_count
                    logger.info(f"ğŸ’¾ æˆåŠŸå„²å­˜ {saved_count} å‰‡æ–°èåˆ°è³‡æ–™åº«")
                else:
                    logger.info("âš ï¸ ç¶“éæ—¥æœŸéæ¿¾å¾Œæ²’æœ‰æ–°èéœ€è¦å„²å­˜")
            
            self.last_crawl_time = start_time
            
            return {
                'status': 'success',
                'message': f'çˆ¬å–å®Œæˆï¼Œå…±è™•ç† {len(all_news)} å‰‡æ–°èï¼Œéæ¿¾å¾Œä¿å­˜ {len(filtered_news) if "filtered_news" in locals() else 0} å‰‡',
                'start_time': start_time.isoformat(),
                'duration': (datetime.now(timezone.utc) - start_time).total_seconds(),
                'results': crawl_results,
                'stats': self.stats,
                'filter_status': self.date_filter.get_status(),
                'total': len(all_news),
                'new': len(filtered_news) if "filtered_news" in locals() else 0
            }
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–ä»»å‹™åŸ·è¡Œå¤±æ•—: {e}")
            return {
                'status': 'error',
                'message': f'çˆ¬å–å¤±æ•—: {str(e)}',
                'results': crawl_results,
                'total': 0,
                'new': 0
            }
        finally:
            self.is_running = False
    
    def _save_news_to_database(self, news_list: List[Dict[str, Any]]) -> int:
        """å°‡æ–°èå„²å­˜åˆ°è³‡æ–™åº«"""
        try:
            # å‹•æ…‹å°å…¥é¿å…å¾ªç’°å°å…¥
            import sys
            import os
            sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            
            from app import create_app
            from config.settings import Config
            from database.models import db, News, NewsCategory, NewsSource
            
            app = create_app(Config)
            saved_count = 0
            
            with app.app_context():
                for news_data in news_list:
                    try:
                        # é–‹å§‹æ–°çš„äº‹å‹™è™•ç†æ¯æ¢æ–°è
                        with db.session.begin():
                            # æª¢æŸ¥é‡è¤‡ - ä½¿ç”¨URLæª¢æŸ¥(æ›´å¯é )
                            url = news_data.get('url', '')
                            if url:
                                # ä½¿ç”¨ no_autoflush é¿å…åœ¨æŸ¥è©¢æ™‚è§¸ç™¼ flush
                                with db.session.no_autoflush:
                                    existing = News.query.filter_by(url=url).first()
                                    if existing:
                                        logger.info(f"æ–°èå·²å­˜åœ¨ï¼Œè·³é: {news_data.get('title', 'No title')}")
                                        continue
                            
                            # å¦‚æœæ²’æœ‰URLï¼Œå‰‡æª¢æŸ¥æ¨™é¡Œ
                            if not url:
                                with db.session.no_autoflush:
                                    existing = News.query.filter_by(title=news_data['title']).first()
                                    if existing:
                                        logger.info(f"ç›¸åŒæ¨™é¡Œæ–°èå·²å­˜åœ¨ï¼Œè·³é: {news_data['title']}")
                                        continue
                            
                            # è™•ç†åˆ†é¡
                            category_name = news_data.get('category', 'å…¶ä»–')
                            with db.session.no_autoflush:
                                category = NewsCategory.query.filter_by(name=category_name).first()
                            if not category:
                                category = NewsCategory(
                                    name=category_name,
                                    description=f"{category_name}ç›¸é—œæ–°è"
                                )
                                db.session.add(category)
                                db.session.flush()
                            
                            # è™•ç†ä¾†æº
                            source_name = news_data.get('source', 'æœªçŸ¥ä¾†æº')
                            with db.session.no_autoflush:
                                source = NewsSource.query.filter_by(name=source_name).first()
                            if not source:
                                source = NewsSource(
                                    name=source_name,
                                    url='',
                                    description=f"{source_name}æ–°èä¾†æº"
                                )
                                db.session.add(source)
                                db.session.flush()
                            
                            # å‰µå»ºæ–°è
                            news = News(
                                title=news_data['title'],
                                content=news_data.get('content', ''),
                                summary=news_data.get('summary', ''),
                                url=news_data.get('url', ''),
                                source_id=source.id,
                                category_id=category.id,
                                published_date=news_data.get('published_date', datetime.now(timezone.utc)),
                                crawled_date=datetime.now(timezone.utc),
                                keywords=news_data.get('keywords', ''),
                                importance_score=news_data.get('importance_score', 0.5),
                                sentiment_score=news_data.get('sentiment_score', 0.0),
                                status='active'
                            )
                            
                            db.session.add(news)
                            # äº‹å‹™æœƒåœ¨ with å¡ŠçµæŸæ™‚è‡ªå‹•æäº¤
                            saved_count += 1
                            
                    except Exception as e:
                        logger.error(f"å„²å­˜å–®å‰‡æ–°èå¤±æ•—: {e}")
                        # äº‹å‹™æœƒè‡ªå‹•å›æ»¾
                        continue
                
                return saved_count
                
        except Exception as e:
            logger.error(f"è³‡æ–™åº«æ“ä½œå¤±æ•—: {e}")
            return 0
    
    def get_crawler_status(self) -> Dict[str, Any]:
        """ç²å–çˆ¬èŸ²ç‹€æ…‹"""
        return {
            'is_running': self.is_running,
            'auto_crawl_enabled': self.auto_crawl_enabled,
            'scheduler_running': self.scheduler_thread is not None and self.scheduler_thread.is_alive(),
            'last_crawl_time': self.last_crawl_time.isoformat() if self.last_crawl_time else None,
            'stats': self.stats,
            'available_crawlers': list(self.crawlers.keys()),
            'date_filter': self.date_filter.get_status()
        }
    
    def update_date_filter_settings(self, max_age_days: int = None, enable_filter: bool = None) -> Dict[str, Any]:
        """
        æ›´æ–°æ—¥æœŸéæ¿¾å™¨è¨­å®š
        
        Args:
            max_age_days: æœ€å¤§å¤©æ•¸
            enable_filter: æ˜¯å¦å•Ÿç”¨éæ¿¾
            
        Returns:
            Dict: æ›´æ–°å¾Œçš„ç‹€æ…‹
        """
        self.date_filter.update_settings(max_age_days=max_age_days, enable_filter=enable_filter)
        
        status_msg = []
        if max_age_days is not None:
            status_msg.append(f"æœ€å¤§å¤©æ•¸è¨­ç‚º {max_age_days} å¤©")
        if enable_filter is not None:
            status_msg.append(f"éæ¿¾åŠŸèƒ½{'å•Ÿç”¨' if enable_filter else 'åœç”¨'}")
        
        logger.info(f"æ—¥æœŸéæ¿¾å™¨è¨­å®šå·²æ›´æ–°: {', '.join(status_msg)}")
        
        return {
            'status': 'success',
            'message': f"æ—¥æœŸéæ¿¾å™¨è¨­å®šå·²æ›´æ–°: {', '.join(status_msg)}",
            'filter_status': self.date_filter.get_status()
        }
    
    def start_scheduled_crawling(self, interval_minutes: int = 60):
        """é–‹å§‹å®šæœŸçˆ¬å– (èƒŒæ™¯åŸ·è¡Œ)"""
        # å¦‚æœå·²ç¶“æœ‰çˆ¬èŸ²ç·šç¨‹åœ¨é‹è¡Œï¼Œå‰‡ä¸é‡è¤‡å•Ÿå‹•
        if self.scheduler_thread and self.scheduler_thread.is_alive():
            logger.info("å®šæœŸçˆ¬èŸ²å·²åœ¨é‹è¡Œä¸­ï¼Œä¸é‡è¤‡å•Ÿå‹•")
            return False
            
        # é‡ç½®åœæ­¢ä¿¡è™Ÿ
        self.should_stop = False
        self.auto_crawl_enabled = True
        
        def run_scheduler():
            while not self.should_stop:
                try:
                    if self.auto_crawl_enabled:
                        logger.info(f"â° å®šæœŸçˆ¬å–ä»»å‹™é–‹å§‹ (é–“éš”: {interval_minutes}åˆ†é˜)")
                        result = self.crawl_all_sources(use_mock=True)
                        logger.info(f"â° å®šæœŸçˆ¬å–å®Œæˆ: {result['message']}")
                    
                    # ç­‰å¾…ä¸‹æ¬¡åŸ·è¡Œï¼Œæ¯5ç§’æª¢æŸ¥ä¸€æ¬¡æ˜¯å¦æ‡‰è©²åœæ­¢
                    for _ in range(int(interval_minutes * 60 / 5)):
                        if self.should_stop:
                            break
                        time.sleep(5)
                        
                except Exception as e:
                    logger.error(f"å®šæœŸçˆ¬å–ä»»å‹™å¤±æ•—: {e}")
                    # å¤±æ•—å¾Œç­‰å¾…1åˆ†é˜å†è©¦ï¼Œä½†åŒæ™‚æª¢æŸ¥æ˜¯å¦æ‡‰è©²åœæ­¢
                    for _ in range(12):  # 1åˆ†é˜ï¼Œæ¯5ç§’æª¢æŸ¥ä¸€æ¬¡
                        if self.should_stop:
                            break
                        time.sleep(5)
            
            logger.info("å®šæœŸçˆ¬èŸ²å·²åœæ­¢")
        
        # åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­åŸ·è¡Œ
        self.scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        self.scheduler_thread.start()
        logger.info(f"ğŸ“… å®šæœŸçˆ¬å–å·²å•Ÿå‹• (é–“éš”: {interval_minutes}åˆ†é˜)")
        return True
        
    def stop_scheduled_crawling(self):
        """åœæ­¢å®šæœŸçˆ¬å–"""
        if not self.scheduler_thread or not self.scheduler_thread.is_alive():
            logger.info("å®šæœŸçˆ¬èŸ²æœªåœ¨é‹è¡Œä¸­")
            return False
            
        logger.info("æ­£åœ¨åœæ­¢å®šæœŸçˆ¬èŸ²...")
        self.should_stop = True
        return True
        
    def toggle_auto_crawl(self, enabled=None):
        """åˆ‡æ›è‡ªå‹•çˆ¬èŸ²åŠŸèƒ½
        
        Args:
            enabled: æ˜¯å¦å•Ÿç”¨ï¼Œå¦‚æœä¸æä¾›å‰‡åˆ‡æ›ç•¶å‰ç‹€æ…‹
        """
        if enabled is None:
            self.auto_crawl_enabled = not self.auto_crawl_enabled
        else:
            self.auto_crawl_enabled = enabled
            
        status = "å•Ÿç”¨" if self.auto_crawl_enabled else "ç¦ç”¨"
        logger.info(f"è‡ªå‹•çˆ¬èŸ²åŠŸèƒ½å·²{status}")
        return self.auto_crawl_enabled
        
    def run_all_crawlers(self, use_real=True) -> Dict[str, Any]:
        """
        é‹è¡Œæ‰€æœ‰å¯ç”¨çˆ¬èŸ²ï¼Œå„ªå…ˆä½¿ç”¨çœŸå¯¦çˆ¬èŸ²
        
        Args:
            use_real: æ˜¯å¦ä½¿ç”¨çœŸå¯¦çˆ¬èŸ²ï¼ˆè€Œéåƒ…æ¨¡æ“¬æ•¸æ“šï¼‰
            
        Returns:
            Dict åŒ…å«çˆ¬å–çµæœ
        """
        logger.info(f"ğŸ“¡ é‹è¡Œæ‰€æœ‰çˆ¬èŸ² (ä½¿ç”¨çœŸå¯¦çˆ¬èŸ²: {use_real})")
        
        if self.is_running:
            logger.warning("âš ï¸ çˆ¬èŸ²å·²åœ¨é‹è¡Œä¸­ï¼Œå¿½ç•¥æ­¤æ¬¡è«‹æ±‚")
            return {
                'status': 'error',
                'message': 'çˆ¬èŸ²å·²åœ¨é‹è¡Œä¸­ï¼Œè«‹ç¨å¾Œå†è©¦',
                'total': 0,
                'new': 0
            }
        
        self.is_running = True
        try:
            # ä½¿ç”¨è¼”åŠ©æ¨¡çµ„åŸ·è¡Œçˆ¬èŸ²
            result = CrawlerHelper.run_all_crawlers(self, use_real=use_real)
            
            # è™•ç†çµæœ
            all_news = result.get('news', [])
            
            # å°‡æ–°èä¿å­˜åˆ°è³‡æ–™åº«
            if all_news:
                # æ‡‰ç”¨æ—¥æœŸéæ¿¾
                filtered_news = self.date_filter.filter_news_list(all_news)
                if filtered_news:
                    saved_count = self._save_news_to_database(filtered_news)
                    self.stats['total_news'] += saved_count
                    logger.info(f"ğŸ’¾ æˆåŠŸå„²å­˜ {saved_count} å‰‡æ–°èåˆ°è³‡æ–™åº«")
                else:
                    logger.info("âš ï¸ ç¶“éæ—¥æœŸéæ¿¾å¾Œæ²’æœ‰æ–°èéœ€è¦å„²å­˜")
            
            # æ›´æ–°çˆ¬å–æ™‚é–“
            self.last_crawl_time = datetime.now(timezone.utc)
            
            # æ•´åˆè¿”å›çµæœ
            return {
                'status': 'success',
                'message': f'çˆ¬å–å®Œæˆï¼Œå…±è™•ç† {len(all_news)} å‰‡æ–°èï¼Œéæ¿¾å¾Œä¿å­˜ {len(filtered_news) if "filtered_news" in locals() else 0} å‰‡',
                'start_time': self.last_crawl_time.isoformat(),
                'duration': result.get('elapsed_time', 0),
                'results': result.get('results', []),
                'stats': self.stats,
                'filter_status': self.date_filter.get_status(),
                'total': len(all_news),
                'new': len(filtered_news) if "filtered_news" in locals() else 0
            }
        
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–ä»»å‹™åŸ·è¡Œå¤±æ•—: {e}")
            return {
                'status': 'error',
                'message': f'çˆ¬å–å¤±æ•—: {str(e)}',
                'total': 0,
                'new': 0
            }
        finally:
            self.is_running = False


# å‰µå»ºå…¨å±€çˆ¬èŸ²ç®¡ç†å™¨å¯¦ä¾‹
_crawler_manager = None

def get_crawler_manager() -> CrawlerManager:
    """
    ç²å–çˆ¬èŸ²ç®¡ç†å™¨å¯¦ä¾‹
    
    Returns:
        çˆ¬èŸ²ç®¡ç†å™¨å¯¦ä¾‹
    """
    global _crawler_manager
    if _crawler_manager is None:
        _crawler_manager = CrawlerManager()
    return _crawler_manager


def test_crawler_manager():
    """æ¸¬è©¦çˆ¬èŸ²ç®¡ç†å™¨"""
    logging.basicConfig(level=logging.INFO)
    manager = get_crawler_manager()
    
    print("ğŸ§ª æ¸¬è©¦çˆ¬èŸ²ç®¡ç†å™¨...")
    
    # æ¸¬è©¦çˆ¬å–
    result = manager.crawl_all_sources(use_mock=True)
    print(f"âœ… çˆ¬å–çµæœ: {result['message']}")
    print(f"ğŸ“Š çµ±è¨ˆ: {result['stats']}")
    
    # æ¸¬è©¦ç‹€æ…‹æŸ¥è©¢
    status = manager.get_crawler_status()
    print(f"ğŸ“‹ çˆ¬èŸ²ç‹€æ…‹: {status}")

if __name__ == "__main__":
    test_crawler_manager()
