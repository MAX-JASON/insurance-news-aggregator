"""
RSSæ–°èçˆ¬èŸ²
RSS News Crawler

é€šéRSS feedç²å–ä¿éšªç›¸é—œæ–°è
"""

import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import logging
import re

logger = logging.getLogger('crawler.rss')

class RSSNewsCrawler:
    """RSSæ–°èçˆ¬èŸ²"""
    
    def __init__(self):
        self.rss_feeds = [
            {
                'name': 'ç¶“æ¿Ÿæ—¥å ± - ä¿éšª',
                'url': 'https://money.udn.com/rssfeed/news/1001/5636?ch=money',
                'source_id': 2
            },
            {
                'name': 'å·¥å•†æ™‚å ± - ä¿éšª',
                'url': 'https://ctee.com.tw/feed',
                'source_id': 1  
            }
        ]
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
    
    def crawl_all_feeds(self) -> List[Dict[str, Any]]:
        """çˆ¬å–æ‰€æœ‰RSS feeds"""
        all_news = []
        
        for feed_info in self.rss_feeds:
            try:
                logger.info(f"ğŸ” æ­£åœ¨çˆ¬å– {feed_info['name']} RSS...")
                news_list = self.crawl_rss_feed(feed_info)
                all_news.extend(news_list)
                logger.info(f"âœ… {feed_info['name']} çˆ¬å–åˆ° {len(news_list)} å‰‡æ–°è")
            except Exception as e:
                logger.error(f"âŒ çˆ¬å– {feed_info['name']} å¤±æ•—: {e}")
        
        return all_news
    
    def crawl_rss_feed(self, feed_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """çˆ¬å–å–®å€‹RSS feed"""
        try:
            # è§£æRSS
            feed = feedparser.parse(feed_info['url'])
            
            if feed.bozo:
                logger.warning(f"âš ï¸ RSSè§£ææœ‰è­¦å‘Š: {feed.bozo_exception}")
            
            news_list = []
            
            for entry in feed.entries[:20]:  # é™åˆ¶æœ€å¤š20å‰‡æ–°è
                try:
                    # æª¢æŸ¥æ˜¯å¦èˆ‡ä¿éšªç›¸é—œ
                    title = entry.get('title', '')
                    if not self._is_insurance_related(title):
                        continue
                    
                    # æå–æ–°èæ•¸æ“š
                    news_item = {
                        'title': title,
                        'url': entry.get('link', ''),
                        'summary': self._clean_summary(entry.get('summary', '')),
                        'published_date': self._parse_date(entry),
                        'source': feed_info['name'],
                        'source_id': feed_info['source_id'],
                        'author': entry.get('author', ''),
                        'tags': self._extract_tags(entry)
                    }
                    
                    # ç²å–å®Œæ•´å…§å®¹
                    if news_item['url']:
                        content = self.get_article_content(news_item['url'])
                        news_item['content'] = content
                    
                    news_list.append(news_item)
                    
                except Exception as e:
                    logger.debug(f"âš ï¸ è™•ç†RSSæ¢ç›®å¤±æ•—: {e}")
                    continue
            
            return news_list
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–RSS feedå¤±æ•—: {e}")
            return []
    
    def _is_insurance_related(self, title: str) -> bool:
        """æª¢æŸ¥æ¨™é¡Œæ˜¯å¦èˆ‡ä¿éšªç›¸é—œ"""
        insurance_keywords = [
            'ä¿éšª', 'ä¿è²»', 'ä¿å–®', 'ç†è³ ', 'æŠ•ä¿', 'æ‰¿ä¿', 'ä¿éšœ',
            'å£½éšª', 'ç”¢éšª', 'å¥åº·éšª', 'æ„å¤–éšª', 'é†«ç™‚éšª', 'ç™Œç—‡éšª',
            'é‡‘ç®¡æœƒ', 'ä¿éšªå…¬å¸', 'ä¿éšªæ¥­', 'ä¿éšªæ³•', 'ä¿éšªé‡‘',
            'å¹´é‡‘', 'é€€ä¼‘é‡‘', 'é€€ä¼‘è¦åŠƒ', 'ä¿éšªç§‘æŠ€', 'Insurtech'
        ]
        
        return any(keyword in title for keyword in insurance_keywords)
    
    def _clean_summary(self, summary: str) -> str:
        """æ¸…ç†æ‘˜è¦æ–‡æœ¬"""
        if not summary:
            return ""
        
        # ç§»é™¤HTMLæ¨™ç±¤
        soup = BeautifulSoup(summary, 'html.parser')
        text = soup.get_text(strip=True)
        
        # é™åˆ¶é•·åº¦
        return text[:300] if text else ""
    
    def _parse_date(self, entry) -> datetime:
        """è§£æç™¼å¸ƒæ—¥æœŸ"""
        try:
            # å˜—è©¦å¤šç¨®æ—¥æœŸæ ¼å¼
            date_fields = ['published_parsed', 'updated_parsed']
            
            for field in date_fields:
                if hasattr(entry, field) and getattr(entry, field):
                    parsed_time = getattr(entry, field)
                    return datetime(*parsed_time[:6], tzinfo=timezone.utc)
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨ç•¶å‰æ™‚é–“
            return datetime.now(timezone.utc)
            
        except Exception:
            return datetime.now(timezone.utc)
    
    def _extract_tags(self, entry) -> str:
        """æå–æ¨™ç±¤"""
        tags = []
        
        try:
            if hasattr(entry, 'tags'):
                tags.extend([tag.term for tag in entry.tags])
            
            # å¾åˆ†é¡ä¸­æå–
            if hasattr(entry, 'category'):
                tags.append(entry.category)
            
            return ','.join(tags[:5])  # é™åˆ¶æœ€å¤š5å€‹æ¨™ç±¤
            
        except Exception:
            return ""
    
    def get_article_content(self, url: str) -> Optional[str]:
        """ç²å–æ–‡ç« å®Œæ•´å…§å®¹"""
        try:
            if not url:
                return None
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
            
            # å°‹æ‰¾æ–‡ç« å…§å®¹çš„å¤šç¨®é¸æ“‡å™¨
            content_selectors = [
                'article',
                '[role="main"]',
                '.article-content',
                '.post-content',
                '.entry-content',
                '.content-body',
                '.story-body',
                '#article-content',
                '.article-body'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # æå–æ®µè½æ–‡æœ¬
                    paragraphs = content_elem.find_all('p')
                    if paragraphs:
                        content = '\n'.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 10])
                        break
            
            # å¦‚æœé‚„æ˜¯æ²’æ‰¾åˆ°ï¼Œå˜—è©¦æå–æ‰€æœ‰æ®µè½
            if not content:
                paragraphs = soup.find_all('p')
                content = '\n'.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            
            return content[:3000] if content else None  # é™åˆ¶å…§å®¹é•·åº¦
            
        except Exception as e:
            logger.debug(f"âŒ ç²å–æ–‡ç« å…§å®¹å¤±æ•— {url}: {e}")
            return None

def test_rss_crawler():
    """æ¸¬è©¦RSSçˆ¬èŸ²"""
    crawler = RSSNewsCrawler()
    
    print("ğŸ§ª æ¸¬è©¦RSSæ–°èçˆ¬èŸ²...")
    news_list = crawler.crawl_all_feeds()
    
    if news_list:
        print(f"âœ… æˆåŠŸçˆ¬å– {len(news_list)} å‰‡ä¿éšªç›¸é—œæ–°è")
        for i, news in enumerate(news_list[:3], 1):
            print(f"\nğŸ“° æ–°è {i}:")
            print(f"æ¨™é¡Œ: {news['title']}")
            print(f"ä¾†æº: {news['source']}")
            print(f"ç¶²å€: {news['url']}")
            print(f"æ‘˜è¦: {news['summary'][:100]}...")
            if news.get('content'):
                print(f"å…§å®¹: {news['content'][:150]}...")
    else:
        print("âŒ æ²’æœ‰çˆ¬å–åˆ°ä»»ä½•æ–°è")

if __name__ == "__main__":
    test_rss_crawler()
