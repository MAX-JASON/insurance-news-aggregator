"""
çˆ¬èŸ²è¼”åŠ©æ¨¡çµ„
Crawler Helper Module

æä¾›çˆ¬èŸ²çš„è¼”åŠ©åŠŸèƒ½ï¼Œç‰¹åˆ¥æ˜¯ç”¨æ–¼ç®¡ç†å™¨ä¸­çš„æ•´åˆåŠŸèƒ½
"""

import logging
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
import os
import json

logger = logging.getLogger('crawler.helper')

class CrawlerHelper:
    """çˆ¬èŸ²è¼”åŠ©é¡"""
    
    @staticmethod
    def run_all_crawlers(crawler_manager, use_real=True) -> Dict[str, Any]:
        """
        é‹è¡Œæ‰€æœ‰çˆ¬èŸ²ï¼Œå„ªå…ˆä½¿ç”¨çœŸå¯¦çˆ¬èŸ²
        
        Args:
            crawler_manager: çˆ¬èŸ²ç®¡ç†å™¨å¯¦ä¾‹
            use_real: æ˜¯å¦ä½¿ç”¨çœŸå¯¦çˆ¬èŸ²ï¼ˆè€Œéæ¨¡æ“¬æ•¸æ“šï¼‰
        
        Returns:
            Dict åŒ…å«çˆ¬å–çµæœ
        """
        all_news = []
        crawl_results = []
        
        # è¨˜éŒ„é–‹å§‹æ™‚é–“
        start_time = time.time()
        
        try:
            # å„ªå…ˆä½¿ç”¨çœŸå¯¦çˆ¬èŸ²
            if use_real:
                # 1. ä½¿ç”¨RSSçˆ¬èŸ²
                if 'rss' in crawler_manager.crawlers:
                    try:
                        logger.info("ğŸ” ä½¿ç”¨RSSçˆ¬èŸ²æŠ“å–æ–°è...")
                        rss_news = crawler_manager.crawlers['rss'].crawl_all_feeds()
                        all_news.extend(rss_news)
                        crawl_results.append({
                            'source': 'RSSæ–°èæº',
                            'success': True,
                            'news_count': len(rss_news),
                            'message': f'æˆåŠŸçˆ¬å–RSSæ–°è {len(rss_news)} å‰‡'
                        })
                        logger.info(f"âœ… RSSçˆ¬èŸ²å®Œæˆï¼Œç²å– {len(rss_news)} å‰‡æ–°è")
                    except Exception as e:
                        logger.error(f"âŒ RSSçˆ¬èŸ²å¤±æ•—: {e}")
                        crawl_results.append({
                            'source': 'RSSæ–°èæº', 
                            'success': False,
                            'news_count': 0,
                            'message': f'çˆ¬å–å¤±æ•—: {str(e)}'
                        })
                
                # 2. ä½¿ç”¨çœŸå¯¦ä¿éšªæ–°èçˆ¬èŸ²
                if 'real' in crawler_manager.crawlers:
                    try:
                        logger.info("ğŸ” ä½¿ç”¨çœŸå¯¦æ–°èçˆ¬èŸ²æŠ“å–æ–°è...")
                        # æª¢æŸ¥æ˜¯å¦æœ‰ crawl_google_news æ–¹æ³•
                        if hasattr(crawler_manager.crawlers['real'], 'crawl_google_news'):
                            real_news = crawler_manager.crawlers['real'].crawl_google_news()
                            method_name = 'crawl_google_news'
                        # æˆ–è€…æª¢æŸ¥æ˜¯å¦æœ‰ crawl_all_sources æ–¹æ³•
                        elif hasattr(crawler_manager.crawlers['real'], 'crawl_all_sources'):
                            real_news = crawler_manager.crawlers['real'].crawl_all_sources()
                            method_name = 'crawl_all_sources'
                        else:
                            logger.error("âŒ çœŸå¯¦çˆ¬èŸ²ç¼ºå°‘é©ç•¶çš„çˆ¬å–æ–¹æ³•")
                            real_news = []
                            method_name = "unknown_method"
                            
                        all_news.extend(real_news)
                        crawl_results.append({
                            'source': 'çœŸå¯¦æ–°èçˆ¬èŸ²',
                            'success': True,
                            'news_count': len(real_news),
                            'method': method_name,
                            'message': f'æˆåŠŸçˆ¬å–çœŸå¯¦æ–°è {len(real_news)} å‰‡'
                        })
                        logger.info(f"âœ… çœŸå¯¦çˆ¬èŸ²å®Œæˆï¼Œç²å– {len(real_news)} å‰‡æ–°è")
                    except Exception as e:
                        logger.error(f"âŒ çœŸå¯¦çˆ¬èŸ²å¤±æ•—: {e}")
                        crawl_results.append({
                            'source': 'çœŸå¯¦çˆ¬èŸ²', 
                            'success': False,
                            'news_count': 0,
                            'message': f'çˆ¬å–å¤±æ•—: {str(e)}'
                        })
                
                # 3. ä½¿ç”¨å·¥å•†æ™‚å ±å°ˆç”¨çˆ¬èŸ²
                if 'ctee' in crawler_manager.crawlers:
                    try:
                        logger.info("ğŸ” ä½¿ç”¨å·¥å•†æ™‚å ±å°ˆç”¨çˆ¬èŸ²æŠ“å–æ–°è...")
                        ctee_news = crawler_manager.crawlers['ctee'].crawl()
                        all_news.extend(ctee_news)
                        crawl_results.append({
                            'source': 'å·¥å•†æ™‚å ±ä¿éšªç‰ˆ',
                            'success': True,
                            'news_count': len(ctee_news),
                            'message': f'æˆåŠŸçˆ¬å–å·¥å•†æ™‚å ±æ–°è {len(ctee_news)} å‰‡'
                        })
                        logger.info(f"âœ… å·¥å•†æ™‚å ±çˆ¬èŸ²å®Œæˆï¼Œç²å– {len(ctee_news)} å‰‡æ–°è")
                    except Exception as e:
                        logger.error(f"âŒ å·¥å•†æ™‚å ±çˆ¬èŸ²å¤±æ•—: {e}")
                        crawl_results.append({
                            'source': 'å·¥å•†æ™‚å ±ä¿éšªç‰ˆ', 
                            'success': False,
                            'news_count': 0,
                            'message': f'çˆ¬å–å¤±æ•—: {str(e)}'
                        })
            
            # å¦‚æœæ²’æœ‰ç²å¾—ä»»ä½•æ–°èï¼Œå˜—è©¦ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š
            if not all_news and 'mock' in crawler_manager.crawlers:
                logger.warning("âš ï¸ æœªç²å–åˆ°çœŸå¯¦æ–°èï¼Œä½¿ç”¨æ¨¡æ“¬æ•¸æ“šä½œç‚ºå‚™ç”¨")
                mock_news = crawler_manager.crawlers['mock'].generate(10)
                all_news.extend(mock_news)
                crawl_results.append({
                    'source': 'æ¨¡æ“¬æ•¸æ“šç”Ÿæˆå™¨',
                    'success': True,
                    'news_count': len(mock_news),
                    'message': 'ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šä½œç‚ºå‚™ç”¨'
                })
            
            # è¨ˆç®—è€—æ™‚
            elapsed_time = time.time() - start_time
            
            return {
                'total_news': len(all_news),
                'news': all_news,
                'results': crawl_results,
                'elapsed_time': elapsed_time
            }
            
        except Exception as e:
            logger.error(f"é‹è¡Œçˆ¬èŸ²æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                'total_news': 0,
                'news': [],
                'results': [{'success': False, 'message': str(e)}],
                'elapsed_time': time.time() - start_time
            }
