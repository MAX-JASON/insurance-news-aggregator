"""
Yahooæ–°èä¿éšªç‰ˆçˆ¬èŸ²
Yahoo News Insurance Crawler

å°ˆé–€çˆ¬å–Yahooæ–°èçš„ä¿éšªç›¸é—œå…§å®¹
"""

import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import logging
from urllib.parse import urljoin, urlparse
import time
import random

logger = logging.getLogger('crawler.yahoo')

class YahooInsuranceCrawler:
    """Yahooæ–°èä¿éšªç‰ˆçˆ¬èŸ²"""
    
    def __init__(self):
        self.base_url = "https://tw.news.yahoo.com"
        self.search_url = "https://tw.news.yahoo.com/tag/ä¿éšª"
        self.session = requests.Session()
        
        # è¨­ç½®User-Agent
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
    
    def crawl_news_list(self, max_pages: int = 3) -> List[Dict[str, Any]]:
        """
        çˆ¬å–æ–°èåˆ—è¡¨
        
        Args:
            max_pages: æœ€å¤§é æ•¸
            
        Returns:
            æ–°èåˆ—è¡¨
        """
        news_list = []
        
        try:
            logger.info(f"ğŸ” é–‹å§‹çˆ¬å–Yahooä¿éšªæ–°èï¼Œæœ€å¤§é æ•¸: {max_pages}")
            
            for page in range(1, max_pages + 1):
                logger.info(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é ...")
                
                # æ§‹å»ºURL (Yahooæ–°èçš„åˆ†é æ©Ÿåˆ¶)
                if page == 1:
                    url = self.search_url
                else:
                    url = f"{self.search_url}?offset={(page-1)*10}"
                
                # ç™¼é€è«‹æ±‚
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                # è§£æHTML
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # å°‹æ‰¾æ–°èé …ç›®
                news_items = self._parse_news_items(soup)
                
                if not news_items:
                    logger.warning(f"âš ï¸ ç¬¬ {page} é æ²’æœ‰æ‰¾åˆ°æ–°èé …ç›®")
                    break
                
                news_list.extend(news_items)
                logger.info(f"âœ… ç¬¬ {page} é æ‰¾åˆ° {len(news_items)} å‰‡æ–°è")
                
                # éš¨æ©Ÿå»¶é²
                time.sleep(random.uniform(2, 4))
            
            logger.info(f"ğŸ‰ ç¸½å…±çˆ¬å–åˆ° {len(news_list)} å‰‡æ–°è")
            return news_list
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–Yahooæ–°èå¤±æ•—: {e}")
            return []
    
    def _parse_news_items(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """è§£ææ–°èé …ç›®"""
        news_items = []
        
        try:
            # Yahooæ–°èçš„çµæ§‹å¯èƒ½æœƒè®ŠåŒ–ï¼Œé€™è£¡æä¾›ä¸€å€‹åŸºæœ¬çš„è§£æé‚è¼¯
            # å°‹æ‰¾å¯èƒ½çš„æ–°èå®¹å™¨
            containers = soup.find_all(['div', 'article'], class_=re.compile(r'(story|news|item|card)', re.I))
            
            for container in containers[:20]:  # é™åˆ¶æ¯é æœ€å¤š20å‰‡
                try:
                    news_item = self._extract_news_data(container)
                    if news_item and self._is_insurance_related(news_item['title']):
                        news_items.append(news_item)
                except Exception as e:
                    logger.debug(f"âš ï¸ è§£æå–®å‰‡æ–°èå¤±æ•—: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"âŒ è§£ææ–°èé …ç›®å¤±æ•—: {e}")
        
        return news_items
    
    def _extract_news_data(self, container) -> Optional[Dict[str, Any]]:
        """å¾å®¹å™¨ä¸­æå–æ–°èæ•¸æ“š"""
        try:
            # å°‹æ‰¾æ¨™é¡Œ
            title_elem = container.find(['h1', 'h2', 'h3', 'h4'], string=re.compile(r'.+'))
            if not title_elem:
                title_elem = container.find('a', string=re.compile(r'.+'))
            
            if not title_elem:
                return None
            
            title = title_elem.get_text(strip=True)
            if len(title) < 10:  # æ¨™é¡Œå¤ªçŸ­å¯èƒ½ä¸æ˜¯çœŸæ­£çš„æ–°è
                return None
            
            # å°‹æ‰¾é€£çµ
            link_elem = title_elem.find_parent('a') or title_elem if title_elem.name == 'a' else container.find('a')
            url = ""
            if link_elem and link_elem.get('href'):
                url = urljoin(self.base_url, link_elem['href'])
            
            # å°‹æ‰¾æ‘˜è¦
            summary = ""
            summary_elem = container.find(['p', 'div'], string=re.compile(r'.+'))
            if summary_elem:
                summary = summary_elem.get_text(strip=True)[:200]
            
            # å°‹æ‰¾æ™‚é–“
            published_date = self._extract_date(container)
            
            return {
                'title': title,
                'url': url,
                'summary': summary,
                'published_date': published_date,
                'source': 'Yahooæ–°è',
                'raw_html': str(container)
            }
            
        except Exception as e:
            logger.debug(f"æå–æ–°èæ•¸æ“šå¤±æ•—: {e}")
            return None
    
    def _extract_date(self, container) -> Optional[datetime]:
        """æå–ç™¼å¸ƒæ™‚é–“"""
        try:
            # å°‹æ‰¾æ™‚é–“ç›¸é—œçš„å…ƒç´ 
            date_patterns = [
                r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})',
                r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',
                r'(\d+)\s*å°æ™‚å‰',
                r'(\d+)\s*åˆ†é˜å‰',
                r'ä»Šå¤©',
                r'æ˜¨å¤©'
            ]
            
            # åœ¨å®¹å™¨æ–‡æœ¬ä¸­æœç´¢æ—¥æœŸ
            text = container.get_text()
            for pattern in date_patterns:
                match = re.search(pattern, text)
                if match:
                    # é€™è£¡å¯ä»¥æ·»åŠ æ›´è¤‡é›œçš„æ—¥æœŸè§£æé‚è¼¯
                    return datetime.now(timezone.utc)
            
            return datetime.now(timezone.utc)
            
        except Exception:
            return datetime.now(timezone.utc)
    
    def _is_insurance_related(self, title: str) -> bool:
        """æª¢æŸ¥æ¨™é¡Œæ˜¯å¦èˆ‡ä¿éšªç›¸é—œ"""
        insurance_keywords = [
            'ä¿éšª', 'ä¿è²»', 'ä¿å–®', 'ç†è³ ', 'æŠ•ä¿', 'æ‰¿ä¿',
            'å£½éšª', 'ç”¢éšª', 'å¥åº·éšª', 'æ„å¤–éšª', 'é†«ç™‚éšª',
            'é‡‘ç®¡æœƒ', 'ä¿éšªå…¬å¸', 'ä¿éšªæ¥­', 'ä¿éšœ',
            'å¹´é‡‘', 'é€€ä¼‘é‡‘', 'ä¿éšªé‡‘'
        ]
        
        return any(keyword in title for keyword in insurance_keywords)
    
    def get_article_content(self, url: str) -> Optional[str]:
        """ç²å–æ–‡ç« å®Œæ•´å…§å®¹"""
        try:
            if not url:
                return None
                
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # å°‹æ‰¾æ–‡ç« å…§å®¹
            content_selectors = [
                'div[data-type="story-body"]',
                '.story-body',
                '.article-body',
                '.content-body',
                'article',
                '.post-content'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text(strip=True)
                    break
            
            # å¦‚æœæ²’æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œå˜—è©¦æå–ä¸»è¦æ®µè½
            if not content:
                paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            
            return content[:2000] if content else None  # é™åˆ¶å…§å®¹é•·åº¦
            
        except Exception as e:
            logger.error(f"âŒ ç²å–æ–‡ç« å…§å®¹å¤±æ•— {url}: {e}")
            return None

def test_yahoo_crawler():
    """æ¸¬è©¦Yahooçˆ¬èŸ²"""
    crawler = YahooInsuranceCrawler()
    
    print("ğŸ§ª æ¸¬è©¦Yahooä¿éšªæ–°èçˆ¬èŸ²...")
    news_list = crawler.crawl_news_list(max_pages=1)
    
    if news_list:
        print(f"âœ… æˆåŠŸçˆ¬å– {len(news_list)} å‰‡æ–°è")
        for i, news in enumerate(news_list[:3], 1):
            print(f"\nğŸ“° æ–°è {i}:")
            print(f"æ¨™é¡Œ: {news['title']}")
            print(f"ç¶²å€: {news['url']}")
            print(f"æ‘˜è¦: {news['summary'][:100]}...")
            
            # æ¸¬è©¦ç²å–å®Œæ•´å…§å®¹
            if news['url']:
                content = crawler.get_article_content(news['url'])
                if content:
                    print(f"å…§å®¹: {content[:150]}...")
    else:
        print("âŒ æ²’æœ‰çˆ¬å–åˆ°ä»»ä½•æ–°è")

if __name__ == "__main__":
    test_yahoo_crawler()
