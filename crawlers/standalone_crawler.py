"""
ç¨ç«‹çš„çˆ¬èŸ²åŸ·è¡Œè…³æœ¬
Standalone Crawler Execution Script

é¿å…Flaskæ‡‰ç”¨é‡è¤‡è¨»å†Šå•é¡Œ
"""

import sys
import os
import logging
from datetime import datetime, timezone, timedelta

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def save_news_to_database_directly(news_list):
    """ç›´æ¥ä¿å­˜æ–°èåˆ°è³‡æ–™åº«ï¼Œé¿å…Flaskæ‡‰ç”¨é‡è¤‡è¨»å†Šå•é¡Œ"""
    import sqlite3
    from datetime import datetime, timezone
    
    try:
        # é€£æ¥SQLiteè³‡æ–™åº«
        db_path = "instance/insurance_news.db"
        if not os.path.exists(db_path):
            db_path = "instance/dev_insurance_news.db"
        
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        
        for news_data in news_list:
            try:
                # æª¢æŸ¥é‡è¤‡ - ä½¿ç”¨æ¨™é¡Œæª¢æŸ¥
                title = news_data.get('title', '')
                if not title:
                    continue
                
                cursor.execute("SELECT id FROM news WHERE title = ?", (title,))
                if cursor.fetchone():
                    print(f"  è·³éé‡è¤‡æ–°è: {title[:50]}...")
                    continue
                
                # è™•ç†ä¾†æº - ç°¡åŒ–è™•ç†ï¼Œä½¿ç”¨å›ºå®šä¾†æºID
                source_id = 4  # ä½¿ç”¨Googleæ–°èçš„ä¾†æºID
                
                # è™•ç†åˆ†é¡ - ä½¿ç”¨é è¨­åˆ†é¡
                category_id = 1  # ä½¿ç”¨é è¨­åˆ†é¡
                
                # æº–å‚™æ’å…¥æ•¸æ“š
                now = datetime.now(timezone.utc).isoformat()
                published_date = news_data.get('published_date')
                if isinstance(published_date, datetime):
                    published_date = published_date.isoformat()
                elif published_date is None:
                    published_date = now
                
                # æ’å…¥æ–°è
                cursor.execute("""
                    INSERT INTO news (
                        title, content, summary, url, source_id, category_id,
                        published_date, crawled_date, importance_score, 
                        sentiment_score, status, created_at, updated_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    title,
                    news_data.get('content', ''),
                    news_data.get('summary', '')[:500],  # é™åˆ¶æ‘˜è¦é•·åº¦
                    news_data.get('url', ''),
                    source_id,
                    category_id,
                    published_date,
                    now,
                    news_data.get('importance_score', 0.5),
                    news_data.get('sentiment_score', 0.0),
                    'active',
                    now,
                    now
                ))
                
                saved_count += 1
                print(f"  âœ… ä¿å­˜æ–°è: {title[:50]}...")
                
            except Exception as e:
                print(f"  âŒ ä¿å­˜å–®å‰‡æ–°èå¤±æ•—: {e}")
                continue
        
        conn.commit()
        conn.close()
        
        return saved_count
        
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«æ“ä½œå¤±æ•—: {e}")
        return 0

def execute_crawler():
    """åŸ·è¡Œçˆ¬èŸ²"""
    try:
        # è¨­ç½®æ—¥èªŒ
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        print("ğŸš€ é–‹å§‹åŸ·è¡Œç¨ç«‹çˆ¬èŸ²...")
        
        # ç›´æ¥ä½¿ç”¨çœŸå¯¦çˆ¬èŸ²
        from crawler.real_crawler_fixed import RealInsuranceNewsCrawler
        
        crawler = RealInsuranceNewsCrawler()
        
        print("ğŸ•·ï¸ æ­£åœ¨æŠ“å–æ–°è...")
        all_news = crawler.crawl_all_sources()
        
        if not all_news:
            print("âŒ æ²’æœ‰æŠ“å–åˆ°ä»»ä½•æ–°è")
            return False
        
        print(f"ğŸ“° ç¸½å…±æŠ“å–åˆ° {len(all_news)} å‰‡æ–°è")
        
        # æ‡‰ç”¨7å¤©éæ¿¾
        print("ğŸ” æ‡‰ç”¨7å¤©æ—¥æœŸéæ¿¾...")
        from crawler.date_filter import create_date_filter
        
        date_filter = create_date_filter(max_age_days=7, enable_filter=True)
        filtered_news = date_filter.filter_news_list(all_news)
        
        print(f"âœ… éæ¿¾å¾Œä¿ç•™ {len(filtered_news)} å‰‡æ–°è")
        
        # ç›´æ¥ä¿å­˜åˆ°è³‡æ–™åº«
        if filtered_news:
            print("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°è³‡æ–™åº«...")
            saved_count = save_news_to_database_directly(filtered_news)
            print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} å‰‡æ–°èåˆ°è³‡æ–™åº«")
            
            if saved_count > 0:
                print("ğŸ‰ æ–°èå·²æ›´æ–°ï¼ç¾åœ¨é‡æ–°æ•´ç†ç¶²é æ‡‰è©²èƒ½çœ‹åˆ°æœ€æ–°æ–°è")
                return True
            else:
                print("âš ï¸ æ²’æœ‰æ–°çš„æ–°èè¢«ä¿å­˜ï¼ˆå¯èƒ½éƒ½æ˜¯é‡è¤‡çš„ï¼‰")
                return False
        else:
            print("âš ï¸ ç¶“ééæ¿¾å¾Œæ²’æœ‰æ–°èéœ€è¦ä¿å­˜")
            return False
            
    except Exception as e:
        print(f"âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸ¯ ç¨ç«‹çˆ¬èŸ²åŸ·è¡Œå™¨")
    print("=" * 50)
    
    if execute_crawler():
        print("\n" + "=" * 50)
        print("ğŸ‰ çˆ¬èŸ²åŸ·è¡ŒæˆåŠŸï¼")
        print("ğŸ’¡ æç¤º: è«‹é‡æ–°æ•´ç†ç¶²é æŸ¥çœ‹æœ€æ–°æ–°è")
        
        # é¡¯ç¤ºæœ€æ–°ä¿å­˜çš„æ–°è
        print("\nğŸ“° æª¢æŸ¥æœ€æ–°ä¿å­˜çš„æ–°è...")
        import sqlite3
        try:
            conn = sqlite3.connect("instance/insurance_news.db")
            cursor = conn.cursor()
            cursor.execute("""
                SELECT title, crawled_date 
                FROM news 
                WHERE crawled_date >= datetime('now', '-1 hour')
                ORDER BY crawled_date DESC 
                LIMIT 5
            """)
            recent_news = cursor.fetchall()
            
            if recent_news:
                print("æœ€æ–°ä¿å­˜çš„æ–°è:")
                for i, (title, crawled_date) in enumerate(recent_news, 1):
                    print(f"  {i}. {title[:60]}...")
                    print(f"     ä¿å­˜æ™‚é–“: {crawled_date}")
            else:
                print("âš ï¸ æœ€è¿‘1å°æ™‚å…§æ²’æœ‰æ–°ä¿å­˜çš„æ–°è")
                
            conn.close()
        except Exception as e:
            print(f"âŒ æª¢æŸ¥å¤±æ•—: {e}")
    else:
        print("\nâŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—")

if __name__ == "__main__":
    main()
