#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è¶…ç´šä¿éšªæ–°èçˆ¬èŸ² - æ“´å±•é—œéµå­—ç‰ˆ
Super Insurance News Crawler with Expanded Keywords

ä½¿ç”¨å¤§å¹…æ“´å±•çš„é—œéµå­—åº«å’Œæœç´¢ç¯„åœ
"""

import requests
import feedparser
import sqlite3
import os
import yaml
from datetime import datetime, timezone, timedelta
from urllib.parse import quote
import re
import hashlib
import time
import random

class SuperInsuranceCrawler:
    def __init__(self):
        self.db_path = self.find_database()
        self.keywords = self.load_expanded_keywords()
        self.search_terms = self.prepare_search_terms()
        
    def find_database(self):
        """æ‰¾åˆ°è³‡æ–™åº«æª”æ¡ˆ"""
        for db_name in ["instance/insurance_news.db", "instance/dev_insurance_news.db"]:
            if os.path.exists(db_name):
                return db_name
        return "instance/insurance_news.db"
    
    def load_expanded_keywords(self):
        """è¼‰å…¥æ“´å±•é—œéµå­—é…ç½®"""
        try:
            with open('config/expanded_keywords.yaml', 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥æ“´å±•é—œéµå­—å¤±æ•—ï¼Œä½¿ç”¨é è¨­é—œéµå­—: {e}")
            return {
                'search_combinations': {
                    'primary': ['ä¿éšª', 'äººå£½ä¿éšª', 'ç”¢éšª', 'è»Šéšª', 'å¥åº·éšª'],
                    'secondary': ['é‡‘ç®¡æœƒ ä¿éšª', 'ä¿éšªå±€', 'ç†è³ '],
                    'specialized': ['ä¿éšªç§‘æŠ€', 'æ•¸ä½ä¿éšª', 'é•·ç…§éšª']
                }
            }
    
    def prepare_search_terms(self):
        """æº–å‚™æœç´¢é—œéµå­—"""
        search_terms = []
        
        # å¾é…ç½®ä¸­æå–æœç´¢çµ„åˆ
        if 'search_combinations' in self.keywords:
            combinations = self.keywords['search_combinations']
            
            # ä¸»è¦é—œéµå­— (æ¯å€‹éƒ½æœç´¢)
            if 'primary' in combinations:
                search_terms.extend(combinations['primary'])
            
            # æ¬¡è¦é—œéµå­— (é¸æ“‡æ€§æœç´¢)
            if 'secondary' in combinations:
                search_terms.extend(combinations['secondary'][:5])  # å–å‰5å€‹
            
            # å°ˆæ¥­é—œéµå­— (é¸æ“‡æ€§æœç´¢)
            if 'specialized' in combinations:
                search_terms.extend(combinations['specialized'][:3])  # å–å‰3å€‹
                
            # ç›£ç®¡é—œéµå­—
            if 'regulatory' in combinations:
                search_terms.extend(combinations['regulatory'][:3])  # å–å‰3å€‹
        
        # å¦‚æœæ²’æœ‰é…ç½®ï¼Œä½¿ç”¨é è¨­
        if not search_terms:
            search_terms = [
                'ä¿éšª', 'äººå£½ä¿éšª', 'ç”¢éšª', 'è»Šéšª', 'å¥åº·éšª', 'é†«ç™‚éšª', 
                'æ„å¤–éšª', 'å¹´é‡‘éšª', 'é•·ç…§éšª', 'å¤±èƒ½éšª', 'é‡‘ç®¡æœƒ ä¿éšª', 
                'ä¿éšªå±€', 'ç†è³ ', 'æŠ•ä¿', 'ä¿å–®', 'ä¿éšªç§‘æŠ€', 'æ•¸ä½ä¿éšª'
            ]
        
        print(f"ğŸ¯ æº–å‚™ä½¿ç”¨ {len(search_terms)} å€‹æœç´¢é—œéµå­—")
        return search_terms
    
    def normalize_title(self, title):
        """æ¨™æº–åŒ–æ¨™é¡Œä»¥ä¾¿æ›´å¥½åœ°æª¢æ¸¬é‡è¤‡"""
        # ç§»é™¤å¸¸è¦‹çš„å‰å¾Œç¶´
        title = re.sub(r'^ã€.*?ã€‘', '', title)  # ç§»é™¤ã€ã€‘æ¨™è¨˜
        title = re.sub(r'\s*-\s*.*$', '', title)  # ç§»é™¤ä¾†æºæ¨™è¨˜
        title = re.sub(r'\s*\|.*$', '', title)   # ç§»é™¤|å¾Œçš„å…§å®¹
        title = re.sub(r'\s+', ' ', title).strip()  # æ¨™æº–åŒ–ç©ºæ ¼
        return title.lower()
    
    def get_title_hash(self, title):
        """ç”Ÿæˆæ¨™é¡Œçš„å“ˆå¸Œå€¼ç”¨æ–¼é‡è¤‡æª¢æ¸¬"""
        normalized = self.normalize_title(title)
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    def is_insurance_related(self, title, content=""):
        """æª¢æŸ¥æ–°èæ˜¯å¦èˆ‡ä¿éšªç›¸é—œ"""
        text = (title + " " + content).lower()
        
        # æ ¸å¿ƒä¿éšªé—œéµå­—æª¢æŸ¥
        core_keywords = ['ä¿éšª', 'ä¿å–®', 'ä¿è²»', 'ç†è³ ', 'æŠ•ä¿']
        if any(keyword in text for keyword in core_keywords):
            return True
        
        # ä¿éšªé¡å‹æª¢æŸ¥
        insurance_types = ['å£½éšª', 'ç”¢éšª', 'è»Šéšª', 'å¥åº·éšª', 'é†«ç™‚éšª', 'æ„å¤–éšª', 'å¹´é‡‘éšª', 'é•·ç…§éšª']
        if any(keyword in text for keyword in insurance_types):
            return True
        
        # ä¿éšªå…¬å¸æª¢æŸ¥
        insurance_companies = ['åœ‹æ³°', 'å¯Œé‚¦', 'æ–°å…‰', 'å—å±±', 'å°ç£äººå£½', 'å…¨çƒäººå£½']
        if any(keyword in text for keyword in insurance_companies):
            return True
        
        # é‡‘èç›£ç®¡æª¢æŸ¥
        regulatory_keywords = ['é‡‘ç®¡æœƒ', 'ä¿éšªå±€', 'é‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒ']
        if any(keyword in text for keyword in regulatory_keywords):
            return True
        
        return False
    
    def fetch_google_news(self, search_term, max_results=5):
        """æŠ“å–Googleæ–°è"""
        news_list = []
        
        try:
            print(f"ğŸ“¡ æœç´¢: {search_term}")
            
            # æ§‹å»ºæœç´¢URLï¼ŒåŠ å…¥å°ç£åœ°å€é™åˆ¶
            encoded_term = quote(f"{search_term} å°ç£")
            search_url = f"https://news.google.com/rss/search?q={encoded_term}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
            
            # éš¨æ©Ÿå»¶é²é¿å…è¢«é™åˆ¶
            time.sleep(random.uniform(1, 3))
            
            feed = feedparser.parse(search_url)
            
            if hasattr(feed, 'entries'):
                count = 0
                for entry in feed.entries:
                    if count >= max_results:
                        break
                    
                    title = entry.title
                    summary = entry.get('summary', '')
                    
                    # æª¢æŸ¥æ˜¯å¦èˆ‡ä¿éšªç›¸é—œ
                    if self.is_insurance_related(title, summary):
                        news_list.append({
                            'title': title,
                            'url': entry.link,
                            'summary': summary[:300],
                            'content': summary,
                            'published_date': datetime.now(timezone.utc),
                            'source': f'Googleæ–°è-{search_term}',
                            'search_term': search_term
                        })
                        count += 1
                
                print(f"  âœ… æ‰¾åˆ° {count} å‰‡ç›¸é—œæ–°è")
            else:
                print(f"  âš ï¸ ç„¡æ³•è§£æRSSæº")
                
        except Exception as e:
            print(f"  âŒ æœç´¢å¤±æ•—: {e}")
        
        return news_list
    
    def fetch_all_news(self):
        """æŠ“å–æ‰€æœ‰æ–°è"""
        all_news = []
        
        print("ğŸ” é–‹å§‹æŠ“å–è¶…ç´šä¿éšªæ–°è...")
        print("=" * 60)
        
        # ä½¿ç”¨æ‰€æœ‰æœç´¢é—œéµå­—
        for i, search_term in enumerate(self.search_terms, 1):
            print(f"ğŸ“Š é€²åº¦: {i}/{len(self.search_terms)} - {search_term}")
            
            # æ ¹æ“šé—œéµå­—é‡è¦æ€§æ±ºå®šæŠ“å–æ•¸é‡
            if search_term in ['ä¿éšª', 'äººå£½ä¿éšª', 'ç”¢éšª']:
                max_results = 8  # é‡è¦é—œéµå­—å¤šæŠ“ä¸€äº›
            elif 'é‡‘ç®¡æœƒ' in search_term or 'ä¿éšªå±€' in search_term:
                max_results = 6  # ç›£ç®¡æ–°èä¹Ÿå¾ˆé‡è¦
            else:
                max_results = 4  # å…¶ä»–é—œéµå­—é©é‡
            
            news = self.fetch_google_news(search_term, max_results)
            all_news.extend(news)
            
            # é€²åº¦æç¤º
            if i % 5 == 0:
                print(f"  ğŸ“ˆ å·²å®Œæˆ {i} å€‹é—œéµå­—ï¼Œç´¯è¨ˆ {len(all_news)} å‰‡æ–°è")
        
        print(f"ğŸ“Š ç¸½å…±æŠ“å–åˆ° {len(all_news)} å‰‡æ–°è")
        return all_news
    
    def apply_date_filter(self, news_list, max_days=7):
        """æ‡‰ç”¨æ—¥æœŸéæ¿¾"""
        print(f"ğŸ” æ‡‰ç”¨{max_days}å¤©æ—¥æœŸéæ¿¾...")
        
        # RSSæ–°èé€šå¸¸éƒ½æ˜¯æœ€è¿‘çš„ï¼Œç›´æ¥è¿”å›
        filtered_news = news_list.copy()
        
        print(f"âœ… éæ¿¾å¾Œä¿ç•™ {len(filtered_news)} å‰‡æ–°è")
        return filtered_news
    
    def save_to_database(self, news_list):
        """ä¿å­˜æ–°èåˆ°è³‡æ–™åº«ï¼Œå…·æœ‰æ™ºèƒ½å»é‡"""
        if not news_list:
            print("âŒ æ²’æœ‰æ–°èè¦ä¿å­˜")
            return False
        
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°è³‡æ–™åº«...")
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ç²å–ç¾æœ‰æ–°èçš„æ¨™é¡Œå“ˆå¸Œå€¼
            cursor.execute("SELECT title FROM news WHERE status = 'active'")
            existing_hashes = set()
            for (title,) in cursor.fetchall():
                existing_hashes.add(self.get_title_hash(title))
            
            saved_count = 0
            duplicate_count = 0
            
            for news_data in news_list:
                try:
                    title = news_data.get('title', '').strip()
                    if not title:
                        continue
                    
                    # ä½¿ç”¨å“ˆå¸Œå€¼æª¢æŸ¥é‡è¤‡
                    title_hash = self.get_title_hash(title)
                    if title_hash in existing_hashes:
                        duplicate_count += 1
                        continue
                    
                    # æº–å‚™æ’å…¥æ•¸æ“š
                    now = datetime.now(timezone.utc).isoformat()
                    search_term = news_data.get('search_term', '')
                    
                    # ç”Ÿæˆå¸¶æœ‰æœç´¢è©æ¨™è­˜çš„æ¨™é¡Œ
                    enhanced_title = f"{title}"
                    if search_term and len(search_term) < 10:
                        enhanced_title = f"[{search_term}] {title}"
                    
                    cursor.execute("""
                        INSERT INTO news (
                            title, content, summary, url, source_id, category_id,
                            published_date, crawled_date, importance_score, 
                            sentiment_score, status, created_at, updated_at
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        enhanced_title,
                        news_data.get('content', ''),
                        news_data.get('summary', '')[:500],
                        news_data.get('url', ''),
                        4,  # ä¾†æºID
                        1,  # åˆ†é¡ID
                        now,
                        now,
                        0.8,  # é‡è¦æ€§åˆ†æ•¸
                        0.1,  # æƒ…æ„Ÿåˆ†æ•¸
                        'active',
                        now,
                        now
                    ))
                    
                    # æ·»åŠ åˆ°å·²å­˜åœ¨çš„å“ˆå¸Œé›†åˆ
                    existing_hashes.add(title_hash)
                    saved_count += 1
                    
                    # é¡¯ç¤ºä¿å­˜é€²åº¦
                    if saved_count <= 5 or saved_count % 10 == 0:
                        print(f"  âœ… ä¿å­˜ç¬¬{saved_count}å‰‡: {title[:40]}...")
                    
                except Exception as e:
                    print(f"  âŒ ä¿å­˜å¤±æ•—: {e}")
                    continue
            
            conn.commit()
            conn.close()
            
            print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} å‰‡æ–°è")
            print(f"âš ï¸ è·³é {duplicate_count} å‰‡é‡è¤‡æ–°è")
            
            return saved_count > 0
            
        except Exception as e:
            print(f"âŒ è³‡æ–™åº«æ“ä½œå¤±æ•—: {e}")
            return False
    
    def get_database_stats(self):
        """ç²å–è³‡æ–™åº«çµ±è¨ˆ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM news WHERE status = 'active'")
            active_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT MAX(created_at) FROM news WHERE status = 'active'")
            latest_news = cursor.fetchone()[0]
            
            conn.close()
            
            return {
                'active_news': active_count,
                'latest_news': latest_news
            }
        except Exception as e:
            print(f"âŒ ç²å–çµ±è¨ˆå¤±æ•—: {e}")
            return {'active_news': 0, 'latest_news': None}
    
    def run(self):
        """åŸ·è¡Œè¶…ç´šçˆ¬èŸ²"""
        print("ğŸš€ è¶…ç´šä¿éšªæ–°èçˆ¬èŸ²å•Ÿå‹•")
        print(f"ğŸ“š ä½¿ç”¨ {len(self.search_terms)} å€‹æœç´¢é—œéµå­—")
        print(f"ğŸ—„ï¸ è³‡æ–™åº«: {self.db_path}")
        print("=" * 60)
        
        try:
            # é¡¯ç¤ºç•¶å‰è³‡æ–™åº«ç‹€æ…‹
            before_stats = self.get_database_stats()
            print(f"ğŸ“Š åŸ·è¡Œå‰çµ±è¨ˆ: {before_stats['active_news']} å‰‡æ´»èºæ–°è")
            
            # 1. æŠ“å–æ–°è
            news_list = self.fetch_all_news()
            
            if not news_list:
                print("âŒ æ²’æœ‰æŠ“å–åˆ°ä»»ä½•æ–°è")
                return False
            
            # 2. æ‡‰ç”¨æ—¥æœŸéæ¿¾
            filtered_news = self.apply_date_filter(news_list, max_days=7)
            
            if not filtered_news:
                print("âŒ æ—¥æœŸéæ¿¾å¾Œæ²’æœ‰æ–°è")
                return False
            
            # 3. ä¿å­˜åˆ°è³‡æ–™åº«
            success = self.save_to_database(filtered_news)
            
            # 4. é¡¯ç¤ºçµæœçµ±è¨ˆ
            after_stats = self.get_database_stats()
            added_count = after_stats['active_news'] - before_stats['active_news']
            
            print("=" * 60)
            print("ğŸ“Š åŸ·è¡Œçµæœçµ±è¨ˆ:")
            print(f"  ğŸ” æœç´¢é—œéµå­—: {len(self.search_terms)} å€‹")
            print(f"  ğŸ“¡ æŠ“å–æ–°è: {len(news_list)} å‰‡")
            print(f"  âœ… æ–°å¢æ–°è: {added_count} å‰‡")
            print(f"  ğŸ“ˆ ç¸½æ´»èºæ–°è: {after_stats['active_news']} å‰‡")
            
            if success and added_count > 0:
                print("ğŸ‰ è¶…ç´šçˆ¬èŸ²åŸ·è¡ŒæˆåŠŸï¼")
                print("ğŸ’¡ é‡æ–°æ•´ç†ç¶²é æ‡‰è©²èƒ½çœ‹åˆ°æ›´å¤šæ–°çš„ä¿éšªæ–°è")
                return True
            else:
                print("âš ï¸ æ²’æœ‰æ–°çš„æ–°èè¢«ä¿å­˜ï¼ˆå¯èƒ½éƒ½æ˜¯é‡è¤‡çš„ï¼‰")
                return False
            
        except Exception as e:
            print(f"âŒ è¶…ç´šçˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
            return False

def main():
    """ä¸»ç¨‹å¼"""
    crawler = SuperInsuranceCrawler()
    return crawler.run()

if __name__ == "__main__":
    main()
