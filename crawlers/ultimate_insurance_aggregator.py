#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
çµ‚æ¥µä¿éšªæ–°èèšåˆå™¨
Ultimate Insurance News Aggregator

ä½¿ç”¨æœ€å…¨é¢çš„æ–°èæºå’Œé—œéµå­—é…ç½®
"""

import requests
import feedparser
import sqlite3
import os
import yaml
from datetime import datetime, timezone, timedelta
from urllib.parse import quote
import re
import hashlib
import time
import random
import concurrent.futures
from threading import Lock

class UltimateInsuranceAggregator:
    def __init__(self):
        self.db_path = self.find_database()
        self.sources_config = self.load_comprehensive_sources()
        self.search_terms = self.prepare_comprehensive_search_terms()
        self.db_lock = Lock()
        self.results = {
            'total_searched': 0,
            'total_found': 0,
            'total_saved': 0,
            'duplicates': 0,
            'errors': 0
        }
        
    def find_database(self):
        """æ‰¾åˆ°è³‡æ–™åº«æª”æ¡ˆ"""
        for db_name in ["instance/insurance_news.db", "instance/dev_insurance_news.db"]:
            if os.path.exists(db_name):
                return db_name
        return "instance/insurance_news.db"
    
    def load_comprehensive_sources(self):
        """è¼‰å…¥å…¨é¢æ–°èæºé…ç½®"""
        try:
            with open('config/comprehensive_sources.yaml', 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥å…¨é¢é…ç½®å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨é…ç½®: {e}")
            return self.get_fallback_config()
    
    def get_fallback_config(self):
        """å‚™ç”¨é…ç½®"""
        return {
            'google_news_searches': {
                'primary_insurance': ['ä¿éšª', 'äººå£½ä¿éšª', 'ç”¢éšª', 'è»Šéšª', 'å¥åº·éšª'],
                'regulatory': ['é‡‘ç®¡æœƒ ä¿éšª', 'ä¿éšªå±€'],
                'market_trends': ['ä¿éšªç§‘æŠ€', 'æ•¸ä½ä¿éšª'],
                'companies': ['åœ‹æ³°äººå£½', 'å¯Œé‚¦äººå£½', 'æ–°å…‰äººå£½']
            }
        }
    
    def prepare_comprehensive_search_terms(self):
        """æº–å‚™å…¨é¢çš„æœç´¢é—œéµå­—"""
        search_terms = []
        
        # å¾é…ç½®ä¸­æå–Googleæ–°èæœç´¢
        if 'google_news_searches' in self.sources_config:
            searches = self.sources_config['google_news_searches']
            
            for category, terms in searches.items():
                if isinstance(terms, list):
                    search_terms.extend(terms)
        
        # å¦‚æœæ²’æœ‰é…ç½®ï¼Œä½¿ç”¨é è¨­
        if not search_terms:
            search_terms = [
                'ä¿éšª å°ç£', 'äººå£½ä¿éšª å°ç£', 'ç”¢éšª å°ç£', 'è»Šéšª å°ç£',
                'å¥åº·éšª å°ç£', 'é†«ç™‚éšª å°ç£', 'æ„å¤–éšª å°ç£', 'å¹´é‡‘éšª å°ç£',
                'é•·ç…§éšª å°ç£', 'å¤±èƒ½éšª å°ç£', 'é‡‘ç®¡æœƒ ä¿éšª', 'ä¿éšªå±€',
                'ä¿éšªç§‘æŠ€ å°ç£', 'æ•¸ä½ä¿éšª', 'ç¶²è·¯æŠ•ä¿', 'ä¿éšªç†è³ ',
                'ä¿éšªç³¾ç´›', 'ä¿éšªè©é¨™', 'åœ‹æ³°äººå£½', 'å¯Œé‚¦äººå£½'
            ]
        
        # å»é‡ä¸¦é™åˆ¶æ•¸é‡ï¼ˆé¿å…éåº¦è«‹æ±‚ï¼‰
        search_terms = list(set(search_terms))[:50]  # æœ€å¤š50å€‹é—œéµå­—
        
        print(f"ğŸ¯ æº–å‚™ä½¿ç”¨ {len(search_terms)} å€‹æœç´¢é—œéµå­—")
        return search_terms
    
    def normalize_title(self, title):
        """æ¨™æº–åŒ–æ¨™é¡Œä»¥ä¾¿æ›´å¥½åœ°æª¢æ¸¬é‡è¤‡"""
        # ç§»é™¤å¸¸è¦‹çš„å‰å¾Œç¶´å’Œæ¨™é»
        title = re.sub(r'^ã€.*?ã€‘', '', title)  # ç§»é™¤ã€ã€‘æ¨™è¨˜
        title = re.sub(r'\s*[-|]\s*.*$', '', title)  # ç§»é™¤ä¾†æºæ¨™è¨˜
        title = re.sub(r'[^\w\s]', '', title)  # ç§»é™¤æ¨™é»ç¬¦è™Ÿ
        title = re.sub(r'\s+', ' ', title).strip()  # æ¨™æº–åŒ–ç©ºæ ¼
        return title.lower()
    
    def get_title_hash(self, title):
        """ç”Ÿæˆæ¨™é¡Œçš„å“ˆå¸Œå€¼ç”¨æ–¼é‡è¤‡æª¢æ¸¬"""
        normalized = self.normalize_title(title)
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    def is_insurance_related(self, title, content="", threshold=0.7):
        """æª¢æŸ¥æ–°èæ˜¯å¦èˆ‡ä¿éšªç›¸é—œï¼ˆå¢å¼·ç‰ˆï¼‰"""
        text = (title + " " + content).lower()
        
        # æ ¸å¿ƒä¿éšªé—œéµå­— (æ¬Šé‡3)
        core_keywords = ['ä¿éšª', 'ä¿å–®', 'ä¿è²»', 'ç†è³ ', 'æŠ•ä¿', 'çºŒä¿', 'é€€ä¿']
        core_score = sum(3 for keyword in core_keywords if keyword in text)
        
        # ä¿éšªé¡å‹ (æ¬Šé‡2)
        insurance_types = [
            'å£½éšª', 'ç”¢éšª', 'è»Šéšª', 'å¥åº·éšª', 'é†«ç™‚éšª', 'æ„å¤–éšª', 
            'å¹´é‡‘éšª', 'é•·ç…§éšª', 'å¤±èƒ½éšª', 'é‡å¤§ç–¾ç—…éšª', 'ç™Œç—‡éšª'
        ]
        type_score = sum(2 for keyword in insurance_types if keyword in text)
        
        # ä¿éšªå…¬å¸ (æ¬Šé‡2)
        companies = [
            'åœ‹æ³°', 'å¯Œé‚¦', 'æ–°å…‰', 'å—å±±', 'å°ç£äººå£½', 'å…¨çƒäººå£½',
            'ä¸‰å•†ç¾é‚¦', 'å®æ³°', 'é é›„', 'ä¸­åœ‹äººå£½'
        ]
        company_score = sum(2 for keyword in companies if keyword in text)
        
        # é‡‘èç›£ç®¡ (æ¬Šé‡3)
        regulatory = ['é‡‘ç®¡æœƒ', 'ä¿éšªå±€', 'é‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒ', 'ä¿éšªå®‰å®šåŸºé‡‘']
        regulatory_score = sum(3 for keyword in regulatory if keyword in text)
        
        # ç›¸é—œè¡“èª (æ¬Šé‡1)
        related_terms = [
            'éŠ€è¡Œä¿éšª', 'éŠ€ä¿', 'è²¡å¯Œç®¡ç†', 'ç†è²¡', 'é¢¨éšªç®¡ç†',
            'è³‡ç”¢é…ç½®', 'é€€ä¼‘è¦åŠƒ', 'é•·ç…§', 'é†«ç™‚', 'å¥åº·'
        ]
        related_score = sum(1 for keyword in related_terms if keyword in text)
        
        # è¨ˆç®—ç¸½åˆ†å’Œç›¸é—œæ€§
        total_score = core_score + type_score + company_score + regulatory_score + related_score
        max_possible = len(core_keywords) * 3 + len(insurance_types) * 2 + len(companies) * 2 + len(regulatory) * 3 + len(related_terms) * 1
        
        relevance = total_score / max_possible if max_possible > 0 else 0
        
        return relevance >= threshold or total_score >= 3
    
    def fetch_google_news_batch(self, search_term, max_results=6):
        """æ‰¹é‡æŠ“å–Googleæ–°è"""
        news_list = []
        
        try:
            # æ§‹å»ºæœç´¢URL
            encoded_term = quote(search_term)
            search_url = f"https://news.google.com/rss/search?q={encoded_term}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
            
            # éš¨æ©Ÿå»¶é²é¿å…è¢«é™åˆ¶
            time.sleep(random.uniform(0.5, 2.0))
            
            feed = feedparser.parse(search_url)
            
            if hasattr(feed, 'entries'):
                count = 0
                for entry in feed.entries:
                    if count >= max_results:
                        break
                    
                    title = entry.title
                    summary = entry.get('summary', '')
                    
                    # æª¢æŸ¥æ˜¯å¦èˆ‡ä¿éšªç›¸é—œ
                    if self.is_insurance_related(title, summary):
                        news_list.append({
                            'title': title,
                            'url': entry.link,
                            'summary': summary[:400],
                            'content': summary,
                            'published_date': datetime.now(timezone.utc),
                            'source': f'Googleæ–°è',
                            'search_term': search_term
                        })
                        count += 1
                
                self.results['total_found'] += count
                return news_list
            else:
                self.results['errors'] += 1
                return []
                
        except Exception as e:
            print(f"  âŒ æœç´¢ '{search_term}' å¤±æ•—: {e}")
            self.results['errors'] += 1
            return []
    
    def fetch_rss_source(self, source_name, rss_url):
        """æŠ“å–RSSæ–°èæº"""
        news_list = []
        
        try:
            feed = feedparser.parse(rss_url)
            
            if hasattr(feed, 'entries'):
                count = 0
                for entry in feed.entries[:10]:  # RSSæºæœ€å¤šå–10å‰‡
                    title = entry.title
                    summary = entry.get('summary', '')
                    
                    # æª¢æŸ¥æ˜¯å¦èˆ‡ä¿éšªç›¸é—œ
                    if self.is_insurance_related(title, summary):
                        news_list.append({
                            'title': title,
                            'url': entry.link,
                            'summary': summary[:400],
                            'content': entry.get('content', [{'value': summary}])[0].get('value', summary),
                            'published_date': datetime.now(timezone.utc),
                            'source': source_name,
                            'search_term': 'RSS'
                        })
                        count += 1
                
                print(f"  ğŸ“° {source_name}: {count} å‰‡ä¿éšªç›¸é—œæ–°è")
                return news_list
            else:
                return []
                
        except Exception as e:
            print(f"  âŒ RSSæº '{source_name}' æŠ“å–å¤±æ•—: {e}")
            return []
    
    def fetch_all_news_parallel(self):
        """ä¸¦è¡ŒæŠ“å–æ‰€æœ‰æ–°è"""
        all_news = []
        
        print("ğŸš€ å•Ÿå‹•çµ‚æ¥µä¿éšªæ–°èèšåˆå™¨")
        print(f"ğŸ“Š å°‡ä½¿ç”¨ {len(self.search_terms)} å€‹æœç´¢é—œéµå­—")
        print("=" * 70)
        
        # ä¸¦è¡Œè™•ç†æœç´¢é—œéµå­—
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤æ‰€æœ‰æœç´¢ä»»å‹™
            future_to_search = {}
            
            for search_term in self.search_terms:
                future = executor.submit(self.fetch_google_news_batch, search_term)
                future_to_search[future] = search_term
                self.results['total_searched'] += 1
            
            # æ”¶é›†çµæœ
            completed = 0
            for future in concurrent.futures.as_completed(future_to_search):
                search_term = future_to_search[future]
                try:
                    news_list = future.result()
                    all_news.extend(news_list)
                    completed += 1
                    
                    if completed % 10 == 0:
                        print(f"ğŸ“ˆ å·²å®Œæˆ {completed}/{len(self.search_terms)} å€‹æœç´¢ï¼Œç´¯è¨ˆ {len(all_news)} å‰‡æ–°è")
                        
                except Exception as e:
                    print(f"âŒ æœç´¢ '{search_term}' è™•ç†å¤±æ•—: {e}")
                    self.results['errors'] += 1
        
        # æª¢æŸ¥æ˜¯å¦æœ‰RSSæºé…ç½®
        rss_sources = self.extract_rss_sources()
        if rss_sources:
            print(f"\nğŸ“¡ é–‹å§‹æŠ“å– {len(rss_sources)} å€‹RSSæº...")
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                rss_futures = []
                for source_name, rss_url in rss_sources.items():
                    future = executor.submit(self.fetch_rss_source, source_name, rss_url)
                    rss_futures.append(future)
                
                for future in concurrent.futures.as_completed(rss_futures):
                    try:
                        rss_news = future.result()
                        all_news.extend(rss_news)
                    except Exception as e:
                        print(f"âŒ RSSæŠ“å–å¤±æ•—: {e}")
        
        print(f"\nğŸ“Š ç¸½å…±æŠ“å–åˆ° {len(all_news)} å‰‡æ–°è")
        return all_news
    
    def extract_rss_sources(self):
        """æå–RSSæºé…ç½®"""
        rss_sources = {}
        
        # å¾é…ç½®ä¸­æå–RSSæº
        try:
            for category, sources in self.sources_config.items():
                if isinstance(sources, dict):
                    for subcategory, source_list in sources.items():
                        if isinstance(source_list, list):
                            for source in source_list:
                                if isinstance(source, dict) and source.get('rss'):
                                    rss_sources[source['name']] = source['rss']
        except Exception as e:
            print(f"âš ï¸ æå–RSSæºå¤±æ•—: {e}")
        
        # æ·»åŠ ä¸€äº›å·²çŸ¥çš„RSSæº
        default_rss = {
            'ç¶“æ¿Ÿæ—¥å ±': 'https://udn.com/rssfeed/news/1/6644',
            'Hehoå¥åº·': 'https://heho.com.tw/feed/',
            'é¢¨å‚³åª’è²¡ç¶“': 'https://www.storm.mg/feeds/finance.xml'
        }
        
        rss_sources.update(default_rss)
        return rss_sources
    
    def save_to_database_batch(self, news_list):
        """æ‰¹é‡ä¿å­˜æ–°èåˆ°è³‡æ–™åº«"""
        if not news_list:
            print("âŒ æ²’æœ‰æ–°èè¦ä¿å­˜")
            return False
        
        print("ğŸ’¾ æ­£åœ¨æ‰¹é‡ä¿å­˜åˆ°è³‡æ–™åº«...")
        
        try:
            with self.db_lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                # ç²å–ç¾æœ‰æ–°èçš„æ¨™é¡Œå“ˆå¸Œå€¼
                cursor.execute("SELECT title FROM news WHERE status = 'active'")
                existing_hashes = set()
                for (title,) in cursor.fetchall():
                    existing_hashes.add(self.get_title_hash(title))
                
                saved_count = 0
                duplicate_count = 0
                batch_data = []
                
                for news_data in news_list:
                    try:
                        title = news_data.get('title', '').strip()
                        if not title:
                            continue
                        
                        # ä½¿ç”¨å“ˆå¸Œå€¼æª¢æŸ¥é‡è¤‡
                        title_hash = self.get_title_hash(title)
                        if title_hash in existing_hashes:
                            duplicate_count += 1
                            continue
                        
                        # æº–å‚™æ‰¹é‡æ’å…¥æ•¸æ“š
                        now = datetime.now(timezone.utc).isoformat()
                        search_term = news_data.get('search_term', '')
                        
                        # å„ªåŒ–æ¨™é¡Œé¡¯ç¤º
                        enhanced_title = title
                        if search_term and search_term != 'RSS' and len(search_term) < 15:
                            enhanced_title = f"[{search_term}] {title}"
                        
                        batch_data.append((
                            enhanced_title,
                            news_data.get('content', ''),
                            news_data.get('summary', '')[:500],
                            news_data.get('url', ''),
                            4,  # ä¾†æºID
                            1,  # åˆ†é¡ID
                            now,
                            now,
                            0.8,  # é‡è¦æ€§åˆ†æ•¸
                            0.1,  # æƒ…æ„Ÿåˆ†æ•¸
                            'active',
                            now,
                            now
                        ))
                        
                        # æ·»åŠ åˆ°å·²å­˜åœ¨çš„å“ˆå¸Œé›†åˆ
                        existing_hashes.add(title_hash)
                        saved_count += 1
                        
                        # æ‰¹é‡è™•ç†ï¼Œæ¯100ç­†æäº¤ä¸€æ¬¡
                        if len(batch_data) >= 100:
                            cursor.executemany("""
                                INSERT INTO news (
                                    title, content, summary, url, source_id, category_id,
                                    published_date, crawled_date, importance_score, 
                                    sentiment_score, status, created_at, updated_at
                                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """, batch_data)
                            conn.commit()
                            batch_data = []
                            print(f"  ğŸ’¾ å·²ä¿å­˜ {saved_count} å‰‡æ–°è...")
                        
                    except Exception as e:
                        print(f"  âŒ è™•ç†æ–°èå¤±æ•—: {e}")
                        continue
                
                # ä¿å­˜å‰©é¤˜çš„æ•¸æ“š
                if batch_data:
                    cursor.executemany("""
                        INSERT INTO news (
                            title, content, summary, url, source_id, category_id,
                            published_date, crawled_date, importance_score, 
                            sentiment_score, status, created_at, updated_at
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, batch_data)
                    conn.commit()
                
                conn.close()
                
                self.results['total_saved'] = saved_count
                self.results['duplicates'] = duplicate_count
                
                print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} å‰‡æ–°è")
                print(f"âš ï¸ è·³é {duplicate_count} å‰‡é‡è¤‡æ–°è")
                
                return saved_count > 0
                
        except Exception as e:
            print(f"âŒ è³‡æ–™åº«æ“ä½œå¤±æ•—: {e}")
            return False
    
    def get_database_stats(self):
        """ç²å–è³‡æ–™åº«çµ±è¨ˆ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM news WHERE status = 'active'")
            active_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT MAX(created_at) FROM news WHERE status = 'active'")
            latest_news = cursor.fetchone()[0]
            
            conn.close()
            
            return {
                'active_news': active_count,
                'latest_news': latest_news
            }
        except Exception as e:
            print(f"âŒ ç²å–çµ±è¨ˆå¤±æ•—: {e}")
            return {'active_news': 0, 'latest_news': None}
    
    def run(self):
        """åŸ·è¡Œçµ‚æ¥µèšåˆå™¨"""
        print("ğŸŒŸ çµ‚æ¥µä¿éšªæ–°èèšåˆå™¨å•Ÿå‹•")
        print(f"ğŸ” æœç´¢ç¯„åœ: {len(self.search_terms)} å€‹é—œéµå­—")
        print(f"ğŸ—„ï¸ è³‡æ–™åº«: {self.db_path}")
        print("=" * 70)
        
        start_time = time.time()
        
        try:
            # é¡¯ç¤ºç•¶å‰è³‡æ–™åº«ç‹€æ…‹
            before_stats = self.get_database_stats()
            print(f"ğŸ“Š åŸ·è¡Œå‰çµ±è¨ˆ: {before_stats['active_news']} å‰‡æ´»èºæ–°è")
            
            # 1. ä¸¦è¡ŒæŠ“å–æ‰€æœ‰æ–°è
            news_list = self.fetch_all_news_parallel()
            
            if not news_list:
                print("âŒ æ²’æœ‰æŠ“å–åˆ°ä»»ä½•æ–°è")
                return False
            
            # 2. æ‰¹é‡ä¿å­˜åˆ°è³‡æ–™åº«
            success = self.save_to_database_batch(news_list)
            
            # 3. é¡¯ç¤ºè©³ç´°çµæœçµ±è¨ˆ
            after_stats = self.get_database_stats()
            added_count = after_stats['active_news'] - before_stats['active_news']
            execution_time = time.time() - start_time
            
            print("=" * 70)
            print("ğŸ¯ çµ‚æ¥µèšåˆå™¨åŸ·è¡Œçµæœ:")
            print(f"  â±ï¸ åŸ·è¡Œæ™‚é–“: {execution_time:.1f} ç§’")
            print(f"  ğŸ” æœç´¢é—œéµå­—: {self.results['total_searched']} å€‹")
            print(f"  ğŸ“¡ æŠ“å–æ–°è: {self.results['total_found']} å‰‡")
            print(f"  âœ… æ–°å¢æ–°è: {added_count} å‰‡")
            print(f"  âš ï¸ é‡è¤‡æ–°è: {self.results['duplicates']} å‰‡")
            print(f"  âŒ éŒ¯èª¤æ•¸é‡: {self.results['errors']} å€‹")
            print(f"  ğŸ“ˆ ç¸½æ´»èºæ–°è: {after_stats['active_news']} å‰‡")
            print(f"  âš¡ å¹³å‡é€Ÿåº¦: {self.results['total_found']/execution_time:.1f} å‰‡/ç§’")
            
            if success and added_count > 0:
                print("\nğŸ‰ çµ‚æ¥µèšåˆå™¨åŸ·è¡ŒæˆåŠŸï¼")
                print("ğŸ’¡ æ‚¨çš„ä¿éšªæ–°èåº«å·²å¤§å¹…æ“´å±•ï¼Œé‡æ–°æ•´ç†ç¶²é å³å¯æŸ¥çœ‹")
                return True
            else:
                print("\nâš ï¸ åŸ·è¡Œå®Œæˆï¼Œä½†æ²’æœ‰æ–°å¢æ–°èï¼ˆå¯èƒ½éƒ½æ˜¯é‡è¤‡çš„ï¼‰")
                return False
            
        except Exception as e:
            print(f"âŒ çµ‚æ¥µèšåˆå™¨åŸ·è¡Œå¤±æ•—: {e}")
            return False

def main():
    """ä¸»ç¨‹å¼"""
    aggregator = UltimateInsuranceAggregator()
    return aggregator.run()

if __name__ == "__main__":
    main()
