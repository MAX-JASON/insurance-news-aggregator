#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¯æ—¥ä¿éšªæ–°èçˆ¬èŸ² - 60ç¯‡é™å®šç‰ˆ
Daily Insurance News Crawler - 60 Articles Limit

å°ˆé–€ç‚ºæ¯æ—¥é–±è®€è¨­è¨ˆï¼Œæ§åˆ¶åœ¨60ç¯‡ç²¾é¸æ–°è
"""

import requests
import feedparser
import sqlite3
import os
from datetime import datetime, timezone, timedelta
from urllib.parse import quote
import re
import hashlib
import time
import random
import sys
import uuid

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

# å°å…¥åœ–ç‰‡æå–å·¥å…·
try:
    from utils.image_extractor import extract_image_from_url
    IMAGE_EXTRACTION_ENABLED = True
    print("âœ… åœ–ç‰‡æå–åŠŸèƒ½å·²å•Ÿç”¨")
except ImportError as e:
    print(f"âš ï¸ åœ–ç‰‡æå–åŠŸèƒ½æœªå•Ÿç”¨: {e}")
    IMAGE_EXTRACTION_ENABLED = False

class DailyInsuranceCrawler:
    def __init__(self, target_count=60):
        self.db_path = self.find_database()
        self.target_count = target_count
        self.search_terms = self.get_focused_search_terms()
        
    def find_database(self):
        """æ‰¾åˆ°è³‡æ–™åº«æª”æ¡ˆ"""
        for db_name in ["instance/insurance_news.db", "instance/dev_insurance_news.db"]:
            if os.path.exists(db_name):
                return db_name
        return "instance/insurance_news.db"
    
    def get_focused_search_terms(self):
        """ç²å–ç²¾é¸çš„æœç´¢é—œéµå­—ï¼ˆå°ˆæ³¨æ–¼é‡è¦æ–°èï¼‰"""
        # ç²¾é¸é—œéµå­—ï¼Œå„ªå…ˆç²å–é‡è¦æ–°è
        terms = [
            # æ ¸å¿ƒä¿éšªï¼ˆé«˜å„ªå…ˆç´šï¼‰
            'ä¿éšª å°ç£', 'äººå£½ä¿éšª å°ç£', 'ç”¢éšª å°ç£', 
            'è»Šéšª å°ç£', 'å¥åº·éšª å°ç£', 'é†«ç™‚éšª å°ç£',
            
            # ç›£ç®¡æ”¿ç­–ï¼ˆé«˜å„ªå…ˆç´šï¼‰
            'é‡‘ç®¡æœƒ ä¿éšª', 'ä¿éšªå±€ å°ç£', 'ä¿éšªæ³•ä¿®æ­£',
            
            # ä¸»è¦ä¿éšªå…¬å¸ï¼ˆä¸­å„ªå…ˆç´šï¼‰
            'åœ‹æ³°äººå£½', 'å¯Œé‚¦äººå£½', 'æ–°å…‰äººå£½', 'å—å±±äººå£½',
            'å°ç£äººå£½', 'å…¨çƒäººå£½', 'åœ‹æ³°ç”¢éšª', 'å¯Œé‚¦ç”¢éšª',
            
            # é‡è¦è­°é¡Œï¼ˆä¸­å„ªå…ˆç´šï¼‰
            'é•·ç…§éšª å°ç£', 'å¹´é‡‘éšª å°ç£', 'ä¿éšªç§‘æŠ€ å°ç£',
            'ä¿éšªç†è³ ', 'ä¿éšªç³¾ç´›', 'æ•¸ä½ä¿éšª',
            
            # ç¤¾æœƒç›¸é—œï¼ˆä½å„ªå…ˆç´šï¼‰
            'é«˜é½¡åŒ– ä¿éšª', 'é€€ä¼‘è¦åŠƒ å°ç£', 'é†«ç™‚ ä¿éšª å°ç£'
        ]
        
        print(f"ğŸ¯ ä½¿ç”¨ {len(terms)} å€‹ç²¾é¸é—œéµå­—ï¼Œç›®æ¨™ {self.target_count} ç¯‡æ–°è")
        return terms
    
    def normalize_title(self, title):
        """æ¨™æº–åŒ–æ¨™é¡Œ"""
        if not title:
            return ""
        
        # ç§»é™¤å¸¸è¦‹çš„å‰å¾Œç¶´
        title = re.sub(r'^ã€.*?ã€‘', '', title)
        title = re.sub(r'\s*[-|]\s*.*?(æ–°èç¶²|æ™‚å ±|æ—¥å ±).*$', '', title)
        title = re.sub(r'[^\w\s]', '', title)
        title = re.sub(r'\s+', ' ', title).strip()
        return title.lower()
    
    def get_content_hash(self, title, content=""):
        """ç”Ÿæˆå…§å®¹å“ˆå¸Œå€¼"""
        combined = self.normalize_title(title) + " " + content[:100].lower()
        combined = re.sub(r'\s+', ' ', combined).strip()
        return hashlib.md5(combined.encode('utf-8')).hexdigest()
    
    def is_insurance_related(self, title, content=""):
        """æª¢æŸ¥æ˜¯å¦èˆ‡ä¿éšªç›¸é—œ"""
        text = (title + " " + content).lower()
        
        # æ ¸å¿ƒä¿éšªé—œéµå­—ï¼ˆå¿…é ˆåŒ…å«ï¼‰
        core_keywords = ['ä¿éšª', 'ä¿å–®', 'ä¿è²»', 'ç†è³ ', 'æŠ•ä¿']
        if not any(keyword in text for keyword in core_keywords):
            return False
        
        # è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸
        score = 0
        
        # ä¿éšªé¡å‹ (+2åˆ†)
        insurance_types = ['å£½éšª', 'ç”¢éšª', 'è»Šéšª', 'å¥åº·éšª', 'é†«ç™‚éšª', 'æ„å¤–éšª', 'å¹´é‡‘éšª', 'é•·ç…§éšª']
        score += sum(2 for keyword in insurance_types if keyword in text)
        
        # ä¿éšªå…¬å¸ (+1åˆ†)
        companies = ['åœ‹æ³°', 'å¯Œé‚¦', 'æ–°å…‰', 'å—å±±', 'å°ç£äººå£½', 'å…¨çƒäººå£½']
        score += sum(1 for keyword in companies if keyword in text)
        
        # ç›£ç®¡ç›¸é—œ (+3åˆ†)
        regulatory = ['é‡‘ç®¡æœƒ', 'ä¿éšªå±€']
        score += sum(3 for keyword in regulatory if keyword in text)
        
        return score >= 2
    
    def fetch_news_for_term(self, search_term, max_results):
        """ç‚ºå–®å€‹æœç´¢è©æŠ“å–æ–°è"""
        news_list = []
        
        try:
            encoded_term = quote(search_term)
            search_url = f"https://news.google.com/rss/search?q={encoded_term}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
            
            time.sleep(random.uniform(0.5, 1.5))
            
            feed = feedparser.parse(search_url)
            
            if hasattr(feed, 'entries'):
                count = 0
                for entry in feed.entries:
                    if count >= max_results:
                        break
                    
                    title = entry.title
                    summary = entry.get('summary', '')
                    
                    if self.is_insurance_related(title, summary):
                        news_list.append({
                            'title': title,
                            'url': entry.link,
                            'summary': summary[:300],
                            'content': summary,
                            'published_date': datetime.now(timezone.utc),
                            'source': 'Googleæ–°è',
                            'search_term': search_term
                        })
                        count += 1
                
                return news_list
            else:
                return []
                
        except Exception as e:
            print(f"  âŒ æœç´¢ '{search_term}' å¤±æ•—: {e}")
            return []
    
    def fetch_daily_news(self):
        """æŠ“å–æ¯æ—¥æ–°èï¼ˆæ™ºèƒ½åˆ†é…ï¼‰"""
        all_news = []
        
        print("ğŸ“° é–‹å§‹æŠ“å–æ¯æ—¥ä¿éšªæ–°è...")
        print("=" * 50)
        
        # æ ¹æ“šé—œéµå­—é‡è¦æ€§åˆ†é…æ•¸é‡
        high_priority = self.search_terms[:6]      # å‰6å€‹é«˜å„ªå…ˆç´šï¼Œæ¯å€‹8ç¯‡ = 48ç¯‡
        medium_priority = self.search_terms[6:14]  # ä¸­8å€‹ä¸­å„ªå…ˆç´šï¼Œæ¯å€‹1ç¯‡ = 8ç¯‡  
        low_priority = self.search_terms[14:]      # å…¶é¤˜ä½å„ªå…ˆç´šï¼Œæ¯å€‹1ç¯‡ = 4ç¯‡
        
        # é«˜å„ªå…ˆç´šé—œéµå­—
        print("ğŸ”¥ æŠ“å–é«˜å„ªå…ˆç´šæ–°è...")
        for i, term in enumerate(high_priority, 1):
            print(f"  {i}/6: {term}")
            news = self.fetch_news_for_term(term, 8)
            all_news.extend(news)
            if len(all_news) >= 48:
                break
        
        # å¦‚æœé‚„æ²’é”åˆ°ç›®æ¨™ï¼ŒæŠ“å–ä¸­å„ªå…ˆç´š
        if len(all_news) < self.target_count:
            print("ğŸ“Š æŠ“å–ä¸­å„ªå…ˆç´šæ–°è...")
            remaining = self.target_count - len(all_news)
            per_term = max(1, remaining // len(medium_priority))
            
            for term in medium_priority:
                if len(all_news) >= self.target_count:
                    break
                news = self.fetch_news_for_term(term, per_term)
                all_news.extend(news)
        
        # å¦‚æœé‚„ä¸å¤ ï¼ŒæŠ“å–ä½å„ªå…ˆç´š
        if len(all_news) < self.target_count:
            print("ğŸ“ˆ æŠ“å–è£œå……æ–°è...")
            remaining = self.target_count - len(all_news)
            
            for term in low_priority:
                if len(all_news) >= self.target_count:
                    break
                news = self.fetch_news_for_term(term, 1)
                all_news.extend(news)
        
        print(f"ğŸ“Š ç¸½å…±æŠ“å–åˆ° {len(all_news)} å‰‡æ–°è")
        return all_news
    
    def deduplicate_and_limit(self, news_list):
        """å»é‡ä¸¦é™åˆ¶æ•¸é‡"""
        print("ğŸ”„ æ­£åœ¨å»é‡ä¸¦é™åˆ¶æ•¸é‡...")
        
        seen_hashes = set()
        unique_news = []
        
        for news in news_list:
            content_hash = self.get_content_hash(news['title'], news['content'])
            if content_hash not in seen_hashes:
                seen_hashes.add(content_hash)
                unique_news.append(news)
                
                # é”åˆ°ç›®æ¨™æ•¸é‡å°±åœæ­¢
                if len(unique_news) >= self.target_count:
                    break
        
        print(f"âœ… å»é‡å®Œæˆï¼Œæœ€çµ‚ä¿ç•™ {len(unique_news)} å‰‡æ–°è")
        return unique_news
    
    def save_to_database(self, news_list):
        """ä¿å­˜æ–°èåˆ°è³‡æ–™åº«"""
        if not news_list:
            print("âŒ æ²’æœ‰æ–°èè¦ä¿å­˜")
            return False
        
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°è³‡æ–™åº«...")
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ç²å–ç¾æœ‰æ–°èçš„å…§å®¹å“ˆå¸Œå€¼
            cursor.execute("SELECT title, content FROM news WHERE status = 'active'")
            existing_hashes = set()
            for title, content in cursor.fetchall():
                existing_hashes.add(self.get_content_hash(title, content or ""))
            
            saved_count = 0
            duplicate_count = 0
            
            for news_data in news_list:
                try:
                    title = news_data.get('title', '').strip()
                    content = news_data.get('content', '')
                    
                    if not title:
                        continue
                    
                    # æª¢æŸ¥é‡è¤‡
                    content_hash = self.get_content_hash(title, content)
                    if content_hash in existing_hashes:
                        duplicate_count += 1
                        continue
                    
                    # ç”Ÿæˆå”¯ä¸€URL
                    import uuid
                    original_url = news_data.get('url', '')
                    unique_url = f"{original_url}#daily_{uuid.uuid4().hex[:8]}"
                    
                    # æå–åœ–ç‰‡ URL
                    image_url = None
                    if IMAGE_EXTRACTION_ENABLED and original_url:
                        try:
                            print(f"  ğŸ–¼ï¸ æ­£åœ¨æå–åœ–ç‰‡: {title[:30]}...")
                            image_url = extract_image_from_url(original_url)
                            if image_url:
                                print(f"  âœ… åœ–ç‰‡å·²ç²å–: {image_url[:50]}...")
                            else:
                                print(f"  âš ï¸ æœªæ‰¾åˆ°åˆé©åœ–ç‰‡")
                        except Exception as img_e:
                            print(f"  âŒ åœ–ç‰‡æå–å¤±æ•—: {img_e}")
                    
                    # æº–å‚™æ’å…¥æ•¸æ“š
                    now = datetime.now(timezone.utc).isoformat()
                    
                    cursor.execute("""
                        INSERT INTO news (
                            title, content, summary, url, source_id, category_id,
                            published_date, crawled_date, importance_score, 
                            sentiment_score, status, created_at, updated_at, image_url
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        title,
                        content,
                        news_data.get('summary', '')[:400],
                        unique_url,
                        4,  # ä¾†æºID
                        1,  # åˆ†é¡ID
                        now,
                        now,
                        0.8,  # é‡è¦æ€§åˆ†æ•¸
                        0.1,  # æƒ…æ„Ÿåˆ†æ•¸
                        'active',
                        now,
                        now,
                        image_url  # åœ–ç‰‡URL
                    ))
                    
                    existing_hashes.add(content_hash)
                    saved_count += 1
                    
                    if saved_count <= 5 or saved_count % 10 == 0:
                        print(f"  âœ… å·²ä¿å­˜ {saved_count} å‰‡æ–°è...")
                    
                except Exception as e:
                    print(f"  âŒ ä¿å­˜å¤±æ•—: {e}")
                    continue
            
            conn.commit()
            conn.close()
            
            print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} å‰‡æ–°è")
            print(f"âš ï¸ è·³é {duplicate_count} å‰‡é‡è¤‡æ–°è")
            
            return saved_count > 0
            
        except Exception as e:
            print(f"âŒ è³‡æ–™åº«æ“ä½œå¤±æ•—: {e}")
            return False
    
    def get_database_stats(self):
        """ç²å–è³‡æ–™åº«çµ±è¨ˆ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM news WHERE status = 'active'")
            active_count = cursor.fetchone()[0]
            
            conn.close()
            return {'active_news': active_count}
        except Exception as e:
            print(f"âŒ ç²å–çµ±è¨ˆå¤±æ•—: {e}")
            return {'active_news': 0}
    
    def run(self):
        """åŸ·è¡Œæ¯æ—¥çˆ¬èŸ²"""
        print("â˜€ï¸ æ¯æ—¥ä¿éšªæ–°èçˆ¬èŸ²å•Ÿå‹•")
        print(f"ğŸ¯ ç›®æ¨™æ•¸é‡: {self.target_count} ç¯‡æ–°è")
        print(f"ğŸ—„ï¸ è³‡æ–™åº«: {self.db_path}")
        print("=" * 50)
        
        start_time = time.time()
        
        try:
            # é¡¯ç¤ºç•¶å‰ç‹€æ…‹
            before_stats = self.get_database_stats()
            print(f"ğŸ“Š åŸ·è¡Œå‰: {before_stats['active_news']} å‰‡æ´»èºæ–°è")
            
            # 1. æŠ“å–æ¯æ—¥æ–°è
            news_list = self.fetch_daily_news()
            
            if not news_list:
                print("âŒ æ²’æœ‰æŠ“å–åˆ°ä»»ä½•æ–°è")
                return False
            
            # 2. å»é‡ä¸¦é™åˆ¶æ•¸é‡
            final_news = self.deduplicate_and_limit(news_list)
            
            if not final_news:
                print("âŒ å»é‡å¾Œæ²’æœ‰æ–°è")
                return False
            
            # 3. ä¿å­˜åˆ°è³‡æ–™åº«
            success = self.save_to_database(final_news)
            
            # 4. é¡¯ç¤ºçµæœ
            after_stats = self.get_database_stats()
            added_count = after_stats['active_news'] - before_stats['active_news']
            execution_time = time.time() - start_time
            
            print("=" * 50)
            print("ğŸ“ˆ æ¯æ—¥çˆ¬èŸ²åŸ·è¡Œçµæœ:")
            print(f"  â±ï¸ åŸ·è¡Œæ™‚é–“: {execution_time:.1f} ç§’")
            print(f"  ğŸ“¡ æŠ“å–æ–°è: {len(news_list)} å‰‡")
            print(f"  ğŸ¯ ç›®æ¨™æ•¸é‡: {self.target_count} å‰‡")
            print(f"  âœ… å¯¦éš›æ–°å¢: {added_count} å‰‡")
            print(f"  ğŸ“ˆ ç¸½æ´»èºæ–°è: {after_stats['active_news']} å‰‡")
            
            if success and added_count > 0:
                print(f"\nğŸ‰ æ¯æ—¥æ–°èæ›´æ–°å®Œæˆï¼æ–°å¢äº† {added_count} å‰‡ç²¾é¸ä¿éšªæ–°è")
                print("ğŸ’¡ ç¾åœ¨å¯ä»¥æŸ¥çœ‹ç¶²ç«™ç²å¾—ä»Šæ—¥æœ€æ–°ä¿éšªè³‡è¨Š")
                return True
            else:
                print("\nâš ï¸ åŸ·è¡Œå®Œæˆï¼Œä½†æ²’æœ‰æ–°å¢æ–°èï¼ˆå¯èƒ½éƒ½æ˜¯é‡è¤‡çš„ï¼‰")
                return False
            
        except Exception as e:
            print(f"âŒ æ¯æ—¥çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
            return False

def main():
    """ä¸»ç¨‹å¼"""
    print("è«‹é¸æ“‡æ¯æ—¥æ–°èæ•¸é‡:")
    print("1. 60ç¯‡æ–°è (æ¨è–¦)")
    print("2. 30ç¯‡æ–°è (ç²¾ç°¡)")
    print("3. 100ç¯‡æ–°è (å®Œæ•´)")
    print("4. è‡ªè¨‚æ•¸é‡")
    
    choice = input("\nè«‹é¸æ“‡ (1-4): ").strip()
    
    if choice == "1":
        target_count = 60
    elif choice == "2":
        target_count = 30
    elif choice == "3":
        target_count = 100
    elif choice == "4":
        try:
            target_count = int(input("è«‹è¼¸å…¥ç›®æ¨™æ–°èæ•¸é‡: "))
            if target_count <= 0 or target_count > 200:
                print("âŒ æ•¸é‡å¿…é ˆåœ¨1-200ä¹‹é–“")
                return
        except ValueError:
            print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")
            return
    else:
        print("âŒ ç„¡æ•ˆé¸æ“‡")
        return
    
    crawler = DailyInsuranceCrawler(target_count)
    crawler.run()

if __name__ == "__main__":
    main()
