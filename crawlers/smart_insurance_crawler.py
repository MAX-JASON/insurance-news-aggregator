#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ä¿éšªæ–°èçˆ¬èŸ² - æœ€çµ‚ç‰ˆ
Smart Insurance News Crawler - Final Version

è§£æ±ºURLé‡è¤‡å•é¡Œï¼Œä½¿ç”¨æ™ºèƒ½å»é‡ç®—æ³•
"""

import requests
import feedparser
import sqlite3
import os
import yaml
from datetime import datetime, timezone, timedelta
from urllib.parse import quote, urlparse
import re
import hashlib
import time
import random
import uuid

class SmartInsuranceCrawler:
    def __init__(self):
        self.db_path = self.find_database()
        self.search_terms = self.get_expanded_search_terms()
        
    def find_database(self):
        """æ‰¾åˆ°è³‡æ–™åº«æª”æ¡ˆ"""
        for db_name in ["instance/insurance_news.db", "instance/dev_insurance_news.db"]:
            if os.path.exists(db_name):
                return db_name
        return "instance/insurance_news.db"
    
    def get_expanded_search_terms(self):
        """ç²å–æ“´å±•çš„æœç´¢é—œéµå­—"""
        # è¶…ç´šæ“´å±•çš„ä¿éšªç›¸é—œé—œéµå­—
        terms = [
            # æ ¸å¿ƒä¿éšªé¡å‹
            'ä¿éšª å°ç£', 'äººå£½ä¿éšª å°ç£', 'ç”¢éšª å°ç£', 'è»Šéšª å°ç£', 
            'å¥åº·éšª å°ç£', 'é†«ç™‚éšª å°ç£', 'æ„å¤–éšª å°ç£', 'å¹´é‡‘éšª å°ç£',
            'é•·ç…§éšª å°ç£', 'å¤±èƒ½éšª å°ç£', 'é‡å¤§ç–¾ç—…éšª', 'ç™Œç—‡éšª å°ç£',
            
            # ç›£ç®¡èˆ‡æ”¿ç­–
            'é‡‘ç®¡æœƒ ä¿éšª', 'ä¿éšªå±€ å°ç£', 'ä¿éšªæ³•ä¿®æ­£', 'ä¿éšªæ¥­æ³•',
            'é‡‘èæª¢æŸ¥ ä¿éšª', 'ä¿éšªç›£ç†', 'ä¿éšªæ”¿ç­–', 'ä¿éšªæ”¹é©',
            
            # ä¿éšªç§‘æŠ€èˆ‡å‰µæ–°
            'ä¿éšªç§‘æŠ€ å°ç£', 'æ•¸ä½ä¿éšª', 'ç¶²è·¯æŠ•ä¿', 'AIä¿éšª',
            'å€å¡Šéˆä¿éšª', 'ä¿éšªå‰µæ–°', 'InsurTechå°ç£', 'é‡‘èç§‘æŠ€ä¿éšª',
            
            # ä¸»è¦ä¿éšªå…¬å¸
            'åœ‹æ³°äººå£½', 'å¯Œé‚¦äººå£½', 'æ–°å…‰äººå£½', 'å—å±±äººå£½', 
            'å°ç£äººå£½', 'å…¨çƒäººå£½', 'ä¸‰å•†ç¾é‚¦', 'ä¸­åœ‹äººå£½å°ç£',
            'åœ‹æ³°ç”¢éšª', 'å¯Œé‚¦ç”¢éšª', 'æ–°å…‰ç”¢éšª', 'æ˜å°ç”¢éšª',
            
            # æ¥­å‹™èˆ‡æœå‹™
            'ä¿éšªç†è³ ', 'ä¿éšªç³¾ç´›', 'ä¿éšªç”³è¨´', 'ä¿éšªè©•è­°',
            'ä¿éšªè©é¨™ å°ç£', 'éŠ€è¡Œä¿éšª', 'ä¿éšªé€šè·¯', 'ä¿éšªæ¥­ç¸¾',
            
            # ç¤¾æœƒè­°é¡Œç›¸é—œ
            'é«˜é½¡åŒ– ä¿éšª', 'å°‘å­åŒ– ä¿éšª', 'é•·ç…§ ä¿éšª', 'é€€ä¼‘è¦åŠƒ å°ç£',
            'é†«ç™‚ ä¿éšª å°ç£', 'å¥ä¿ ä¿éšª', 'è·ç½ ä¿éšª', 'äº¤é€šäº‹æ•… ä¿éšª',
            
            # ç¶“æ¿Ÿèˆ‡å¸‚å ´
            'ä¿éšªå¸‚å ´ å°ç£', 'ä¿éšªä½µè³¼', 'ä¿éšªæŠ•è³‡', 'ä¿éšªæº–å‚™é‡‘',
            'ä¿éšªè³‡æœ¬', 'ä¿éšªç²åˆ©', 'ä¿éšªè‚¡åƒ¹', 'ä¿éšªåˆ†ç´…',
            
            # ç‰¹æ®Šéšªç¨®
            'æ—…å¹³éšª å°ç£', 'å¯µç‰©ä¿éšª å°ç£', 'ä½å®…éšª', 'ç«éšª å°ç£',
            'åœ°éœ‡éšª å°ç£', 'é¢±é¢¨éšª', 'ä¼æ¥­ä¿éšª', 'è‘£ç›£äº‹è²¬ä»»éšª',
            
            # åœ‹éš›èˆ‡è¶¨å‹¢
            'ESGä¿éšª', 'æ°¸çºŒä¿éšª', 'æ°£å€™ä¿éšª', 'ç¶ è‰²ä¿éšª',
            'ç–«æƒ…ä¿éšª', 'COVIDä¿éšª', 'é˜²ç–«ä¿éšª å°ç£',
            
            # å°ˆæ¥­æœå‹™
            'ä¿éšªä»£ç†äºº', 'ä¿éšªç¶“ç´€äºº', 'ä¿éšªå…¬è­‰äºº', 'ä¿éšªç²¾ç®—',
            'ä¿éšªæ³•å‹™', 'ä¿éšªç¨…å‹™', 'ä¿éšªæœƒè¨ˆ', 'ä¿éšªé¡§å•'
        ]
        
        print(f"ğŸ¯ æº–å‚™ä½¿ç”¨ {len(terms)} å€‹æ“´å±•æœç´¢é—œéµå­—")
        return terms
    
    def clean_url(self, url):
        """æ¸…ç†å’Œæ¨™æº–åŒ–URL"""
        if not url:
            return ""
        
        # ç§»é™¤Googleæ–°èçš„é‡å®šå‘åƒæ•¸
        if 'news.google.com' in url and 'url=' in url:
            try:
                import urllib.parse
                parsed = urllib.parse.urlparse(url)
                params = urllib.parse.parse_qs(parsed.query)
                if 'url' in params:
                    url = params['url'][0]
            except:
                pass
        
        # ç§»é™¤å¸¸è¦‹çš„è¿½è¹¤åƒæ•¸
        tracking_params = ['utm_source', 'utm_medium', 'utm_campaign', 'utm_content', 'utm_term', 'ref', 'source']
        try:
            from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
            parsed = urlparse(url)
            query_params = parse_qs(parsed.query)
            
            # ç§»é™¤è¿½è¹¤åƒæ•¸
            for param in tracking_params:
                query_params.pop(param, None)
            
            # é‡å»ºURL
            new_query = urlencode(query_params, doseq=True)
            cleaned_url = urlunparse((
                parsed.scheme, parsed.netloc, parsed.path,
                parsed.params, new_query, parsed.fragment
            ))
            
            return cleaned_url
        except:
            return url
    
    def normalize_title(self, title):
        """æ¨™æº–åŒ–æ¨™é¡Œ"""
        if not title:
            return ""
        
        # ç§»é™¤å¸¸è¦‹çš„å‰å¾Œç¶´
        title = re.sub(r'^ã€.*?ã€‘', '', title)  # ç§»é™¤ã€ã€‘æ¨™è¨˜
        title = re.sub(r'\s*[-|]\s*.*?æ–°èç¶².*$', '', title)  # ç§»é™¤ä¾†æºæ¨™è¨˜
        title = re.sub(r'\s*[-|]\s*.*?æ™‚å ±.*$', '', title)  # ç§»é™¤æ™‚å ±æ¨™è¨˜
        title = re.sub(r'\s*[-|]\s*.*?æ—¥å ±.*$', '', title)  # ç§»é™¤æ—¥å ±æ¨™è¨˜
        title = re.sub(r'[^\w\s]', '', title)  # ç§»é™¤æ¨™é»ç¬¦è™Ÿ
        title = re.sub(r'\s+', ' ', title).strip()  # æ¨™æº–åŒ–ç©ºæ ¼
        return title.lower()
    
    def get_content_hash(self, title, content=""):
        """ç”Ÿæˆå…§å®¹å“ˆå¸Œå€¼ç”¨æ–¼æ™ºèƒ½å»é‡"""
        # åˆä½µæ¨™é¡Œå’Œå…§å®¹çš„å‰200å­—
        combined = self.normalize_title(title) + " " + content[:200].lower()
        combined = re.sub(r'\s+', ' ', combined).strip()
        return hashlib.md5(combined.encode('utf-8')).hexdigest()
    
    def is_insurance_related(self, title, content=""):
        """å¢å¼·çš„ä¿éšªç›¸é—œæ€§æª¢æ¸¬"""
        text = (title + " " + content).lower()
        
        # æ ¸å¿ƒä¿éšªé—œéµå­—ï¼ˆå¿…é ˆåŒ…å«è‡³å°‘ä¸€å€‹ï¼‰
        core_keywords = ['ä¿éšª', 'ä¿å–®', 'ä¿è²»', 'ç†è³ ', 'æŠ•ä¿']
        has_core = any(keyword in text for keyword in core_keywords)
        
        if not has_core:
            return False
        
        # è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸
        score = 0
        
        # ä¿éšªé¡å‹ (+2åˆ†æ¯å€‹)
        insurance_types = [
            'å£½éšª', 'ç”¢éšª', 'è»Šéšª', 'å¥åº·éšª', 'é†«ç™‚éšª', 'æ„å¤–éšª', 
            'å¹´é‡‘éšª', 'é•·ç…§éšª', 'å¤±èƒ½éšª', 'é‡å¤§ç–¾ç—…éšª', 'ç™Œç—‡éšª'
        ]
        score += sum(2 for keyword in insurance_types if keyword in text)
        
        # ä¿éšªå…¬å¸ (+1åˆ†æ¯å€‹)
        companies = [
            'åœ‹æ³°', 'å¯Œé‚¦', 'æ–°å…‰', 'å—å±±', 'å°ç£äººå£½', 'å…¨çƒäººå£½',
            'ä¸‰å•†ç¾é‚¦', 'å®æ³°', 'é é›„', 'ä¸­åœ‹äººå£½', 'æ˜å°', 'ç”¢éšª'
        ]
        score += sum(1 for keyword in companies if keyword in text)
        
        # ç›£ç®¡æ©Ÿæ§‹ (+3åˆ†æ¯å€‹)
        regulatory = ['é‡‘ç®¡æœƒ', 'ä¿éšªå±€', 'é‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒ']
        score += sum(3 for keyword in regulatory if keyword in text)
        
        # æ¥­å‹™ç›¸é—œ (+1åˆ†æ¯å€‹)
        business_terms = ['ç†è³ ', 'æŠ•ä¿', 'çºŒä¿', 'é€€ä¿', 'ä¿è²»', 'ä¿é¡', 'çµ¦ä»˜']
        score += sum(1 for keyword in business_terms if keyword in text)
        
        # å¦‚æœåˆ†æ•¸é”åˆ°3åˆ†ä»¥ä¸Šï¼Œèªç‚ºé«˜åº¦ç›¸é—œ
        return score >= 3
    
    def fetch_news_for_term(self, search_term, max_results=8):
        """ç‚ºå–®å€‹æœç´¢è©æŠ“å–æ–°è"""
        news_list = []
        
        try:
            # æ§‹å»ºæœç´¢URL
            encoded_term = quote(search_term)
            search_url = f"https://news.google.com/rss/search?q={encoded_term}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
            
            # éš¨æ©Ÿå»¶é²
            time.sleep(random.uniform(0.3, 1.5))
            
            feed = feedparser.parse(search_url)
            
            if hasattr(feed, 'entries'):
                count = 0
                for entry in feed.entries:
                    if count >= max_results:
                        break
                    
                    title = entry.title
                    summary = entry.get('summary', '')
                    url = self.clean_url(entry.link)
                    
                    # æª¢æŸ¥æ˜¯å¦èˆ‡ä¿éšªç›¸é—œ
                    if self.is_insurance_related(title, summary):
                        news_list.append({
                            'title': title,
                            'url': url,
                            'summary': summary[:400],
                            'content': summary,
                            'published_date': datetime.now(timezone.utc),
                            'source': 'Googleæ–°è',
                            'search_term': search_term
                        })
                        count += 1
                
                return news_list
            else:
                return []
                
        except Exception as e:
            print(f"  âŒ æœç´¢ '{search_term}' å¤±æ•—: {e}")
            return []
    
    def fetch_all_news(self):
        """æŠ“å–æ‰€æœ‰æ–°è"""
        all_news = []
        
        print("ğŸ” é–‹å§‹æ™ºèƒ½ä¿éšªæ–°èæŠ“å–...")
        print("=" * 60)
        
        total_terms = len(self.search_terms)
        
        for i, search_term in enumerate(self.search_terms, 1):
            print(f"ğŸ“Š é€²åº¦: {i}/{total_terms} - {search_term}")
            
            # æ ¹æ“šé—œéµå­—é‡è¦æ€§æ±ºå®šæŠ“å–æ•¸é‡
            if any(x in search_term for x in ['é‡‘ç®¡æœƒ', 'ä¿éšªå±€', 'ä¿éšªæ³•']):
                max_results = 10  # ç›£ç®¡æ–°èå¾ˆé‡è¦
            elif any(x in search_term for x in ['ä¿éšª', 'äººå£½', 'ç”¢éšª']):
                max_results = 8   # æ ¸å¿ƒä¿éšªé—œéµå­—
            else:
                max_results = 6   # å…¶ä»–é—œéµå­—
            
            news = self.fetch_news_for_term(search_term, max_results)
            all_news.extend(news)
            
            # é€²åº¦æç¤º
            if i % 10 == 0:
                print(f"  ğŸ“ˆ å·²å®Œæˆ {i} å€‹é—œéµå­—ï¼Œç´¯è¨ˆ {len(all_news)} å‰‡æ–°è")
        
        print(f"ğŸ“Š ç¸½å…±æŠ“å–åˆ° {len(all_news)} å‰‡æ–°è")
        return all_news
    
    def deduplicate_news(self, news_list):
        """æ™ºèƒ½å»é‡"""
        print("ğŸ”„ æ­£åœ¨é€²è¡Œæ™ºèƒ½å»é‡...")
        
        seen_hashes = set()
        seen_urls = set()
        unique_news = []
        
        for news in news_list:
            # æª¢æŸ¥URLé‡è¤‡
            clean_url = self.clean_url(news['url'])
            if clean_url in seen_urls:
                continue
            
            # æª¢æŸ¥å…§å®¹é‡è¤‡
            content_hash = self.get_content_hash(news['title'], news['content'])
            if content_hash in seen_hashes:
                continue
            
            # å¦‚æœéƒ½ä¸é‡è¤‡ï¼ŒåŠ å…¥çµæœ
            seen_urls.add(clean_url)
            seen_hashes.add(content_hash)
            unique_news.append(news)
        
        removed_count = len(news_list) - len(unique_news)
        print(f"âœ… å»é‡å®Œæˆï¼Œç§»é™¤ {removed_count} å‰‡é‡è¤‡æ–°èï¼Œä¿ç•™ {len(unique_news)} å‰‡")
        
        return unique_news
    
    def save_to_database(self, news_list):
        """ä¿å­˜æ–°èåˆ°è³‡æ–™åº«ï¼ˆè§£æ±ºURLé‡è¤‡å•é¡Œï¼‰"""
        if not news_list:
            print("âŒ æ²’æœ‰æ–°èè¦ä¿å­˜")
            return False
        
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°è³‡æ–™åº«...")
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ç²å–ç¾æœ‰æ–°èçš„å…§å®¹å“ˆå¸Œå€¼ï¼ˆè€Œä¸æ˜¯URLï¼‰
            cursor.execute("SELECT title, content FROM news WHERE status = 'active'")
            existing_hashes = set()
            for title, content in cursor.fetchall():
                existing_hashes.add(self.get_content_hash(title, content or ""))
            
            saved_count = 0
            duplicate_count = 0
            
            for news_data in news_list:
                try:
                    title = news_data.get('title', '').strip()
                    content = news_data.get('content', '')
                    if not title:
                        continue
                    
                    # ä½¿ç”¨å…§å®¹å“ˆå¸Œæª¢æŸ¥é‡è¤‡
                    content_hash = self.get_content_hash(title, content)
                    if content_hash in existing_hashes:
                        duplicate_count += 1
                        continue
                    
                    # ç”Ÿæˆå”¯ä¸€URLï¼ˆå¦‚æœURLé‡è¤‡ï¼‰
                    original_url = news_data.get('url', '')
                    unique_url = original_url
                    
                    # æª¢æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
                    cursor.execute("SELECT COUNT(*) FROM news WHERE url = ?", (original_url,))
                    if cursor.fetchone()[0] > 0:
                        # ç”Ÿæˆå”¯ä¸€URL
                        unique_url = f"{original_url}#duplicate_{uuid.uuid4().hex[:8]}"
                    
                    # æº–å‚™æ’å…¥æ•¸æ“š
                    now = datetime.now(timezone.utc).isoformat()
                    search_term = news_data.get('search_term', '')
                    
                    # æ™ºèƒ½æ¨™é¡Œè™•ç†
                    enhanced_title = title
                    if search_term and len(search_term) <= 15:
                        # åªç‚ºçŸ­é—œéµå­—æ·»åŠ æ¨™ç±¤
                        if not any(company in search_term for company in ['åœ‹æ³°', 'å¯Œé‚¦', 'æ–°å…‰', 'å—å±±']):
                            enhanced_title = f"[{search_term}] {title}"
                    
                    cursor.execute("""
                        INSERT INTO news (
                            title, content, summary, url, source_id, category_id,
                            published_date, crawled_date, importance_score, 
                            sentiment_score, status, created_at, updated_at
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        enhanced_title,
                        content,
                        news_data.get('summary', '')[:500],
                        unique_url,
                        4,  # ä¾†æºID
                        1,  # åˆ†é¡ID
                        now,
                        now,
                        0.8,  # é‡è¦æ€§åˆ†æ•¸
                        0.1,  # æƒ…æ„Ÿåˆ†æ•¸
                        'active',
                        now,
                        now
                    ))
                    
                    # æ·»åŠ åˆ°å·²å­˜åœ¨çš„å“ˆå¸Œé›†åˆ
                    existing_hashes.add(content_hash)
                    saved_count += 1
                    
                    # é¡¯ç¤ºä¿å­˜é€²åº¦
                    if saved_count <= 10 or saved_count % 20 == 0:
                        print(f"  âœ… ä¿å­˜ç¬¬{saved_count}å‰‡: {title[:50]}...")
                    
                except Exception as e:
                    print(f"  âŒ ä¿å­˜å¤±æ•—: {e}")
                    continue
            
            conn.commit()
            conn.close()
            
            print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} å‰‡æ–°è")
            print(f"âš ï¸ è·³é {duplicate_count} å‰‡é‡è¤‡æ–°è")
            
            return saved_count > 0
            
        except Exception as e:
            print(f"âŒ è³‡æ–™åº«æ“ä½œå¤±æ•—: {e}")
            return False
    
    def get_database_stats(self):
        """ç²å–è³‡æ–™åº«çµ±è¨ˆ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM news WHERE status = 'active'")
            active_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT MAX(created_at) FROM news WHERE status = 'active'")
            latest_news = cursor.fetchone()[0]
            
            conn.close()
            
            return {
                'active_news': active_count,
                'latest_news': latest_news
            }
        except Exception as e:
            print(f"âŒ ç²å–çµ±è¨ˆå¤±æ•—: {e}")
            return {'active_news': 0, 'latest_news': None}
    
    def run(self):
        """åŸ·è¡Œæ™ºèƒ½çˆ¬èŸ²"""
        print("ğŸ§  æ™ºèƒ½ä¿éšªæ–°èçˆ¬èŸ²å•Ÿå‹•")
        print(f"ğŸ” æœç´¢ç¯„åœ: {len(self.search_terms)} å€‹é—œéµå­—")
        print(f"ğŸ—„ï¸ è³‡æ–™åº«: {self.db_path}")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # é¡¯ç¤ºç•¶å‰è³‡æ–™åº«ç‹€æ…‹
            before_stats = self.get_database_stats()
            print(f"ğŸ“Š åŸ·è¡Œå‰çµ±è¨ˆ: {before_stats['active_news']} å‰‡æ´»èºæ–°è")
            
            # 1. æŠ“å–æ‰€æœ‰æ–°è
            news_list = self.fetch_all_news()
            
            if not news_list:
                print("âŒ æ²’æœ‰æŠ“å–åˆ°ä»»ä½•æ–°è")
                return False
            
            # 2. æ™ºèƒ½å»é‡
            unique_news = self.deduplicate_news(news_list)
            
            if not unique_news:
                print("âŒ å»é‡å¾Œæ²’æœ‰æ–°è")
                return False
            
            # 3. ä¿å­˜åˆ°è³‡æ–™åº«
            success = self.save_to_database(unique_news)
            
            # 4. é¡¯ç¤ºçµæœçµ±è¨ˆ
            after_stats = self.get_database_stats()
            added_count = after_stats['active_news'] - before_stats['active_news']
            execution_time = time.time() - start_time
            
            print("=" * 60)
            print("ğŸ¯ æ™ºèƒ½çˆ¬èŸ²åŸ·è¡Œçµæœ:")
            print(f"  â±ï¸ åŸ·è¡Œæ™‚é–“: {execution_time:.1f} ç§’")
            print(f"  ğŸ” æœç´¢é—œéµå­—: {len(self.search_terms)} å€‹")
            print(f"  ğŸ“¡ æŠ“å–æ–°è: {len(news_list)} å‰‡")
            print(f"  ğŸ”„ å»é‡å¾Œ: {len(unique_news)} å‰‡")
            print(f"  âœ… æ–°å¢æ–°è: {added_count} å‰‡")
            print(f"  ğŸ“ˆ ç¸½æ´»èºæ–°è: {after_stats['active_news']} å‰‡")
            print(f"  âš¡ è™•ç†é€Ÿåº¦: {len(news_list)/execution_time:.1f} å‰‡/ç§’")
            
            if success and added_count > 0:
                print("\nğŸ‰ æ™ºèƒ½çˆ¬èŸ²åŸ·è¡ŒæˆåŠŸï¼")
                print("ğŸ’¡ ä¿éšªæ–°èåº«å·²å¤§å¹…æ“´å±•ï¼Œæ¶µè“‹æ›´å¤šç›¸é—œé ˜åŸŸ")
                print("ğŸŒŸ åŒ…å«äº†ç›£ç®¡æ”¿ç­–ã€ç§‘æŠ€å‰µæ–°ã€å…¬å¸å‹•æ…‹ã€ç¤¾æœƒè­°é¡Œç­‰å¤šå€‹é¢å‘")
                return True
            else:
                print("\nâš ï¸ åŸ·è¡Œå®Œæˆï¼Œä½†æ²’æœ‰æ–°å¢æ–°èï¼ˆå…§å®¹å¯èƒ½å·²è¦†è“‹ï¼‰")
                return False
            
        except Exception as e:
            print(f"âŒ æ™ºèƒ½çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
            return False

def main():
    """ä¸»ç¨‹å¼"""
    crawler = SmartInsuranceCrawler()
    return crawler.run()

if __name__ == "__main__":
    main()
