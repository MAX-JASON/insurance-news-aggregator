#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¢å¼·ç‰ˆç¨ç«‹çˆ¬èŸ²
Enhanced Standalone Crawler

æ”¹é€²çš„çˆ¬èŸ²ï¼ŒåŒ…å«æ›´å¥½çš„é‡è¤‡æª¢æ¸¬å’Œæ›´å¤šæ–°èæº
"""

import requests
import feedparser
import sqlite3
import os
from datetime import datetime, timezone, timedelta
from urllib.parse import quote
import re
import hashlib

def normalize_title(title):
    """æ¨™æº–åŒ–æ¨™é¡Œä»¥ä¾¿æ›´å¥½åœ°æª¢æ¸¬é‡è¤‡"""
    # ç§»é™¤å¸¸è¦‹çš„å‰å¾Œç¶´
    title = re.sub(r'^ã€.*?ã€‘', '', title)  # ç§»é™¤ã€ã€‘æ¨™è¨˜
    title = re.sub(r'\s*-\s*.*$', '', title)  # ç§»é™¤ä¾†æºæ¨™è¨˜
    title = re.sub(r'\s*\|.*$', '', title)   # ç§»é™¤|å¾Œçš„å…§å®¹
    title = re.sub(r'\s+', ' ', title).strip()  # æ¨™æº–åŒ–ç©ºæ ¼
    return title.lower()

def get_title_hash(title):
    """ç”Ÿæˆæ¨™é¡Œçš„å“ˆå¸Œå€¼ç”¨æ–¼é‡è¤‡æª¢æ¸¬"""
    normalized = normalize_title(title)
    return hashlib.md5(normalized.encode('utf-8')).hexdigest()

def fetch_insurance_news():
    """æŠ“å–ä¿éšªæ–°è"""
    news_list = []
    
    print("ğŸ” æ­£åœ¨æŠ“å–æœ€æ–°ä¿éšªæ–°è...")
    
    # 1. Googleæ–°è - ä¿éšª
    try:
        print("ğŸ“¡ ä¾†æº1: Googleæ–°è - ä¿éšª")
        search_url = "https://news.google.com/rss/search?q=ä¿éšª&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
        feed = feedparser.parse(search_url)
        
        if hasattr(feed, 'entries'):
            for entry in feed.entries[:5]:
                news_list.append({
                    'title': entry.title,
                    'url': entry.link,
                    'summary': entry.get('summary', '')[:200],
                    'content': entry.get('summary', ''),
                    'published_date': datetime.now(timezone.utc),
                    'source': 'Googleæ–°è'
                })
            print(f"  âœ… ç²å¾— {min(5, len(feed.entries))} å‰‡æ–°è")
    except Exception as e:
        print(f"  âŒ Googleæ–°èæŠ“å–å¤±æ•—: {e}")
    
    # 2. Googleæ–°è - äººå£½ä¿éšª
    try:
        print("ğŸ“¡ ä¾†æº2: Googleæ–°è - äººå£½ä¿éšª")
        search_url = "https://news.google.com/rss/search?q=äººå£½ä¿éšª&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
        feed = feedparser.parse(search_url)
        
        if hasattr(feed, 'entries'):
            for entry in feed.entries[:3]:
                news_list.append({
                    'title': entry.title,
                    'url': entry.link,
                    'summary': entry.get('summary', '')[:200],
                    'content': entry.get('summary', ''),
                    'published_date': datetime.now(timezone.utc),
                    'source': 'Googleæ–°è'
                })
            print(f"  âœ… ç²å¾— {min(3, len(feed.entries))} å‰‡æ–°è")
    except Exception as e:
        print(f"  âŒ äººå£½ä¿éšªæ–°èæŠ“å–å¤±æ•—: {e}")
    
    # 3. Googleæ–°è - ç”¢éšª
    try:
        print("ğŸ“¡ ä¾†æº3: Googleæ–°è - ç”¢éšª")
        search_url = "https://news.google.com/rss/search?q=ç”¢éšª&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
        feed = feedparser.parse(search_url)
        
        if hasattr(feed, 'entries'):
            for entry in feed.entries[:3]:
                news_list.append({
                    'title': entry.title,
                    'url': entry.link,
                    'summary': entry.get('summary', '')[:200],
                    'content': entry.get('summary', ''),
                    'published_date': datetime.now(timezone.utc),
                    'source': 'Googleæ–°è'
                })
            print(f"  âœ… ç²å¾— {min(3, len(feed.entries))} å‰‡æ–°è")
    except Exception as e:
        print(f"  âŒ ç”¢éšªæ–°èæŠ“å–å¤±æ•—: {e}")
    
    # 4. Googleæ–°è - é‡‘ç®¡æœƒä¿éšª
    try:
        print("ğŸ“¡ ä¾†æº4: Googleæ–°è - é‡‘ç®¡æœƒä¿éšª")
        search_url = "https://news.google.com/rss/search?q=é‡‘ç®¡æœƒ+ä¿éšª&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
        feed = feedparser.parse(search_url)
        
        if hasattr(feed, 'entries'):
            for entry in feed.entries[:3]:
                news_list.append({
                    'title': entry.title,
                    'url': entry.link,
                    'summary': entry.get('summary', '')[:200],
                    'content': entry.get('summary', ''),
                    'published_date': datetime.now(timezone.utc),
                    'source': 'Googleæ–°è'
                })
            print(f"  âœ… ç²å¾— {min(3, len(feed.entries))} å‰‡æ–°è")
    except Exception as e:
        print(f"  âŒ é‡‘ç®¡æœƒä¿éšªæ–°èæŠ“å–å¤±æ•—: {e}")
    
    print(f"ğŸ“Š ç¸½å…±æŠ“å–åˆ° {len(news_list)} å‰‡æ–°è")
    return news_list

def apply_date_filter(news_list, max_days=7):
    """æ‡‰ç”¨æ—¥æœŸéæ¿¾"""
    print(f"ğŸ” æ‡‰ç”¨{max_days}å¤©æ—¥æœŸéæ¿¾...")
    
    # å°æ–¼RSSæ–°èï¼Œæˆ‘å€‘å‡è¨­å®ƒå€‘éƒ½æ˜¯æœ€è¿‘çš„
    # å› ç‚ºRSSé€šå¸¸åªåŒ…å«æœ€æ–°çš„æ–°è
    filtered_news = news_list.copy()
    
    print(f"âœ… éæ¿¾å¾Œä¿ç•™ {len(filtered_news)} å‰‡æ–°è")
    return filtered_news

def save_to_database(news_list):
    """ä¿å­˜æ–°èåˆ°è³‡æ–™åº«"""
    if not news_list:
        print("âŒ æ²’æœ‰æ–°èè¦ä¿å­˜")
        return False
    
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°è³‡æ–™åº«...")
    
    # æ‰¾åˆ°è³‡æ–™åº«
    db_path = "instance/insurance_news.db"
    if not os.path.exists(db_path):
        db_path = "instance/dev_insurance_news.db"
    
    if not os.path.exists(db_path):
        print("âŒ æ‰¾ä¸åˆ°è³‡æ–™åº«æª”æ¡ˆ")
        return False
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # ç²å–ç¾æœ‰æ–°èçš„æ¨™é¡Œå“ˆå¸Œå€¼
        cursor.execute("SELECT title FROM news WHERE status = 'active'")
        existing_hashes = set()
        for (title,) in cursor.fetchall():
            existing_hashes.add(get_title_hash(title))
        
        saved_count = 0
        duplicate_count = 0
        
        for news_data in news_list:
            try:
                title = news_data.get('title', '').strip()
                if not title:
                    continue
                
                # ä½¿ç”¨å“ˆå¸Œå€¼æª¢æŸ¥é‡è¤‡
                title_hash = get_title_hash(title)
                if title_hash in existing_hashes:
                    duplicate_count += 1
                    print(f"  è·³éç›¸ä¼¼æ–°è: {title[:50]}...")
                    continue
                
                # ç”Ÿæˆå”¯ä¸€çš„æ¨™é¡Œ
                unique_title = f"{title} - {datetime.now().strftime('%H:%M')}"
                
                # æº–å‚™æ’å…¥æ•¸æ“š
                now = datetime.now(timezone.utc).isoformat()
                
                cursor.execute("""
                    INSERT INTO news (
                        title, content, summary, url, source_id, category_id,
                        published_date, crawled_date, importance_score, 
                        sentiment_score, status, created_at, updated_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    unique_title,
                    news_data.get('content', ''),
                    news_data.get('summary', '')[:500],
                    news_data.get('url', ''),
                    4,  # ä¾†æºID
                    1,  # åˆ†é¡ID
                    now,
                    now,
                    0.8,  # é‡è¦æ€§åˆ†æ•¸
                    0.1,  # æƒ…æ„Ÿåˆ†æ•¸
                    'active',
                    now,
                    now
                ))
                
                # æ·»åŠ åˆ°å·²å­˜åœ¨çš„å“ˆå¸Œé›†åˆ
                existing_hashes.add(title_hash)
                saved_count += 1
                print(f"  âœ… ä¿å­˜æ–°è: {title[:50]}...")
                
            except Exception as e:
                print(f"  âŒ ä¿å­˜å¤±æ•—: {e}")
                continue
        
        conn.commit()
        conn.close()
        
        print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} å‰‡æ–°è")
        print(f"âš ï¸ è·³é {duplicate_count} å‰‡é‡è¤‡æ–°è")
        
        return saved_count > 0
        
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«æ“ä½œå¤±æ•—: {e}")
        return False

def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸ¯ å¢å¼·ç‰ˆä¿éšªæ–°èçˆ¬èŸ²")
    print("=" * 50)
    
    try:
        # 1. æŠ“å–æ–°è
        news_list = fetch_insurance_news()
        
        if not news_list:
            print("âŒ æ²’æœ‰æŠ“å–åˆ°ä»»ä½•æ–°è")
            return False
        
        # 2. æ‡‰ç”¨æ—¥æœŸéæ¿¾
        filtered_news = apply_date_filter(news_list, max_days=7)
        
        if not filtered_news:
            print("âŒ æ—¥æœŸéæ¿¾å¾Œæ²’æœ‰æ–°è")
            return False
        
        # 3. ä¿å­˜åˆ°è³‡æ–™åº«
        success = save_to_database(filtered_news)
        
        if success:
            print("ğŸ‰ çˆ¬èŸ²åŸ·è¡ŒæˆåŠŸï¼")
            print("ğŸ’¡ ç¾åœ¨é‡æ–°æ•´ç†ç¶²é æ‡‰è©²èƒ½çœ‹åˆ°æ–°çš„æ–°è")
        else:
            print("âš ï¸ æ²’æœ‰æ–°çš„æ–°èè¢«ä¿å­˜")
        
        return success
        
    except Exception as e:
        print(f"âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    main()
