"""
å°ç£ä¿éšªæ–°èèšåˆå™¨ - åˆ†æå¼•æ“
æä¾›æ–‡æœ¬åˆ†æã€æƒ…æ„Ÿåˆ†æã€é—œéµè©æå–å’Œè¶¨å‹¢åˆ†æåŠŸèƒ½
æ•´åˆæ–‡æœ¬è™•ç†ã€é‡è¦æ€§è©•åˆ†å’Œçµæœç·©å­˜
"""

import logging
import re
import os
import json
import jieba
import jieba.analyse
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple, Optional, Set, Union
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity

from config.logging import get_logger
from analyzer.insurance_dictionary import (
    get_insurance_keywords, 
    get_insurance_companies,
    is_insurance_related,
    extract_insurance_keywords,
    calculate_insurance_relevance_score,
    INSURANCE_COMPANIES,
    INSURANCE_PRODUCTS,
    INSURANCE_TERMS,
    REGULATORY_BODIES,
    FINANCIAL_TERMS,
    MEDICAL_TERMS
)
from analyzer.cache import get_cache, cached_analysis, invalidate_cache
from analyzer.text_processor import get_text_processor, TextProcessor
from analyzer.importance_rating import ImportanceRater

# åˆå§‹åŒ–æ—¥èªŒ
logger = get_logger(__name__)

class InsuranceNewsAnalyzer:
    """ä¿éšªæ–°èåˆ†æå¼•æ“"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå¼•æ“"""
        self.logger = logger
        
        # åˆå§‹åŒ–ç›¸é—œçµ„ä»¶
        self.text_processor = get_text_processor()
        self.importance_rater = ImportanceRater()
        self.cache = get_cache()
        
        # è¼‰å…¥ä¿éšªå°ˆæ¥­è©åº«
        self._load_insurance_categories()
        
        # æƒ…æ„Ÿè©å…¸è¨­ç½®
        self._setup_sentiment_words()
        
        # TF-IDF å‘é‡åŒ–å™¨ (ç”¨æ–¼æ–‡æœ¬ç›¸ä¼¼åº¦æ¯”è¼ƒå’Œèšé¡)
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 2),
            stop_words=self.text_processor._get_stop_words()
        )
        
        logger.info("ğŸ§  ä¿éšªæ–°èåˆ†æå¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _load_insurance_categories(self):
        """è¼‰å…¥ä¿éšªåˆ†é¡é—œéµè©åº«"""
        try:
            # ä½¿ç”¨æ“´å±•å¾Œçš„ä¿éšªè©å…¸åˆ†é¡
            self.insurance_categories = {
                'ä¿éšªå…¬å¸': list(INSURANCE_COMPANIES),
                'å•†å“é¡å‹': list(INSURANCE_PRODUCTS),
                'ä¿éšªè¡“èª': list(INSURANCE_TERMS),
                'æ³•è¦ç›£ç®¡': list(REGULATORY_BODIES),
                'é‡‘èæ¦‚å¿µ': list(FINANCIAL_TERMS),
                'é†«ç™‚å¥åº·': list(MEDICAL_TERMS)
            }
            
            # çµ±è¨ˆé—œéµè©ç¸½æ•¸
            total_keywords = sum(len(words) for words in self.insurance_categories.values())
            logger.info(f"âœ… ä¿éšªé—œéµè©åº«è¼‰å…¥å®Œæˆï¼Œå…± {len(self.insurance_categories)} é¡ï¼Œç¸½è¨ˆ {total_keywords} å€‹é—œéµè©")
        
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥ä¿éšªé—œéµè©å¤±æ•—: {e}")
            self.insurance_categories = {}
    
    def _setup_sentiment_words(self):
        """è¨­ç½®æƒ…æ„Ÿè©å…¸"""
        self.positive_words = {
            # ç©æ¥µæƒ…æ„Ÿè©å½™
            'æˆé•·', 'å¢åŠ ', 'æå‡', 'æ”¹å–„', 'å„ªåŒ–', 'çªç ´', 'å‰µæ–°',
            'ä¾¿åˆ©', 'æ•ˆç‡', 'ä¿éšœ', 'å®‰å…¨', 'ç©©å®š', 'å„ªè³ª', 'é ˜å…ˆ',
            'æˆåŠŸ', 'ç²åˆ©', 'æ”¶ç›Š', 'æ©Ÿæœƒ', 'å‰æ™¯', 'æ¨‚è§€', 'æ­£é¢',
            'æ»¿æ„', 'å“è¶Š', 'å‚‘å‡º', 'å„ªå‹¢', 'å‰µæ–°', 'æ‹“å±•', 'æé«˜',
            'å“è¶Š', 'å‚‘å‡º', 'å„ªç§€', 'å¼·å¤§', 'å‰µç´€éŒ„', 'çªç ´æ€§',
            'å¢é•·', 'å„ªæƒ ', 'çå‹µ', 'ç¯€ç´„', 'å›é¥‹', 'å¤šå…ƒåŒ–'
        }
        
        self.negative_words = {
            # æ¶ˆæ¥µæƒ…æ„Ÿè©å½™
            'ä¸‹é™', 'æ¸›å°‘', 'æƒ¡åŒ–', 'é¢¨éšª', 'æå¤±', 'å›°é›£', 'æŒ‘æˆ°',
            'å•é¡Œ', 'ç¼ºé™·', 'ä¸è¶³', 'å±æ©Ÿ', 'è¡°é€€', 'è™§æ', 'è² é¢',
            'æ“”æ†‚', 'ä¸ç¢ºå®š', 'ä¸ç©©å®š', 'å¨è„…', 'éšœç¤™', 'é™åˆ¶',
            'é•è¦', 'ç½°æ¬¾', 'è¡°é€€', 'æ»‘è½', 'é™ä½', 'é€€æ­¥', 'ç–²è»Ÿ',
            'å¤±æ•—', 'åš´é‡', 'å›°å¢ƒ', 'ç³¾ç´›', 'ç³¾ç´›', 'è¨´è¨Ÿ', 'æŠ•è¨´',
            'å±éšª', 'è­¦å‘Š', 'åš´å²', 'æ‰¹è©•', 'è²¬é›£', 'è³ªç–‘', 'çˆ­è­°'
        }
        
        logger.info("âœ… æƒ…æ„Ÿè©å…¸è¨­ç½®å®Œæˆï¼Œæ­£é¢è©å½™ %d å€‹ï¼Œè² é¢è©å½™ %d å€‹", 
                   len(self.positive_words), len(self.negative_words))
    
    def extract_keywords(self, text: str, top_k: int = 10) -> List[Tuple[str, float]]:
        """
        æå–æ–‡æœ¬é—œéµè©
        
        Args:
            text: è¼¸å…¥æ–‡æœ¬
            top_k: è¿”å›é—œéµè©æ•¸é‡
            
        Returns:
            é—œéµè©åˆ—è¡¨ï¼Œæ ¼å¼ç‚º[(è©èª, æ¬Šé‡), ...]
        """
        try:
            if not text or not text.strip():
                return []
            
            # ä½¿ç”¨æ–‡æœ¬è™•ç†å™¨æå–é—œéµè©
            keywords = self.text_processor.extract_keywords(text, topK=top_k)
            
            logger.debug(f"æå–é—œéµè©: {keywords[:5] if keywords else 'ç„¡'}")
            return keywords
            
        except Exception as e:
            logger.error(f"é—œéµè©æå–å¤±æ•—: {e}")
            return []
    
    def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æœ¬æƒ…æ„Ÿ
        
        Args:
            text: è¼¸å…¥æ–‡æœ¬
            
        Returns:
            æƒ…æ„Ÿåˆ†æçµæœ
        """
        try:
            if not text or not text.strip():
                return {
                    'sentiment': 'neutral',
                    'score': 0.0,
                    'confidence': 0.0,
                    'positive_words': [],
                    'negative_words': []
                }
            
            # ä½¿ç”¨æ–‡æœ¬è™•ç†å™¨é€²è¡Œåˆ†è©
            words = self.text_processor.segment_text(text)
            
            # çµ±è¨ˆæ­£è² é¢è©èª
            positive_words = [word for word in words if word in self.positive_words]
            negative_words = [word for word in words if word in self.negative_words]
            
            # è¨ˆç®—æƒ…æ„Ÿåˆ†æ•¸
            positive_score = len(positive_words)
            negative_score = len(negative_words)
            total_score = positive_score - negative_score
            
            # æ­£è¦åŒ–åˆ†æ•¸
            max_possible = max(positive_score + negative_score, 1)
            normalized_score = total_score / max_possible
            
            # åˆ¤æ–·æƒ…æ„Ÿå‚¾å‘
            if normalized_score > 0.1:
                sentiment = 'positive'
            elif normalized_score < -0.1:
                sentiment = 'negative'
            else:
                sentiment = 'neutral'
            
            # è¨ˆç®—ä¿¡å¿ƒåº¦ (è€ƒæ…®æƒ…æ„Ÿè©æ•¸é‡ä½”æ¯”)
            sentiment_word_ratio = (positive_score + negative_score) / max(len(words), 1)
            confidence = abs(normalized_score) * min(sentiment_word_ratio * 5, 1.0)
            
            result = {
                'sentiment': sentiment,
                'score': normalized_score,
                'confidence': confidence,
                'positive_words': positive_words,
                'negative_words': negative_words,
                'word_count': len(words),
                'sentiment_word_count': positive_score + negative_score,
                'sentiment_ratio': sentiment_word_ratio
            }
            
            logger.debug(f"æƒ…æ„Ÿåˆ†æçµæœ: {sentiment} (åˆ†æ•¸: {normalized_score:.3f}, ä¿¡å¿ƒåº¦: {confidence:.3f})")
            return result
            
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿåˆ†æå¤±æ•—: {e}")
            return {
                'sentiment': 'neutral',
                'score': 0.0,
                'confidence': 0.0,
                'positive_words': [],
                'negative_words': []
            }
    
    def classify_insurance_category(self, text: str) -> Dict[str, Any]:
        """
        åˆ†é¡ä¿éšªæ–°èé¡åˆ¥
        
        Args:
            text: è¼¸å…¥æ–‡æœ¬
            
        Returns:
            åˆ†é¡çµæœ
        """
        try:
            if not text or not text.strip():
                return {'category': 'unknown', 'confidence': 0.0, 'matches': {}}
            
            # ä½¿ç”¨æ–‡æœ¬è™•ç†å™¨åˆ†ææ–‡æœ¬é¡åˆ¥
            category_scores = self.text_processor.analyze_text_categories(
                text, 
                {k: list(v) for k, v in self.insurance_categories.items()}
            )
            
            # æŸ¥æ‰¾é—œéµè©åŒ¹é…
            category_matches = {}
            for category, keywords in self.insurance_categories.items():
                matches = self.text_processor.find_keywords_in_text(text, list(keywords), use_synonym=True)
                if matches:
                    category_matches[category] = matches
            
            # æ‰¾å‡ºæœ€ä½³åŒ¹é…é¡åˆ¥
            if category_scores:
                # æ‰¾å‡ºå¾—åˆ†æœ€é«˜çš„é¡åˆ¥
                best_category = max(category_scores, key=category_scores.get)
                max_score = category_scores[best_category]
                
                # è¨ˆç®—ä¿¡å¿ƒåº¦ (ç›¸å°æ–¼ç¬¬äºŒé«˜çš„é¡åˆ¥)
                sorted_categories = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)
                
                # å¦‚æœåªæœ‰ä¸€å€‹é¡åˆ¥æˆ–è€…ç¬¬ä¸€åé¡¯è‘—é«˜æ–¼ç¬¬äºŒå
                if len(sorted_categories) == 1 or (len(sorted_categories) > 1 and sorted_categories[0][1] > sorted_categories[1][1] * 1.5):
                    confidence = min(max_score * 1.2, 1.0)  # å¢åŠ ä¿¡å¿ƒåº¦
                else:
                    # è¨ˆç®—èˆ‡ç¬¬äºŒåçš„å·®è·ä½œç‚ºä¿¡å¿ƒåº¦
                    confidence = min(max_score / sorted_categories[1][1] if sorted_categories[1][1] > 0 else 1.0, 1.0)
                
                result = {
                    'category': best_category,
                    'confidence': confidence,
                    'matches': category_matches,
                    'all_scores': category_scores,
                    'top_categories': sorted_categories[:3]  # è¿”å›å‰ä¸‰å
                }
            else:
                result = {
                    'category': 'unknown',
                    'confidence': 0.0,
                    'matches': {},
                    'all_scores': {},
                    'top_categories': []
                }
            
            logger.debug(f"åˆ†é¡çµæœ: {result['category']} (ä¿¡å¿ƒåº¦: {result.get('confidence', 0):.3f})")
            return result
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬åˆ†é¡å¤±æ•—: {e}")
            return {'category': 'unknown', 'confidence': 0.0, 'matches': {}}
    
    def find_similar_articles(self, target_text: str, article_list: List[str], 
                            top_k: int = 5) -> List[Tuple[int, float]]:
        """
        æ‰¾å‡ºç›¸ä¼¼æ–‡ç« 
        
        Args:
            target_text: ç›®æ¨™æ–‡æœ¬
            article_list: æ–‡ç« åˆ—è¡¨
            top_k: è¿”å›ç›¸ä¼¼æ–‡ç« æ•¸é‡
            
        Returns:
            ç›¸ä¼¼æ–‡ç« ç´¢å¼•å’Œç›¸ä¼¼åº¦åˆ—è¡¨
        """
        try:
            if not target_text or not article_list:
                return []
            
            # æº–å‚™æ–‡æœ¬åˆ—è¡¨
            texts = [target_text] + article_list
            
            # å°æ–‡æœ¬é€²è¡Œåˆ†è©è™•ç†
            processed_texts = []
            for text in texts:
                words = jieba.cut(text)
                processed_text = ' '.join(words)
                processed_texts.append(processed_text)
            
            # è¨ˆç®—TF-IDFçŸ©é™£
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(processed_texts)
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            target_vector = tfidf_matrix[0:1]
            article_vectors = tfidf_matrix[1:]
            
            similarities = cosine_similarity(target_vector, article_vectors)[0]
            
            # ç²å–æœ€ç›¸ä¼¼çš„æ–‡ç« 
            similar_indices = np.argsort(similarities)[::-1][:top_k]
            results = [(idx, similarities[idx]) for idx in similar_indices 
                      if similarities[idx] > 0.1]  # éæ¿¾æ‰ç›¸ä¼¼åº¦éä½çš„æ–‡ç« 
            
            logger.debug(f"æ‰¾åˆ° {len(results)} ç¯‡ç›¸ä¼¼æ–‡ç« ")
            return results
            
        except Exception as e:
            logger.error(f"ç›¸ä¼¼æ–‡ç« æŸ¥æ‰¾å¤±æ•—: {e}")
            return []
    
    def analyze_trends(self, articles_data: List[Dict[str, Any]], 
                      time_range: int = 30) -> Dict[str, Any]:
        """
        åˆ†ææ–°èè¶¨å‹¢
        
        Args:
            articles_data: æ–‡ç« æ•¸æ“šåˆ—è¡¨ï¼ŒåŒ…å«title, content, published_dateç­‰å­—æ®µ
            time_range: åˆ†ææ™‚é–“ç¯„åœï¼ˆå¤©æ•¸ï¼‰
            
        Returns:
            è¶¨å‹¢åˆ†æçµæœ
        """
        try:
            if not articles_data:
                return {'trends': {}, 'hot_topics': [], 'sentiment_trend': {}}
            
            # éæ¿¾æ™‚é–“ç¯„åœå…§çš„æ–‡ç« 
            cutoff_date = datetime.now() - timedelta(days=time_range)
            recent_articles = [
                article for article in articles_data
                if article.get('published_date') and 
                   article['published_date'] >= cutoff_date
            ]
            
            if not recent_articles:
                logger.warning("æ²’æœ‰æ‰¾åˆ°æŒ‡å®šæ™‚é–“ç¯„åœå…§çš„æ–‡ç« ")
                return {'trends': {}, 'hot_topics': [], 'sentiment_trend': {}}
            
            # é—œéµè©è¶¨å‹¢åˆ†æ
            all_keywords = []
            daily_keywords = defaultdict(list)
            daily_sentiment = defaultdict(list)
            
            for article in recent_articles:
                text = f"{article.get('title', '')} {article.get('content', '')}"
                
                # æå–é—œéµè©
                keywords = self.extract_keywords(text, top_k=10)
                all_keywords.extend([kw[0] for kw in keywords])
                
                # æŒ‰æ—¥æœŸåˆ†çµ„é—œéµè©
                date_str = article['published_date'].strftime('%Y-%m-%d')
                daily_keywords[date_str].extend([kw[0] for kw in keywords])
                
                # æƒ…æ„Ÿåˆ†æ
                sentiment = self.analyze_sentiment(text)
                daily_sentiment[date_str].append(sentiment['score'])
            
            # çµ±è¨ˆç†±é–€é—œéµè©
            keyword_counter = Counter(all_keywords)
            hot_topics = keyword_counter.most_common(20)
            
            # è¨ˆç®—æ¯æ—¥å¹³å‡æƒ…æ„Ÿ
            sentiment_trend = {}
            for date, scores in daily_sentiment.items():
                if scores:
                    sentiment_trend[date] = {
                        'average_sentiment': np.mean(scores),
                        'sentiment_variance': np.var(scores),
                        'article_count': len(scores)
                    }
            
            # é—œéµè©è¶¨å‹¢
            keyword_trends = {}
            for date, keywords in daily_keywords.items():
                keyword_counter = Counter(keywords)
                keyword_trends[date] = dict(keyword_counter.most_common(10))
            
            result = {
                'analysis_period': {
                    'start_date': cutoff_date.strftime('%Y-%m-%d'),
                    'end_date': datetime.now().strftime('%Y-%m-%d'),
                    'total_articles': len(recent_articles)
                },
                'hot_topics': hot_topics,
                'keyword_trends': keyword_trends,
                'sentiment_trend': sentiment_trend,
                'category_distribution': self._analyze_category_distribution(recent_articles)
            }
            
            logger.info(f"è¶¨å‹¢åˆ†æå®Œæˆï¼Œå…±åˆ†æ {len(recent_articles)} ç¯‡æ–‡ç« ")
            return result
            
        except Exception as e:
            logger.error(f"è¶¨å‹¢åˆ†æå¤±æ•—: {e}")
            return {'trends': {}, 'hot_topics': [], 'sentiment_trend': {}}
    
    def _analyze_category_distribution(self, articles: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†ææ–‡ç« é¡åˆ¥åˆ†å¸ƒ"""
        category_count = defaultdict(int)
        
        for article in articles:
            text = f"{article.get('title', '')} {article.get('content', '')}"
            classification = self.classify_insurance_category(text)
            category_count[classification['category']] += 1
        
        return dict(category_count)
    
    def cluster_articles(self, articles_data: List[Dict[str, Any]], 
                        n_clusters: int = 5) -> Dict[str, Any]:
        """
        å°æ–‡ç« é€²è¡Œèšé¡åˆ†æ
        
        Args:
            articles_data: æ–‡ç« æ•¸æ“šåˆ—è¡¨
            n_clusters: èšé¡æ•¸é‡
            
        Returns:
            èšé¡çµæœ
        """
        try:
            if len(articles_data) < n_clusters:
                n_clusters = max(1, len(articles_data))
            
            # æº–å‚™æ–‡æœ¬æ•¸æ“š
            texts = []
            for article in articles_data:
                text = f"{article.get('title', '')} {article.get('content', '')}"
                words = jieba.cut(text)
                processed_text = ' '.join(words)
                texts.append(processed_text)
            
            if not texts:
                return {'clusters': {}, 'cluster_centers': []}
            
            # TF-IDFå‘é‡åŒ–
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
            
            # K-meansèšé¡
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(tfidf_matrix)
            
            # çµ„ç¹”èšé¡çµæœ
            clusters = defaultdict(list)
            for idx, label in enumerate(cluster_labels):
                clusters[f"cluster_{label}"].append({
                    'index': idx,
                    'title': articles_data[idx].get('title', ''),
                    'published_date': articles_data[idx].get('published_date')
                })
            
            # æå–èšé¡ä¸­å¿ƒé—œéµè©
            feature_names = self.tfidf_vectorizer.get_feature_names_out()
            cluster_centers = []
            
            for i, center in enumerate(kmeans.cluster_centers_):
                top_indices = center.argsort()[-10:][::-1]
                top_keywords = [feature_names[idx] for idx in top_indices]
                cluster_centers.append({
                    f"cluster_{i}": top_keywords
                })
            
            result = {
                'clusters': dict(clusters),
                'cluster_centers': cluster_centers,
                'n_clusters': n_clusters,
                'total_articles': len(articles_data)
            }
            
            logger.info(f"æ–‡ç« èšé¡å®Œæˆï¼Œå…± {n_clusters} å€‹èšé¡")
            return result
            
        except Exception as e:
            logger.error(f"æ–‡ç« èšé¡å¤±æ•—: {e}")
            return {'clusters': {}, 'cluster_centers': []}
    
    def generate_summary(self, text: str, max_sentences: int = 3) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
        
        Args:
            text: è¼¸å…¥æ–‡æœ¬
            max_sentences: æœ€å¤§å¥å­æ•¸é‡
            
        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        try:
            if not text or not text.strip():
                return ""
            
            # ä½¿ç”¨æ–‡æœ¬è™•ç†å™¨ç”Ÿæˆæ‘˜è¦
            summary = self.text_processor.get_text_summary(text, max_length=max_sentences*100)
            
            logger.debug(f"ç”Ÿæˆæ‘˜è¦ï¼ŒåŸæ–‡é•·åº¦ {len(text)} å­—å…ƒï¼Œæ‘˜è¦é•·åº¦ {len(summary)} å­—å…ƒ")
            
            return summary
            
        except Exception as e:
            logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}")
            return text[:200] + "..." if len(text) > 200 else text
    
    def analyze_article(self, article_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å°å–®ç¯‡æ–‡ç« é€²è¡Œå®Œæ•´åˆ†æ
        
        Args:
            article_data: æ–‡ç« æ•¸æ“šï¼ŒåŒ…å«title, contentç­‰å­—æ®µ
            
        Returns:
            å®Œæ•´åˆ†æçµæœ
        """
        try:
            title = article_data.get('title', '')
            content = article_data.get('content', '')
            text = f"{title} {content}"
            
            if not text.strip():
                return {'error': 'æ–‡ç« å…§å®¹ç‚ºç©º'}
            
            # åŸ·è¡Œå„é …åˆ†æ
            keywords = self.extract_keywords(text, top_k=10)
            sentiment = self.analyze_sentiment(text)
            classification = self.classify_insurance_category(text)
            summary = self.generate_summary(content, max_sentences=3)
            
            result = {
                'article_info': {
                    'title': title,
                    'content_length': len(content),
                    'published_date': article_data.get('published_date'),
                    'source': article_data.get('source')
                },
                'keywords': keywords,
                'sentiment_analysis': sentiment,
                'classification': classification,
                'summary': summary,
                'analysis_timestamp': datetime.now().isoformat()
            }
            
            logger.info(f"æ–‡ç« åˆ†æå®Œæˆ: {title[:50]}...")
            return result
            
        except Exception as e:
            logger.error(f"æ–‡ç« åˆ†æå¤±æ•—: {e}")
            return {'error': str(e)}
    
    @cached_analysis('analysis')
    def analyze_news_article(self, article_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å®Œæ•´åˆ†æå–®ç¯‡æ–°èæ–‡ç« 
        
        Args:
            article_data: æ–°èæ–‡ç« æ•¸æ“šï¼ŒåŒ…å« title, content, summary ç­‰
            
        Returns:
            å®Œæ•´çš„åˆ†æçµæœ
        """
        try:
            # æå–æ–‡ç« å…§å®¹
            title = article_data.get('title', '')
            content = article_data.get('content', '')
            summary = article_data.get('summary', '')
            full_text = f"{title} {content} {summary}".strip()
            
            if not full_text:
                return self._empty_analysis_result()
            
            # 1. ä¿éšªç›¸é—œæ€§åˆ†æ
            insurance_relevance = calculate_insurance_relevance_score(full_text)
            insurance_keywords = extract_insurance_keywords(full_text)
            is_insurance = is_insurance_related(full_text)
            
            # 2. é—œéµè©æå–
            keywords = self.extract_keywords(full_text, top_k=15)
            
            # 3. æƒ…æ„Ÿåˆ†æ
            sentiment = self.analyze_sentiment(full_text)
            
            # 4. ä¿éšªé¡åˆ¥åˆ†é¡
            category_info = self.classify_insurance_category(full_text)
            
            # 5. å¤šç¶­åº¦é‡è¦æ€§è©•åˆ†
            importance_result = self._calculate_multidimensional_importance(article_data)
            
            # 6. æ¥­å‹™å½±éŸ¿åˆ†æ
            business_impact = self.importance_rater.analyze_business_impact(article_data)
            
            # 7. å®¢æˆ¶èˆˆè¶£è©•åˆ†
            client_interest = self.importance_rater.calculate_client_interest(article_data)
            
            # 8. æ–‡æœ¬çµ±è¨ˆ
            segmented_words = self.text_processor.segment_text(full_text)
            word_count = len(segmented_words)
            reading_time = max(1, word_count // 200)  # å‡è¨­æ¯åˆ†é˜è®€200å€‹è©
            
            # ç”Ÿæˆæ™ºèƒ½æ‘˜è¦ï¼ˆå¦‚æœæ²’æœ‰æ‘˜è¦ï¼‰
            auto_summary = ""
            if not summary and content and len(content) > 100:
                auto_summary = self.generate_summary(content, max_sentences=3)
            
            analysis_result = {
                # åŸºæœ¬ä¿¡æ¯
                'analyzed_at': datetime.now(),
                'text_length': len(full_text),
                'word_count': word_count,
                'reading_time': reading_time,
                
                # ä¿éšªç›¸é—œæ€§
                'insurance_relevance': {
                    'score': insurance_relevance,
                    'is_related': is_insurance,
                    'keywords': insurance_keywords[:15],  # æœ€å¤š15å€‹é—œéµè©
                    'keyword_count': len(insurance_keywords)
                },
                
                # é—œéµè©
                'keywords': [{'word': word, 'weight': weight} for word, weight in keywords],
                
                # æƒ…æ„Ÿåˆ†æ
                'sentiment': sentiment,
                
                # åˆ†é¡
                'category': category_info,
                
                # é‡è¦æ€§è©•åˆ†ï¼ˆå¤šç¶­åº¦ï¼‰
                'importance': importance_result,
                
                # æ¥­å‹™å½±éŸ¿
                'business_impact': business_impact,
                
                # å®¢æˆ¶èˆˆè¶£
                'client_interest': client_interest,
                
                # å…§å®¹ç‰¹å¾µ
                'features': {
                    'has_title': bool(title),
                    'has_content': bool(content),
                    'has_summary': bool(summary),
                    'title_length': len(title),
                    'content_length': len(content),
                    'auto_summary': auto_summary
                }
            }
            
            logger.info(f"âœ… æ–°èåˆ†æå®Œæˆ: ä¿éšªç›¸é—œåº¦={insurance_relevance:.3f}, é‡è¦æ€§={importance_result.get('final_score', 0):.3f}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ æ–°èåˆ†æå¤±æ•—: {e}")
            return self._empty_analysis_result()
    
    def _empty_analysis_result(self) -> Dict[str, Any]:
        """è¿”å›ç©ºçš„åˆ†æçµæœ"""
        return {
            'analyzed_at': datetime.now(),
            'text_length': 0,
            'word_count': 0,
            'reading_time': 0,
            'insurance_relevance': {
                'score': 0.0,
                'is_related': False,
                'keywords': [],
                'keyword_count': 0
            },
            'keywords': [],
            'sentiment': {
                'sentiment': 'neutral',
                'score': 0.0,
                'confidence': 0.0
            },
            'category': {
                'category': 'unknown',
                'confidence': 0.0
            },
            'importance': {
                'final_score': 0.0,
                'dimensions': {}
            },
            'business_impact': {
                'type': 'æœªçŸ¥',
                'level': 'ä½',
                'urgency': 'ä¸€èˆ¬åƒè€ƒ',
                'action': 'è¿½è¹¤æ–°èå¾ŒçºŒç™¼å±•'
            },
            'client_interest': {
                'level': 'ä½',
                'score': 0.0,
                'reason': 'ä¸€èˆ¬è¡Œæ¥­è³‡è¨Š'
            },
            'features': {
                'has_title': False,
                'has_content': False,
                'has_summary': False,
                'title_length': 0,
                'content_length': 0
            }
        }
    
    def _calculate_multidimensional_importance(self, article_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¨ˆç®—æ–°èå¤šç¶­åº¦é‡è¦æ€§è©•åˆ†
        
        Args:
            article_data: æ–‡ç« æ•¸æ“š
            
        Returns:
            å¤šç¶­åº¦è©•åˆ†çµæœ
        """
        try:
            # ä½¿ç”¨é‡è¦æ€§è©•åˆ†å™¨è¨ˆç®—å¤šç¶­åº¦è©•åˆ†
            importance_score = self.importance_rater.rate_importance(article_data)
            
            # å°æ–‡ç« æ•¸æ“šé€²è¡Œé‡è¦æ€§ç¶­åº¦è¨ˆç®—
            title = article_data.get('title', '')
            summary = article_data.get('summary', '')
            content = article_data.get('content', '')
            
            dimensions = self.importance_rater.calculate_dimensions(
                article_data, title, summary, content
            )
            
            # æ“´å±•è©•åˆ†çµæœ
            result = {
                'final_score': importance_score,
                'dimensions': dimensions,
                'level': self._get_importance_level(importance_score)
            }
            
            return result
            
        except Exception as e:
            logger.error(f"å¤šç¶­åº¦é‡è¦æ€§è©•åˆ†è¨ˆç®—å¤±æ•—: {e}")
            return {'final_score': 0.5, 'dimensions': {}, 'level': 'ä¸­'}
    
    def _get_importance_level(self, score: float) -> str:
        """æ ¹æ“šåˆ†æ•¸ç¢ºå®šé‡è¦æ€§ç­‰ç´š"""
        if score >= 0.75:
            return 'é«˜'
        elif score >= 0.4:
            return 'ä¸­'
        else:
            return 'ä½'

# å…¨å±€åˆ†æå™¨å¯¦ä¾‹
analyzer = None

def get_analyzer() -> InsuranceNewsAnalyzer:
    """ç²å–åˆ†æå™¨å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    global analyzer
    if analyzer is None:
        analyzer = InsuranceNewsAnalyzer()
    return analyzer


# ä¾¿æ·å‡½æ•¸
def analyze_news_article(article_data: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ†æå–®ç¯‡æ–°èæ–‡ç« """
    return get_analyzer().analyze_article(article_data)

def extract_article_keywords(text: str, top_k: int = 10) -> List[Tuple[str, float]]:
    """æå–æ–‡ç« é—œéµè©"""
    return get_analyzer().extract_keywords(text, top_k)

def analyze_article_sentiment(text: str) -> Dict[str, Any]:
    """åˆ†ææ–‡ç« æƒ…æ„Ÿ"""
    return get_analyzer().analyze_sentiment(text)

def classify_article_category(text: str) -> Dict[str, Any]:
    """åˆ†é¡æ–‡ç« é¡åˆ¥"""
    return get_analyzer().classify_insurance_category(text)

def analyze_news_trends(articles: List[Dict[str, Any]], days: int = 30) -> Dict[str, Any]:
    """åˆ†ææ–°èè¶¨å‹¢"""
    return get_analyzer().analyze_trends(articles, days)
